[
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Tentative Schedule",
    "section": "",
    "text": "Week 1: August 25-29, 2025\n\n\n\n\n\n\nDr. Seals out sick :(\n\n\n\n\n\n\n\n\n\n\nWeek 2: September 1-5, 2025\n\n\n\n\n\n\nMeeting 1:\n\nLabor Day holiday - campus closed.\n\nMeeting 2:\n\nTopic(s):\n\nVery Brief Review of Probability\n\nSlides:\n\n.html: view slides here\n.qmd: see underlying code here\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 3: September 8-12, 2025\n\n\n\n\n\n\nMeeting 1:\n\nSchedule a meeting with your EES collaborator.\nWorksheet: see Canvas.\n\nMeeting 2:\n\nTopic(s):\n\nDiscrete distributions\n\nSlides:\n\n.html: view slides here\n.qmd: see underlying code here\n\n\nAssignment 1: Probability Distributions - due September 16\n\n.html: view assignment here\n.qmd: see underlying code here\n\n\n\n\n\n\n\n\n\n\n\nWeek 4: September 15-19, 2025\n\n\n\n\n\n\nMeeting 1:\n\nTopic(s):\n\nContinuous distributions\n\nSlides:\n\n.html: view slides here\n.qmd: see underlying code here\n\n\nAssignment 1: Probability Distributions - due September 21\n\n.html: view assignment here\n.qmd: see underlying code here\n\nMeeting 2:\n\nTopic(s):\n\nThinking like a Bayesian\nBayes’ Rule/Theorem\n\nSlides:\n\n.html: view slides here\n.qmd: see underlying code here\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 5: September 22-26, 2025\n\n\n\n\n\n\nMeeting 1:\n\nTopic(s):\n\nBeta-Binomial\n\nSlides:\n\n.html:\n.qmd:\n\n\nMeeting 2:\n\nTopic(s):\n\nBalance and Sequentiality\n\nSlides:\n\n.html:\n.qmd:\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 6: September 29-October 3, 2025\n\n\n\n\n\n\nMeeting 1:\n\nTopic(s):\n\nGamma-Poisson\nNormal-Normal\n\nSlides:\n\n.html:\n.qmd:\n\n\nMeeting 2:\n\nTopic(s):\n\nApproximating the posterior\nMCMC\n\nSlides:\n\n.html:\n.qmd:\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 7: October 6-10, 2025\n\n\n\n\n\n\nMeeting 1:\n\nTopic(s):\n\nPosterior inference\nPosterior prediction\n\nSlides:\n\n.html:\n.qmd:\n\n\nAssignment 2: Basic Bayesian Analysis - .html: - .qmd:\n\n\n\n\n\n\n\n\n\n\nWeek 8: October 13-17, 2025\n\n\n\n\n\n\nMeeting 1:\n\nColumbus Day holiday - campus closed.\n\nMeeting 2:\n\nTopic(s):\n\nRegression under the normal distribution\n\nSlides:\n\n.html:\n.qmd:\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 9: October 20-24, 2025\n\n\n\n\n\n\nMeeting 1:\n\nTopic(s):\n\nEvaluating the model\n\nSlides:\n\n.html:\n.qmd:\n\n\nMeeting 2:\n\nTopic(s):\n\nPoisson/negative binomial regressions\nLogistic regression\n\nSlides:\n\n.html:\n.qmd:\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 10: October 27-31, 2025\n\n\n\n\n\n\nMeeting 2:\n\nTopic(s):\n\nHow to “read” research papers outside of statistics.\n\nProject 2a: Reviewing literature (complete in class)\n\n.html:\n.qmd:\n\nProject 2b: Reviewing literature (complete at home)\n\n.html:\n.qmd:\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 11: November 3-7, 2025\n\n\n\n\n\n\nMeeting 1:\n\nAssignment 3: Regression Analysis\n\n.html:\n.qmd:\n\nCheck in with Project 2b\n\nMeeting 2:\n\nMeet with your EES collaborator on your own to discuss methodology.\nYour annotations (Project 2b) will be completed, which will help guide the discussion.\nProject 2: Discussion worksheet\n\n\n\n\n\n\n\n\n\n\n\nWeek 12: November 10-14, 2025\n\n\n\n\n\n\nMeeting 1:\n\nWork on analysis\n\nMeeting 2:\n\nWork on analysis\nProject 3: Analysis worksheet/check in\n\n\n\n\n\n\n\n\n\n\n\nWeek 13: November 17-21, 2025\n\n\n\n\n\n\nMeeting 1:\n\nTopic(s):\n\nGeneral guidelines for writing about statistical methodology.\n\nEthical considerations.\n\nGeneral guidelines for providing results.\n\nTables.\nVisualizations.\nWritten summaries.\n\n\nSlides:\n\n.html:\n.qmd:\n\nProject 4a: Methods paragraph\nProject 4b: Tables\nProject 4c: Graphs\n\nMeeting 2:\n\n2 minute presentation of your methodology/results\n\n\n\n\n\n\n\n\n\n\n\nWeek 14: November 24-28, 2025\n\n\n\n\n\n\nMeeting 1:\n\nMeet with your EES collaborator on your own to discuss results.\nProject 5: Worksheet for discussion\n\nMeeting 2:\n\nThanksgiving holiday - campus closed.\n\n\n\n\n\n\n\n\n\n\n\nWeek 15: December 1-5, 2025\n\n\n\n\n\n\nMeeting 1:\n\nNo meeting.\nTake home final opens.\n\nMeeting 2:\n\nNo meeting.\nFinal draft of research due.\n\n\n\n\n\n\n\n\n\n\n\nWeek 16: Exam Week!\n\n\n\n\n\n\nFinal exam due this week."
  },
  {
    "objectID": "files/lectures/01-1-probability-theory.html#introduction",
    "href": "files/lectures/01-1-probability-theory.html#introduction",
    "title": "Overview of Probability Theory",
    "section": "Introduction",
    "text": "Introduction\n\nToday we will review very basic probability rules to help us build towards analysis under the Bayeisan framework.\nWe will discuss\n\nBasic terminology\nBasic properties\nTypes of probabilities\nTypes of events"
  },
  {
    "objectID": "files/lectures/01-1-probability-theory.html#terminology-for-probability",
    "href": "files/lectures/01-1-probability-theory.html#terminology-for-probability",
    "title": "Overview of Probability Theory",
    "section": "Terminology for Probability",
    "text": "Terminology for Probability\n\nExperiment: A process that results in one and only one of many possible observations.\nSimple outcomes: The possible results of our experiment.\nSample space: Collection of possible outcomes of the experiment.\nEvent: A collection of one or more of the outcomes of the experiment.\nExample: rolling a die once.\n\nOutcome: the result of the die.\nSample space: {1, 2, 3, 4, 5, 6}\nEvents: rolling an odd; rolling a multiple of 3; rolling a 3 or better."
  },
  {
    "objectID": "files/lectures/01-1-probability-theory.html#properties-of-probability",
    "href": "files/lectures/01-1-probability-theory.html#properties-of-probability",
    "title": "Overview of Probability Theory",
    "section": "Properties of Probability",
    "text": "Properties of Probability\n\nThere are two properties of probability that we must keep at the forefront of our mind.\nFirst, probability always falls between 0 and 1. Mathematically,\n\n0 \\le P[E_i] \\le 1\n\nWhat does p=0 imply?\nWhat does p=0.5 imply?\nWhat does p=1 imply?"
  },
  {
    "objectID": "files/lectures/01-1-probability-theory.html#properties-of-probability-1",
    "href": "files/lectures/01-1-probability-theory.html#properties-of-probability-1",
    "title": "Overview of Probability Theory",
    "section": "Properties of Probability",
    "text": "Properties of Probability\n\nThere are two properties of probability that we must keep at the forefront of our mind.\nSecond, the sum of all simple events for an experiment is always 1. Mathematically,\n\n\\sum_{i=1}^n P[E_i] = P[E_1] + ... + P[E_n] = 1\n\nIf there are 2 events and we know P[E_1]=0.7, what is P[E_2]? \nIf there are 4 events and we know P[E_1]=P[E_2]=0.1, P[E_3]=0.6, what is P[E_4]?"
  },
  {
    "objectID": "files/lectures/01-1-probability-theory.html#assigning-probabilities",
    "href": "files/lectures/01-1-probability-theory.html#assigning-probabilities",
    "title": "Overview of Probability Theory",
    "section": "Assigning Probabilities",
    "text": "Assigning Probabilities\n\nHow do we assign probabilities?\n\nSubjective probability\nClassicial probability rule\nRelative frequency"
  },
  {
    "objectID": "files/lectures/01-1-probability-theory.html#subjective-probability",
    "href": "files/lectures/01-1-probability-theory.html#subjective-probability",
    "title": "Overview of Probability Theory",
    "section": "Subjective Probability",
    "text": "Subjective Probability\n\nSubjective probability is the probability assigned to an event based on subjective judgement, experience, information, and belief.\nExamples:\n\nP[UWF wins national championship]\nP[tomato plant eaten by hornworms]\nP[A in this course]"
  },
  {
    "objectID": "files/lectures/01-1-probability-theory.html#classical-probability",
    "href": "files/lectures/01-1-probability-theory.html#classical-probability",
    "title": "Overview of Probability Theory",
    "section": "Classical Probability",
    "text": "Classical Probability\n\nLet A be an event for an experiment with equally likely outcomes,\n\nP[A] = \\frac{\\text{Number of outcomes favorable to $A$}}{\\text{Total number of outcomes for the experiment}}\n\nExamples:\n\nP[2 heads on 3 coin tosses]\nP[at least 2 heads on 4 coin tosses]\nP[even when rolling die]"
  },
  {
    "objectID": "files/lectures/01-1-probability-theory.html#relative-frequency",
    "href": "files/lectures/01-1-probability-theory.html#relative-frequency",
    "title": "Overview of Probability Theory",
    "section": "Relative Frequency",
    "text": "Relative Frequency\n\nIf an experiment is repeated n times and an event A is observed f times, then\n\nP[A] = \\frac{f}{n} = \\frac{\\text{Frequency of $A$}}{\\text{Sample size}}\n\nExample:\n\nP[car is a lemon] given 10/500 sampled cars from a factory are lemons.\nP[person is a homeowner] given 730/1000 sampled individuals own a home."
  },
  {
    "objectID": "files/lectures/01-1-probability-theory.html#contingency-tables",
    "href": "files/lectures/01-1-probability-theory.html#contingency-tables",
    "title": "Overview of Probability Theory",
    "section": "Contingency Tables",
    "text": "Contingency Tables\n\nSuppose 100 employees at Target were asked whether they are in favor of or against extending store hours during the holiday season.\n\n\n\n\n\n\n\n\nDepartment\n\n\nIn Favor\n\n\nAgainst\n\n\nTotal\n\n\n\n\nElectronics\n\n\n12\n\n\n8\n\n\n20\n\n\n\n\nClothing\n\n\n18\n\n\n12\n\n\n30\n\n\n\n\nGrocery\n\n\n25\n\n\n15\n\n\n40\n\n\n\n\nCustomer Service\n\n\n5\n\n\n5\n\n\n10\n\n\n\n\nTotal\n\n\n60\n\n\n40\n\n\n100"
  },
  {
    "objectID": "files/lectures/01-1-probability-theory.html#marginal-probability",
    "href": "files/lectures/01-1-probability-theory.html#marginal-probability",
    "title": "Overview of Probability Theory",
    "section": "Marginal Probability",
    "text": "Marginal Probability\n\nA marginal probability is the probability of a single event occurring without considering any other variables.\n\nIn a contingency table, marginal probabilities are found outside the body of the table.\n\nIt tells us the likelihood of one category happening overall, regardless of how it combines (or interacts) with other categories.\nIn our Target example:\n\nWhat is the probability that a randomly selected employee is in favor?\nWhat is the probability that randomly selected employee works in the Grocery department?"
  },
  {
    "objectID": "files/lectures/01-1-probability-theory.html#marginal-probability-1",
    "href": "files/lectures/01-1-probability-theory.html#marginal-probability-1",
    "title": "Overview of Probability Theory",
    "section": "Marginal Probability",
    "text": "Marginal Probability\n\nRecall,\n\n\n\n\n\n\n\n\nDepartment\n\n\nIn Favor\n\n\nAgainst\n\n\nTotal\n\n\n\n\nElectronics\n\n\n12\n\n\n8\n\n\n20\n\n\n\n\nClothing\n\n\n18\n\n\n12\n\n\n30\n\n\n\n\nGrocery\n\n\n25\n\n\n15\n\n\n40\n\n\n\n\nCustomer Service\n\n\n5\n\n\n5\n\n\n10\n\n\n\n\nTotal\n\n\n60\n\n\n40\n\n\n100\n\n\n\n\n\n\n\nWhat is the probability that a randomly selected employee is in favor?\nWhat is the probability that randomly selected employee works in the Grocery department?"
  },
  {
    "objectID": "files/lectures/01-1-probability-theory.html#joint-probability",
    "href": "files/lectures/01-1-probability-theory.html#joint-probability",
    "title": "Overview of Probability Theory",
    "section": "Joint Probability",
    "text": "Joint Probability\n\nThe joint probability is the probability that two events happen at the same time.\n\nIn a contingency table, joint probabilities are found inside the body of the table.\n\nIt tells us the likelihood that a randomly selected observation falls into both categories simultaneously.\nIn our Target example:\n\nWhat is the probability that an employee is in the Grocery department and in favor of extended hours?\nWhat is the probability that an employee is in Electronics and against extended hours?"
  },
  {
    "objectID": "files/lectures/01-1-probability-theory.html#joint-probability-1",
    "href": "files/lectures/01-1-probability-theory.html#joint-probability-1",
    "title": "Overview of Probability Theory",
    "section": "Joint Probability",
    "text": "Joint Probability\n\nRecall,\n\n\n\n\n\n\n\n\nDepartment\n\n\nIn Favor\n\n\nAgainst\n\n\nTotal\n\n\n\n\nElectronics\n\n\n12\n\n\n8\n\n\n20\n\n\n\n\nClothing\n\n\n18\n\n\n12\n\n\n30\n\n\n\n\nGrocery\n\n\n25\n\n\n15\n\n\n40\n\n\n\n\nCustomer Service\n\n\n5\n\n\n5\n\n\n10\n\n\n\n\nTotal\n\n\n60\n\n\n40\n\n\n100\n\n\n\n\n\n\n\nWhat is the probability that an employee is in the Grocery department and in favor of extended hours?\nWhat is the probability that an employee is in Electronics and against extended hours?"
  },
  {
    "objectID": "files/lectures/01-1-probability-theory.html#conditional-probability",
    "href": "files/lectures/01-1-probability-theory.html#conditional-probability",
    "title": "Overview of Probability Theory",
    "section": "Conditional Probability",
    "text": "Conditional Probability\n\nConditional probability is the probability that one event occurs given that we already know another event has occurred.\n\n“What is the probability of Event A if we know Event B is true?”\nIn a contingency table, conditional probabilities are found by limiting yourself to a specific row or column of the table, then finding the corresponding probability.\n\nIn our Target example,\n\nWhat is the probability that an employee is in favor of extended hours given that they work in the Grocery department?\nWhat is the probability that an employee works in the Electronics department given that they are against extended hours?"
  },
  {
    "objectID": "files/lectures/01-1-probability-theory.html#conditional-probability-1",
    "href": "files/lectures/01-1-probability-theory.html#conditional-probability-1",
    "title": "Overview of Probability Theory",
    "section": "Conditional Probability",
    "text": "Conditional Probability\n\nRecall,\n\n\n\n\n\n\n\n\nDepartment\n\n\nIn Favor\n\n\nAgainst\n\n\nTotal\n\n\n\n\nElectronics\n\n\n12\n\n\n8\n\n\n20\n\n\n\n\nClothing\n\n\n18\n\n\n12\n\n\n30\n\n\n\n\nGrocery\n\n\n25\n\n\n15\n\n\n40\n\n\n\n\nCustomer Service\n\n\n5\n\n\n5\n\n\n10\n\n\n\n\nTotal\n\n\n60\n\n\n40\n\n\n100\n\n\n\n\n\n\n\nWhat is the probability that an employee is in favor of extended hours given that they work in the Grocery department?\nWhat is the probability that an employee works in the Electronics department given that they are against extended hours?"
  },
  {
    "objectID": "files/lectures/01-1-probability-theory.html#mutually-exclusive-events",
    "href": "files/lectures/01-1-probability-theory.html#mutually-exclusive-events",
    "title": "Overview of Probability Theory",
    "section": "Mutually Exclusive Events",
    "text": "Mutually Exclusive Events\n\nEvents that cannot occur together are mutually exclusive or disjoint.\nConsider rolling a single die:\n\nA = an even number = {2, 4, 6}\nB = an odd number = {1, 3, 5}\nC = a number less than 5 = {1, 2, 3, 4}\n\nAre events A and B mutually exclusive?\nAre events A and C mutually exclusive?"
  },
  {
    "objectID": "files/lectures/01-1-probability-theory.html#mutually-exclusive-events-1",
    "href": "files/lectures/01-1-probability-theory.html#mutually-exclusive-events-1",
    "title": "Overview of Probability Theory",
    "section": "Mutually Exclusive Events",
    "text": "Mutually Exclusive Events\n\nConsider rolling a single die:\n\nA = an even number = {2, 4, 6}\nB = an odd number = {1, 3, 5}\nC = a number less than 5 = {1, 2, 3, 4}\n\nAre events A and B mutually exclusive?"
  },
  {
    "objectID": "files/lectures/01-1-probability-theory.html#mutually-exclusive-events-2",
    "href": "files/lectures/01-1-probability-theory.html#mutually-exclusive-events-2",
    "title": "Overview of Probability Theory",
    "section": "Mutually Exclusive Events",
    "text": "Mutually Exclusive Events\n\nConsider rolling a single die:\n\nA = an even number = {2, 4, 6}\nB = an odd number = {1, 3, 5}\nC = a number less than 5 = {1, 2, 3, 4}\n\nAre events A and C mutually exclusive?"
  },
  {
    "objectID": "files/lectures/01-1-probability-theory.html#independent-events",
    "href": "files/lectures/01-1-probability-theory.html#independent-events",
    "title": "Overview of Probability Theory",
    "section": "Independent Events",
    "text": "Independent Events\n\nTwo events are said to be independent if the occurrence of one event does not affect the probability of the occurrence of the other event.\nMathematically,\n\n P[A|B] = P[A] \\text{ or } P[B|A] = P[B]\n\nIn our Target example, is department independent of being in favor of extended hours?\n\nWe are being asked to examine P[department|favor] or P[favor|department]."
  },
  {
    "objectID": "files/lectures/01-1-probability-theory.html#independent-events-1",
    "href": "files/lectures/01-1-probability-theory.html#independent-events-1",
    "title": "Overview of Probability Theory",
    "section": "Independent Events",
    "text": "Independent Events\n\nRecall,\n\n\n\n\n\n\n\n\nDepartment\n\n\nIn Favor\n\n\nAgainst\n\n\nTotal\n\n\n\n\nElectronics\n\n\n12\n\n\n8\n\n\n20\n\n\n\n\nClothing\n\n\n18\n\n\n12\n\n\n30\n\n\n\n\nGrocery\n\n\n25\n\n\n15\n\n\n40\n\n\n\n\nCustomer Service\n\n\n5\n\n\n5\n\n\n10\n\n\n\n\nTotal\n\n\n60\n\n\n40\n\n\n100\n\n\n\n\n\n\n\nWe need to examine P[department|favor] or P[favor|department]. We say that they are independent if\n\nP[department|favor] = P[department]\nP[favor|department] = P[favor]"
  },
  {
    "objectID": "files/lectures/01-1-probability-theory.html#complementary-events",
    "href": "files/lectures/01-1-probability-theory.html#complementary-events",
    "title": "Overview of Probability Theory",
    "section": "Complementary Events",
    "text": "Complementary Events\n\nThe complement of A is the event that includes all the outcomes that are not in A.\n\n\n\n\n\nMathematically,\n\n\n\\begin{align*}\nP[A] &+ P[A^c] = 1 \\\\\nP[A] &= 1 - P[A^c] \\\\\nP[A^c] &= 1 - P[A]\n\\end{align*}"
  },
  {
    "objectID": "files/lectures/01-1-probability-theory.html#complementary-events-1",
    "href": "files/lectures/01-1-probability-theory.html#complementary-events-1",
    "title": "Overview of Probability Theory",
    "section": "Complementary Events",
    "text": "Complementary Events\n\nRecall,\n\n\n\n\n\n\n\n\nDepartment\n\n\nIn Favor\n\n\nAgainst\n\n\nTotal\n\n\n\n\nElectronics\n\n\n12\n\n\n8\n\n\n20\n\n\n\n\nClothing\n\n\n18\n\n\n12\n\n\n30\n\n\n\n\nGrocery\n\n\n25\n\n\n15\n\n\n40\n\n\n\n\nCustomer Service\n\n\n5\n\n\n5\n\n\n10\n\n\n\n\nTotal\n\n\n60\n\n\n40\n\n\n100\n\n\n\n\n\n\n\nFind P[Electronicsc].\nFind P[Electronicsc | Against]."
  },
  {
    "objectID": "files/lectures/01-1-probability-theory.html#wrap-up",
    "href": "files/lectures/01-1-probability-theory.html#wrap-up",
    "title": "Overview of Probability Theory",
    "section": "Wrap Up",
    "text": "Wrap Up\n\nToday we have reviewed probability basics.\n\nNote that this is no replacement for Advanced Probability :)\n\nWe just need to understand general concepts to move foward.\nNext week:\n\nMonday: no class meeting – instead, you will meet with your EES collaborator (at an agreed upon time/date).\n\nPairing document with contact information is linked on Discord. We now have a “EES project” channel.\n\nWednesday: probability distributions"
  },
  {
    "objectID": "files/lectures/04-2-thinking-bayesian.html#introduction",
    "href": "files/lectures/04-2-thinking-bayesian.html#introduction",
    "title": "Thinking Like a Bayesian",
    "section": "Introduction",
    "text": "Introduction\n\nBefore today:\n\nRefresher on probability theory\nWhat does each distribution do?\n\nbeta \\to outcomes limited to [0, 1]\nbinomial \\to binary outcomes\ngamma \\to continuous & positive outcomes; skewed right\nnormal \\to continuous outcomes; mound-shaped & symmetric distribution\nPoisson \\to count outcomes; skewed right\nuniform \\to each outcome has equal probability; rectangular distribution\n\n\nToday: building up Bayesian analysis concepts"
  },
  {
    "objectID": "files/lectures/04-2-thinking-bayesian.html#thinking-like-a-bayesian",
    "href": "files/lectures/04-2-thinking-bayesian.html#thinking-like-a-bayesian",
    "title": "Thinking Like a Bayesian",
    "section": "Thinking Like a Bayesian",
    "text": "Thinking Like a Bayesian\n\nBayesian analysis involves updating beliefs based on observed data."
  },
  {
    "objectID": "files/lectures/04-2-thinking-bayesian.html#thinking-like-a-bayesian-1",
    "href": "files/lectures/04-2-thinking-bayesian.html#thinking-like-a-bayesian-1",
    "title": "Thinking Like a Bayesian",
    "section": "Thinking Like a Bayesian",
    "text": "Thinking Like a Bayesian\n\nThis is the natural Bayesian knowledge-building process of:\n\nacknowledging your preconceptions (prior distribution),\nusing data (data distribution) to update your knowledge (posterior distribution), and\nrepeating (posterior distribution \\to new prior distribution)"
  },
  {
    "objectID": "files/lectures/04-2-thinking-bayesian.html#thinking-like-a-bayesian-2",
    "href": "files/lectures/04-2-thinking-bayesian.html#thinking-like-a-bayesian-2",
    "title": "Thinking Like a Bayesian",
    "section": "Thinking Like a Bayesian",
    "text": "Thinking Like a Bayesian\n\nBayesian and frequentist analyses share a common goal: to learn from data about the world around us.\n\nBoth Bayesian and frequentist analyses use data to fit models, make predictions, and evaluate hypotheses.\nWhen working with the same data, they will typically produce a similar set of conclusions.\n\nStatisticians typically identify as either a “Bayesian” or “frequentist” …\n\n🚫 We are not going to “take sides.”\n✅ We will see these as tools in our toolbox."
  },
  {
    "objectID": "files/lectures/04-2-thinking-bayesian.html#thinking-like-a-bayesian-3",
    "href": "files/lectures/04-2-thinking-bayesian.html#thinking-like-a-bayesian-3",
    "title": "Thinking Like a Bayesian",
    "section": "Thinking Like a Bayesian",
    "text": "Thinking Like a Bayesian\n\nBayesian probability: the relative plausibility of an event.\n\nConsiders prior belief."
  },
  {
    "objectID": "files/lectures/04-2-thinking-bayesian.html#thinking-like-a-bayesian-4",
    "href": "files/lectures/04-2-thinking-bayesian.html#thinking-like-a-bayesian-4",
    "title": "Thinking Like a Bayesian",
    "section": "Thinking Like a Bayesian",
    "text": "Thinking Like a Bayesian\n\nFrequentist probability: the long-run relative frequency of a repeatable event.\n\nDoes not consider prior belief."
  },
  {
    "objectID": "files/lectures/04-2-thinking-bayesian.html#thinking-like-a-bayesian-5",
    "href": "files/lectures/04-2-thinking-bayesian.html#thinking-like-a-bayesian-5",
    "title": "Thinking Like a Bayesian",
    "section": "Thinking Like a Bayesian",
    "text": "Thinking Like a Bayesian\n\nThe Bayesian framework depends upon prior information, data, and the balance between them.\n\nThe balance between the prior information and data is determined by the relative strength of each\n\n\n\n\nWhen we have little data, our posterior can rely more on prior knowledge.\nAs we collect more data, the prior can lose its influence."
  },
  {
    "objectID": "files/lectures/04-2-thinking-bayesian.html#thinking-like-a-bayesian-6",
    "href": "files/lectures/04-2-thinking-bayesian.html#thinking-like-a-bayesian-6",
    "title": "Thinking Like a Bayesian",
    "section": "Thinking Like a Bayesian",
    "text": "Thinking Like a Bayesian\n\nWe can also use this approach to combine analysis results."
  },
  {
    "objectID": "files/lectures/04-2-thinking-bayesian.html#thinking-like-a-bayesian-7",
    "href": "files/lectures/04-2-thinking-bayesian.html#thinking-like-a-bayesian-7",
    "title": "Thinking Like a Bayesian",
    "section": "Thinking Like a Bayesian",
    "text": "Thinking Like a Bayesian\n\nWe will use an example to work through Bayesian logic.\nThe Collins Dictionary named “fake news” the 2017 term of the year.\n\nFake, misleading, and biased news has proliferated along with online news and social media platforms which allow users to post articles with little quality control.\n\nWe want to flag articles as “real” or “fake.”\nWe’ll examine a sample of 150 articles which were posted on Facebook and fact checked by five BuzzFeed journalists (Shu et al. 2017)."
  },
  {
    "objectID": "files/lectures/04-2-thinking-bayesian.html#thinking-like-a-bayesian-8",
    "href": "files/lectures/04-2-thinking-bayesian.html#thinking-like-a-bayesian-8",
    "title": "Thinking Like a Bayesian",
    "section": "Thinking Like a Bayesian",
    "text": "Thinking Like a Bayesian\n\nInformation about each article is stored in the fake_news dataset in the bayesrules package.\n\n\nfake_news &lt;- bayesrules::fake_news\nprint(colnames(fake_news))\n\n [1] \"title\"                   \"text\"                   \n [3] \"url\"                     \"authors\"                \n [5] \"type\"                    \"title_words\"            \n [7] \"text_words\"              \"title_char\"             \n [9] \"text_char\"               \"title_caps\"             \n[11] \"text_caps\"               \"title_caps_percent\"     \n[13] \"text_caps_percent\"       \"title_excl\"             \n[15] \"text_excl\"               \"title_excl_percent\"     \n[17] \"text_excl_percent\"       \"title_has_excl\"         \n[19] \"anger\"                   \"anticipation\"           \n[21] \"disgust\"                 \"fear\"                   \n[23] \"joy\"                     \"sadness\"                \n[25] \"surprise\"                \"trust\"                  \n[27] \"negative\"                \"positive\"               \n[29] \"text_syllables\"          \"text_syllables_per_word\""
  },
  {
    "objectID": "files/lectures/04-2-thinking-bayesian.html#thinking-like-a-bayesian-9",
    "href": "files/lectures/04-2-thinking-bayesian.html#thinking-like-a-bayesian-9",
    "title": "Thinking Like a Bayesian",
    "section": "Thinking Like a Bayesian",
    "text": "Thinking Like a Bayesian\n\nWe could build a simple news filter which uses the following rule: since most articles are real, we should read and believe all articles.\n\nWhile this filter would solve the problem of disregarding real articles, we would read lots of fake news.\nIt also only takes into account the overall rates of, not the typical features of, real and fake news."
  },
  {
    "objectID": "files/lectures/04-2-thinking-bayesian.html#thinking-like-a-bayesian-10",
    "href": "files/lectures/04-2-thinking-bayesian.html#thinking-like-a-bayesian-10",
    "title": "Thinking Like a Bayesian",
    "section": "Thinking Like a Bayesian",
    "text": "Thinking Like a Bayesian\n\nSuppose that the most recent article posted to a social media platform is titled: The president has a funny secret!\n\nSome features of this title probably set off some red flags.\nFor example, the usage of an exclamation point might seem like an odd choice for a real news article.\n\nIn the dataset, what is the split of real and fake articles?"
  },
  {
    "objectID": "files/lectures/04-2-thinking-bayesian.html#thinking-like-a-bayesian-11",
    "href": "files/lectures/04-2-thinking-bayesian.html#thinking-like-a-bayesian-11",
    "title": "Thinking Like a Bayesian",
    "section": "Thinking Like a Bayesian",
    "text": "Thinking Like a Bayesian\n\nIn the dataset, what is the split of real and fake articles?\n\n\n\n\n  \n\n\n\n\nOur data backs up our instinct on the article,\n\n\n\n\n  \n\n\n\n\nIn this dataset, 26.67% (16 of 60) of fake news titles but only 2.22% (2 of 90) of real news titles use an exclamation point."
  },
  {
    "objectID": "files/lectures/04-2-thinking-bayesian.html#thinking-like-a-bayesian-12",
    "href": "files/lectures/04-2-thinking-bayesian.html#thinking-like-a-bayesian-12",
    "title": "Thinking Like a Bayesian",
    "section": "Thinking Like a Bayesian",
    "text": "Thinking Like a Bayesian\n\nWe now have two pieces of contradictory information.\n\nOur prior information suggested that incoming articles are most likely real.\nHowever, the exclamation point data is more consistent with fake news.\n\n\n\n\nThinking like Bayesians, we know that balancing both pieces of information is important in developing a posterior understanding of whether the article is fake."
  },
  {
    "objectID": "files/lectures/04-2-thinking-bayesian.html#building-a-bayesian-model",
    "href": "files/lectures/04-2-thinking-bayesian.html#building-a-bayesian-model",
    "title": "Thinking Like a Bayesian",
    "section": "Building a Bayesian Model",
    "text": "Building a Bayesian Model\n\nOur fake news analysis studies two variables:\n\nan article’s fake vs real status and\nits use of exclamation points.\n\nWe can represent the randomness in these variables using probability models.\nWe will now build:\n\na prior probability model for our prior understanding of whether the most recent article is fake;\na model for interpreting the exclamation point data; and, eventually,\na posterior probability model which summarizes the posterior plausibility that the article is fake."
  },
  {
    "objectID": "files/lectures/04-2-thinking-bayesian.html#building-a-bayesian-model-1",
    "href": "files/lectures/04-2-thinking-bayesian.html#building-a-bayesian-model-1",
    "title": "Thinking Like a Bayesian",
    "section": "Building a Bayesian Model",
    "text": "Building a Bayesian Model\n\nLet’s now formalize our prior understanding of whether the new article is fake.\nBased on our fake_news data, we saw that 40% of articles are fake and 60% are real.\n\nBefore reading the new article, there’s a 0.4 prior probability that it’s fake and a 0.6 prior probability it’s not.\n\n\nP\\left[B\\right] = 0.40 \\text{ and } P\\left[B^c\\right] = 0.60\n\nRemember that a valid probability model must:\n\naccount for all possible events (all articles must be fake or real);\nit assigns prior probabilities to each event; and\nthe probabilities sum to one."
  },
  {
    "objectID": "files/lectures/04-2-thinking-bayesian.html#building-a-bayesian-model-2",
    "href": "files/lectures/04-2-thinking-bayesian.html#building-a-bayesian-model-2",
    "title": "Thinking Like a Bayesian",
    "section": "Building a Bayesian Model",
    "text": "Building a Bayesian Model\n\n\n\n\n\n\n\n\ntitle_has_excl\nfake\nreal\n\n\n\n\nFALSE\n73.3% (44)\n97.8% (88)\n\n\nTRUE\n26.7% (16)\n2.2% (2)\n\n\n\n\n\n\n\n\nWe have the following conditional probabilities:\n\nIf an article is fake (B), then there’s a roughly 26.67% chance it uses exclamation points in the title.\nIf an article is real (B^c), then there’s only a roughly 2.22% chance it uses exclamation points."
  },
  {
    "objectID": "files/lectures/04-2-thinking-bayesian.html#building-a-bayesian-model-3",
    "href": "files/lectures/04-2-thinking-bayesian.html#building-a-bayesian-model-3",
    "title": "Thinking Like a Bayesian",
    "section": "Building a Bayesian Model",
    "text": "Building a Bayesian Model\n\nWe have the following conditional probabilities:\n\nIf an article is fake (B), then there’s a roughly 26.67% chance it uses exclamation points in the title.\nIf an article is real (B^c), then there’s only a roughly 2.22% chance it uses exclamation points.\n\nLooking at the probabilities, we can see that 26.67% of fake articles vs. 2.22% of real articles use exclamation points.\n\nExclamation point usage is much more likely among fake news than real news.\nWe have evidence that the article is fake."
  },
  {
    "objectID": "files/lectures/04-2-thinking-bayesian.html#building-a-bayesian-model-4",
    "href": "files/lectures/04-2-thinking-bayesian.html#building-a-bayesian-model-4",
    "title": "Thinking Like a Bayesian",
    "section": "Building a Bayesian Model",
    "text": "Building a Bayesian Model\n\nNote that we know that the incoming article used exclamation points (A), but we do not actually know if the article is fake (B or B^c).\nIn this case, we compared P[A|B] and P[A|B^c] to ascertain the relative likelihood of observing A under different scenarios.\n\nL\\left[B|A\\right] = P\\left[A|B\\right] \\text{ and } L\\left[B^c|A\\right] = P\\left[A|B^c\\right]"
  },
  {
    "objectID": "files/lectures/04-2-thinking-bayesian.html#building-a-bayesian-model-5",
    "href": "files/lectures/04-2-thinking-bayesian.html#building-a-bayesian-model-5",
    "title": "Thinking Like a Bayesian",
    "section": "Building a Bayesian Model",
    "text": "Building a Bayesian Model\n\n\n\nEvent\nB\nBc\nTotal\n\n\n\n\nPrior Probability\n0.4\n0.6\n1.0\n\n\nLikelihood\n0.2667\n0.0222\n0.2889\n\n\n\nL\\left[B|A\\right] = P\\left[A|B\\right] \\text{ and } L\\left[B^c|A\\right] = P\\left[A|B^c\\right]\n\nIt is important for us to note that the likelihood function is not a probability function.\n\nThis is a framework to compare the relative comparability of our exclamation point data with B and B^c."
  },
  {
    "objectID": "files/lectures/04-2-thinking-bayesian.html#building-a-bayesian-model-6",
    "href": "files/lectures/04-2-thinking-bayesian.html#building-a-bayesian-model-6",
    "title": "Thinking Like a Bayesian",
    "section": "Building a Bayesian Model",
    "text": "Building a Bayesian Model\n\n\n\nEvent\nB (fake)\nBc (real)\nTotal\n\n\n\n\nPrior Probability\n0.4\n0.6\n1.0\n\n\nLikelihood\n0.2667\n0.0222\n0.2889\n\n\n\n\nThe prior evidence suggested the article is most likely real,\n\nP[B] = 0.4 &lt; P[B^c] = 0.6\n\nThe data, however, is more consistent with the article being fake,\n\nL[B|A] = 0.2667 &gt; L[B^c|A] = 0.0222"
  },
  {
    "objectID": "files/lectures/04-2-thinking-bayesian.html#building-a-bayesian-model-7",
    "href": "files/lectures/04-2-thinking-bayesian.html#building-a-bayesian-model-7",
    "title": "Thinking Like a Bayesian",
    "section": "Building a Bayesian Model",
    "text": "Building a Bayesian Model\n\nWe can summarize our probabilities in a table,\n\n\n\n\n\nB\nB^c\nTotal\n\n\n\n\nA\n\n\n\n\n\nA^c\n\n\n\n\n\nTotal\n0.4\n0.6\n1\n\n\n\n\nAs found earlier, P[A|B] = 0.2667 and P[A|B^c]=0.0222.\n\n\n\\begin{align*}\nP[A \\cap B] &= P[A|B] \\times P[B] \\\\\n&= 0.2667 \\times 0.4 \\\\\n&= 0.1067\n\\end{align*}"
  },
  {
    "objectID": "files/lectures/04-2-thinking-bayesian.html#building-a-bayesian-model-8",
    "href": "files/lectures/04-2-thinking-bayesian.html#building-a-bayesian-model-8",
    "title": "Thinking Like a Bayesian",
    "section": "Building a Bayesian Model",
    "text": "Building a Bayesian Model\n\nHere’s what we know,\n\n\n\n\n\nB\nB^c\nTotal\n\n\n\n\nA\n0.1067\n\n\n\n\nA^c\n\n\n\n\n\nTotal\n0.4\n0.6\n1\n\n\n\n\nWe also know P[A|B] = 0.2667 and P[A|B^c]=0.0222.\n\n\n\\begin{align*}\nP[A^c \\cap B] &= P(A^c|B) \\times P[B]  \\\\\n&= (1-P[A|B]) \\times P[B] \\\\\n&= (1-0.2667) \\times 0.4 \\\\\n&= 0.2933\n\\end{align*}"
  },
  {
    "objectID": "files/lectures/04-2-thinking-bayesian.html#building-a-bayesian-model-9",
    "href": "files/lectures/04-2-thinking-bayesian.html#building-a-bayesian-model-9",
    "title": "Thinking Like a Bayesian",
    "section": "Building a Bayesian Model",
    "text": "Building a Bayesian Model\n\nHere’s what we know,\n\n\n\n\n\nB\nB^c\nTotal\n\n\n\n\nA\n0.1067\n\n\n\n\nA^c\n0.2933\n\n\n\n\nTotal\n0.4\n0.6\n1\n\n\n\n\nWe also know P[A|B] = 0.2667 and P[A|B^c]=0.0222.\n\n\n\\begin{align*}\nP[A \\cap B^c] &= P[A|B^c] \\times P[B^c]  \\\\\n&= 0.0222 \\times 0.6 \\\\\n&= 0.0133\n\\end{align*}"
  },
  {
    "objectID": "files/lectures/04-2-thinking-bayesian.html#building-a-bayesian-model-10",
    "href": "files/lectures/04-2-thinking-bayesian.html#building-a-bayesian-model-10",
    "title": "Thinking Like a Bayesian",
    "section": "Building a Bayesian Model",
    "text": "Building a Bayesian Model\n\nHere’s what we know,\n\n\n\n\n\nB\nB^c\nTotal\n\n\n\n\nA\n0.1067\n0.0133\n\n\n\nA^c\n0.2933\n\n\n\n\nTotal\n0.4\n0.6\n1\n\n\n\n\nWe also know P[A|B] = 0.2667 and P[A|B^c]=0.0222.\n\n\n\\begin{align*}\nP[A^c \\cap B^c] &= P[A^c|B^c] \\times P[B^c]  \\\\\n&= 0.9778 \\times 0.6 \\\\\n&= 0.5867\n\\end{align*}"
  },
  {
    "objectID": "files/lectures/04-2-thinking-bayesian.html#building-a-bayesian-model-11",
    "href": "files/lectures/04-2-thinking-bayesian.html#building-a-bayesian-model-11",
    "title": "Thinking Like a Bayesian",
    "section": "Building a Bayesian Model",
    "text": "Building a Bayesian Model\n\nHere’s what we know,\n\n\n\n\n\nB\nB^c\nTotal\n\n\n\n\nA\n0.1067\n0.0133\n\n\n\nA^c\n0.2933\n0.5867\n\n\n\nTotal\n0.4\n0.6\n1\n\n\n\n\nFinally,\n\n\n\\begin{align*}\n&P[A] = 0.1067 + 0.0133 = 0.12 \\\\\n&P[A^c] = 0.2933 + 0.5867 = 0.88\n\\end{align*}"
  },
  {
    "objectID": "files/lectures/04-2-thinking-bayesian.html#building-a-bayesian-model-12",
    "href": "files/lectures/04-2-thinking-bayesian.html#building-a-bayesian-model-12",
    "title": "Thinking Like a Bayesian",
    "section": "Building a Bayesian Model",
    "text": "Building a Bayesian Model\n\nUsing rules of probability, we have completed the table.\n\n\n\n\n\nB\nB^c\nTotal\n\n\n\n\nA\n0.1067\n0.0133\n0.12\n\n\nA^c\n0.2933\n0.5867\n0.88\n\n\nTotal\n0.4\n0.6\n1"
  },
  {
    "objectID": "files/lectures/04-2-thinking-bayesian.html#building-a-bayesian-model-13",
    "href": "files/lectures/04-2-thinking-bayesian.html#building-a-bayesian-model-13",
    "title": "Thinking Like a Bayesian",
    "section": "Building a Bayesian Model",
    "text": "Building a Bayesian Model\n\nWith more information, we can answer the question: what is the probability that the latest article is fake?\nWe will use the posterior probability, P[B|A], which is found using Bayes’ Rule.\nBayes’ Rule: For events A and B,\n\nP[B|A] = \\frac{P[A \\cap B]}{P[A]} = \\frac{P[B] \\times L[B|A]}{P[A]}\n\nBut really, we can think about it like this,\n\n\\text{posterior} = \\frac{\\text{prior} \\times \\text{likelihood}}{\\text{normalizing constant}}"
  },
  {
    "objectID": "files/lectures/04-2-thinking-bayesian.html#example-set-up",
    "href": "files/lectures/04-2-thinking-bayesian.html#example-set-up",
    "title": "Thinking Like a Bayesian",
    "section": "Example Set Up",
    "text": "Example Set Up\n\nIn 1996, Gary Kasparov played a six-game chess match against the IBM supercomputer Deep Blue.\n\nOf the six games, Kasparov won three, drew two, and lost one.\nThus, Kasparov won the overall match.\n\nKasparov and Deep Blue were to meet again for a six-game match in 1997.\nLet \\pi denote Kasparov’s chances of winning any particular game in the re-match.\n\nThus, \\pi is a measure of his overall skill relative to Deep Blue.\nGiven the complexity of chess, machines, and humans, \\pi is unknown and can vary over time.\n\ni.e., \\pi is a random variable."
  },
  {
    "objectID": "files/lectures/04-2-thinking-bayesian.html#example-set-up-1",
    "href": "files/lectures/04-2-thinking-bayesian.html#example-set-up-1",
    "title": "Thinking Like a Bayesian",
    "section": "Example Set Up",
    "text": "Example Set Up\n\nOur first step is to start with a prior model. This model\n\nIdentifies what values \\pi can take,\nassigns a prior weight or probability to each, and\nthese probabilities sum to 1.\n\nBased on what we were told, the prior model for \\pi in our example,\n\n\n\n\n\\pi\n0.2\n0.5\n0.8\nTotal\n\n\n\n\nf(\\pi)\n0.10\n0.25\n0.65\n1"
  },
  {
    "objectID": "files/lectures/04-2-thinking-bayesian.html#example-set-up-2",
    "href": "files/lectures/04-2-thinking-bayesian.html#example-set-up-2",
    "title": "Thinking Like a Bayesian",
    "section": "Example Set Up",
    "text": "Example Set Up\n\nBased on what we were told, the prior model for \\pi in our example,\n\n\n\n\n\\pi\n0.2\n0.5\n0.8\nTotal\n\n\n\n\nf(\\pi)\n0.10\n0.25\n0.65\n1\n\n\n\n\nNote that this is an incredibly simple model.\n\nThe win probability can technically be any number \\in [0, 1].\nHowever, this prior assumes that \\pi has a discrete set of possibilities: 20%, 50%, or 80%."
  },
  {
    "objectID": "files/lectures/04-2-thinking-bayesian.html#example-set-up-3",
    "href": "files/lectures/04-2-thinking-bayesian.html#example-set-up-3",
    "title": "Thinking Like a Bayesian",
    "section": "Example Set Up",
    "text": "Example Set Up\n\nIn the second step of our analysis, we collect and process data which can inform our understanding of \\pi.\nHere, Y = the number of the six games in the 1997 re-match that Kasparov wins.\n\nAs chess match outcome isn’t predetermined, Y is a random variable that can take any value in \\{0, 1, 2, 3, 4, 5, 6\\}.\n\nNote that Y inherently depends upon \\pi.\n\nIf \\pi = 0.80, Y would also be high (on average).\nIf \\pi = 0.20, Y would also be low (on average).\n\nThus, we must model this dependence of Y on \\pi using a conditional probability model."
  },
  {
    "objectID": "files/lectures/04-2-thinking-bayesian.html#binomial-data-model",
    "href": "files/lectures/04-2-thinking-bayesian.html#binomial-data-model",
    "title": "Thinking Like a Bayesian",
    "section": "Binomial Data Model",
    "text": "Binomial Data Model\n\nWe must make two assumptions about the chess match:\n\nGames are independent (the outcome of one game does not influence the outcome of another).\nKasparov has an equal probability of winning any game in the match.\n\ni.e., probability of winning does not increase or decrease as the match goes on.\n\n\nWe will use a binomial model for this problem.\n\nIn our case,\n\n\nY|\\pi \\sim \\text{Bin}(6, \\pi)"
  },
  {
    "objectID": "files/lectures/04-2-thinking-bayesian.html#binomial-data-model-1",
    "href": "files/lectures/04-2-thinking-bayesian.html#binomial-data-model-1",
    "title": "Thinking Like a Bayesian",
    "section": "Binomial Data Model",
    "text": "Binomial Data Model\n\nLet’s assume \\pi = 0.8.\nThe probability that he would win all 6 games is approximately 26%.\n\nf(y=6|\\pi=0.8) = {6 \\choose 6} 0.8^6 (1-0.8)^{6-6}, \n\ndbinom(6, 6, 0.8)\n\n[1] 0.262144"
  },
  {
    "objectID": "files/lectures/04-2-thinking-bayesian.html#binomial-data-model-2",
    "href": "files/lectures/04-2-thinking-bayesian.html#binomial-data-model-2",
    "title": "Thinking Like a Bayesian",
    "section": "Binomial Data Model",
    "text": "Binomial Data Model\n\nLet’s assume \\pi = 0.8.\nThe probability that he would win none of the games is approximately 0%.\n\nf(y=0|\\pi=0.8) = {6 \\choose 0} 0.8^0 (1-0.8)^{6-0}, \n\ndbinom(0, 6, 0.8)\n\n[1] 6.4e-05"
  },
  {
    "objectID": "files/lectures/04-2-thinking-bayesian.html#binomial-data-model-3",
    "href": "files/lectures/04-2-thinking-bayesian.html#binomial-data-model-3",
    "title": "Thinking Like a Bayesian",
    "section": "Binomial Data Model",
    "text": "Binomial Data Model\n\nWe want to reproduce Figure 2.5 from the Bayes Rules! textbook (from Section 2.3.2).\n\n\n\nEach group will complete the graph for a specified value of \\pi.\n\nCampus: \\pi=0.2\nZoom 1: \\pi=0.5\nZoom 2: \\pi=0.8"
  },
  {
    "objectID": "files/lectures/04-2-thinking-bayesian.html#binomial-data-model-4",
    "href": "files/lectures/04-2-thinking-bayesian.html#binomial-data-model-4",
    "title": "Thinking Like a Bayesian",
    "section": "Binomial Data Model",
    "text": "Binomial Data Model\n\nNote that the Binomial gives us the theoretical model of the data we might observe.\n\nKasparov only won one of the six games against Deep Blue in 1997 (Y=1).\n\nNext step: how compatible this particular data is with the various possible \\pi?\n\nWhat is the likelihood of Kasparov winning Y=1 game under each possible \\pi?\n\nRecall, f(y|\\pi) = L(\\pi|Y=y). When Y=1,\n\n\n\\begin{align*}\nL(\\pi | y = 1) &= f(y=1|\\pi) \\\\\n&= {6 \\choose 1} \\pi^1 (1-\\pi)^{6-1} \\\\\n&= 6\\pi(1-\\pi)^5\n\\end{align*}\n\n\nNote that we do not expect all likelihoods to sum to 1."
  },
  {
    "objectID": "files/lectures/04-2-thinking-bayesian.html#binomial-data-model-5",
    "href": "files/lectures/04-2-thinking-bayesian.html#binomial-data-model-5",
    "title": "Thinking Like a Bayesian",
    "section": "Binomial Data Model",
    "text": "Binomial Data Model\n\nUse your results from earlier to tell me the resulting likelihood values.\n\n\n\n\n\n\n\n\n\n\n\\pi\n0.2\n0.5\n0.8\n\n\n\n\nL(\\pi|y=1)"
  },
  {
    "objectID": "files/lectures/04-2-thinking-bayesian.html#binomial-data-model-6",
    "href": "files/lectures/04-2-thinking-bayesian.html#binomial-data-model-6",
    "title": "Thinking Like a Bayesian",
    "section": "Binomial Data Model",
    "text": "Binomial Data Model\n\nUse your results from earlier to tell me the resulting likelihood values.\n\n\n\n\n\\pi\n0.2\n0.5\n0.8\n\n\n\n\nL(\\pi|y=1)\n0.3932\n0.0938\n0.0015\n\n\n\n\nAs we can see, the likelihoods do not sum to 1."
  },
  {
    "objectID": "files/lectures/04-2-thinking-bayesian.html#normalizing-constant",
    "href": "files/lectures/04-2-thinking-bayesian.html#normalizing-constant",
    "title": "Thinking Like a Bayesian",
    "section": "Normalizing Constant",
    "text": "Normalizing Constant\n\nBayes’ Rule requires three pieces of information:\n\nPrior\nLikelihood\nNormalizing constant\n\nNormalizing constant: ensures that the sum of all probabilities is equal to 1.\n\nIt can be a scalar or a function.\nEvery probability distribution that does not sum to 1 will have a normalizing constant."
  },
  {
    "objectID": "files/lectures/04-2-thinking-bayesian.html#normalizing-constant-1",
    "href": "files/lectures/04-2-thinking-bayesian.html#normalizing-constant-1",
    "title": "Thinking Like a Bayesian",
    "section": "Normalizing Constant",
    "text": "Normalizing Constant\n\nWe now must determine the total probability that Kasparov would win Y=1 games across all possible win probabilities \\pi, f(y=1).\n\n\n\\begin{align*}\nf(y=1) =& \\sum_{\\pi} L(\\pi |y=1)f(\\pi) \\\\\n=& L(\\pi=0.2|y=1)f(\\pi=0.2) + L(\\pi=0.5|y=1)f(\\pi=0.5) + \\\\\n& L(\\pi=0.8|y=1)f(\\pi=0.8)\n\\end{align*}\n\n\nWork with your group to find the normalizing constant."
  },
  {
    "objectID": "files/lectures/04-2-thinking-bayesian.html#normalizing-constant-2",
    "href": "files/lectures/04-2-thinking-bayesian.html#normalizing-constant-2",
    "title": "Thinking Like a Bayesian",
    "section": "Normalizing Constant",
    "text": "Normalizing Constant\n\nWe now must determine the total probability that Kasparov would win Y=1 games across all possible win probabilities \\pi, f(y=1).\n\n\n\\begin{align*}\nf(y=1) =& \\sum_{\\pi} L(\\pi |y=1)f(\\pi) \\\\\n=& L(\\pi=0.2|y=1)f(\\pi=0.2) + L(\\pi=0.5|y=1)f(\\pi=0.5) + \\\\\n& L(\\pi=0.8|y=1)f(\\pi=0.8) \\\\\n\\approx& 0.3932 \\cdot 0.10 + 0.0938 \\cdot 0.25 + 0.0015 \\cdot 0.65 \\\\\n\\approx& 0.0637\n\\end{align*}\n\n\nAcross all possible values of \\pi, there is about a 6% chance that Kasparov would have won only one game."
  },
  {
    "objectID": "files/lectures/04-2-thinking-bayesian.html#posterior-probability-model",
    "href": "files/lectures/04-2-thinking-bayesian.html#posterior-probability-model",
    "title": "Thinking Like a Bayesian",
    "section": "Posterior Probability Model",
    "text": "Posterior Probability Model\n\nNow recall,\n\n\\text{posterior} = \\frac{\\text{prior} \\times \\text{likelihood}}{\\text{normalizing constant}}\n\nIn our example, where y = 1,\n\nf(\\pi | y=1) = \\frac{f(\\pi) L(\\pi | y = 1)}{f(y=1)} \\  \\text{for} \\ \\pi \\in \\{ 0.2, 0.5, 0.8\\}\n\nWork with your group to find the posterior probabilities.\n\nYou will have one posterior probability for each value of \\pi."
  },
  {
    "objectID": "files/lectures/04-2-thinking-bayesian.html#posterior-probability-model-1",
    "href": "files/lectures/04-2-thinking-bayesian.html#posterior-probability-model-1",
    "title": "Thinking Like a Bayesian",
    "section": "Posterior Probability Model",
    "text": "Posterior Probability Model\n\nNote!! We do not have to calculate the normalizing constant!\nWe can note that f(Y=y) = 1/c.\nThen, we say that\n\n\n\\begin{align*}\nf(\\pi | y) &= \\frac{f(\\pi) L(\\pi|y)}{f(y)} \\\\\n& \\propto  f(\\pi) L(\\pi|y) \\\\\n\\\\\n\\text{posterior} &\\propto \\text{prior} \\cdot \\text{likelihood}\n\\end{align*}"
  },
  {
    "objectID": "files/lectures/04-2-thinking-bayesian.html#wrap-up",
    "href": "files/lectures/04-2-thinking-bayesian.html#wrap-up",
    "title": "Thinking Like a Bayesian",
    "section": "Wrap Up",
    "text": "Wrap Up\n\nToday we learned how to, in general, approach Bayesian analysis.\nNext week, we will formalize what we observed today and learn about the conjugate families.\n\nBeta-binomial\nGamma-Poisson\nNormal Normal"
  },
  {
    "objectID": "files/lectures/04-2-thinking-bayesian.html#homework-practice",
    "href": "files/lectures/04-2-thinking-bayesian.html#homework-practice",
    "title": "Thinking Like a Bayesian",
    "section": "Homework / Practice",
    "text": "Homework / Practice\n\nFrom the Bayes Rules! textbook:\n\n1.3\n1.4\n1.8\n2.4\n2.9"
  },
  {
    "objectID": "files/assignments/assignment1.html",
    "href": "files/assignments/assignment1.html",
    "title": "Assignment 1: Probability Distributions",
    "section": "",
    "text": "Note: do not read too much into the following situations. The purpose is to get comfortable identifying distributions and finding corresponding probabilities.\nSituation 1: Walter is known for his intense personality during bowling league nights. Whether it’s a foot foul, a scoring dispute, or just general frustration with life, he frequently loses his temper and yells loudly at his teammates or opponents. Based on observations over several league nights, it has been determined that Walter loses his temper at an average rate of 4 outbursts per hour. His outbursts are unpredictable and can happen at any point during the game, but they seem to occur independently of each other.\na. What is the random variable of interest? (What is being measured?)\nReplace with your answer\nb. What distribution is appropriate and why?\nReplace with your answer\nc. What are the parameters of the distribution?\nReplace with your answer\nd. What is the probability that Walter will not have an outburst at all?\nReplace with your answer (i.e., don’t just show me R output here :))\ne. What is the probability that Walter has more than 10 outbursts during league night?\nReplace with your answer (i.e., don’t just show me R output here :))\nScenario 2: Donny often struggles to find parking near the bowling alley, especially on busy league nights. Sometimes he gets lucky and finds a spot fairly quickly, but other times he ends up circling the block several times before he can finally park. His parking times are generally right-skewed as it is much more common for it to take a shorter amount of time… but every now and then it can take quite a while. Over time, his friends have noticed a pattern: it usually takes him a few minutes, but there is a decent chance it could take significantly longer.\na. What is the random variable of interest? (What is being measured?)\nReplace with your answer\nb. What distribution is appropriate and why?\nReplace with your answer\nc. What are the parameters of the distribution? (Hint: for this situation, they are 3 and 4 (in that order)).\nReplace with your answer\nd. What is the probability that Donny will take more than 15 minutes to find a spot?\nReplace with your answer (i.e., don’t just show me R output here :))\ne. What is the probability that Donny will find a parking space quickly – i.e., that he parks within 5 minutes of arriving?\nReplace with your answer (i.e., don’t just show me R output here :))\nSituation 3: Maude is famous for her avant-garde dance performances that leave audiences both puzzled and mesmerized. Her performances at the local art venue are known for their spontaneity and lack of strict timing. Observers have noted that the duration of her performances can last anywhere between 15 and 30 minutes, with any length within this interval being equally likely. There’s no predicting exactly how long Maude will choose to perform on any given night… sometimes it’s a brief expression and sometimes it stretches to the edge of the audience’s patience.\na. What is the random variable of interest? (What is being measured?)\nReplace with your answer\nb. What distribution is appropriate and why?\nReplace with your answer\nc. What are the parameters of the distribution?\nReplace with your answer\nd. What is the probability that Maude’s performance will be on the shorter end – i.e., it lasts somewhere between 15 and 16 minutes?\nReplace with your answer (i.e., don’t just show me R output here :))\ne. What is the probability that Maude’s performance will be on the longer end – i.e., it lasts somewhere between 25 and 30 minutes?\nReplace with your answer (i.e., don’t just show me R output here :))\nScenario 4. The Dude always orders a White Russian when he’s at the bowling alley. However, the bowling alley bar is not always well-stocked. Each time The Dude places an order, there is only a 60% chance that the bartender has all the necessary ingredients on hand to make the drink (they are frequently out of cream). Throughout one particularly long evening of bowling and philosophical debates, The Dude orders 8 White Russians. He hopes that most of them will be successfully made.\na. What is the random variable of interest? (What is being measured?)\nReplace with your answer\nb. What distribution is appropriate and why?\nReplace with your answer\nc. What are the parameters of the distribution?\nReplace with your answer\nd. What is the probability that at least 5 drinks will be successfully made?\nReplace with your answer (i.e., don’t just show me R output here :))\ne. What is the probability that all 8 drinks are successfully made?\nReplace with your answer (i.e., don’t just show me R output here :))\nScenario 5: The Dude’s bowling games tend to last around 45 minutes, give or take. While the exact length can vary from game to game, it’s rare for his games to be extremely short or unusually long. Most of the time, his game lengths cluster fairly tightly around that 45-minute mark, though occasionally he’ll have a quicker match or one that drags on a little longer if the vibe calls for it, to the tune of \\pm 5 minutes.\na. What is the random variable of interest? (What is being measured?)\nReplace with your answer\nb. What distribution is appropriate and why?\nReplace with your answer\nc. What are the parameters of the distribution?\nReplace with your answer\nd. What is the probability that the game will last exactly 45 minutes?\nReplace with your answer (i.e., don’t just show me R output here :))\ne. What is the probability that the game will really drag on – that it lasts somewhere between 50 and 60 minutes?\nReplace with your answer (i.e., don’t just show me R output here :))\nScenario 6: After many laid-back bowling sessions (and several White Russians), The Dude has started keeping track of how often he lands a strike. He knows he’s not perfect, but he’s developed a pretty good feel for his overall strike success rate. It seems like his strike percentage tends to hang out somewhere between 60% and 80% most nights, though there’s still some uncertainty. Sometimes he’s on fire, sometimes he’s a little off, but overall he’s confident his long-term strike rate leans more toward success than failure.\na. What is the random variable of interest? (What is being measured?)\nReplace with your answer\nb. What distribution is appropriate and why?\nReplace with your answer\nc. What are the parameters of the distribution? (Hint: for this example, they are 7 and 3 (in that order)).\nReplace with your answer\nd. What is the probability that The Dude’s strike success rate is less than 50%?\nReplace with your answer (i.e., don’t just show me R output here :))\ne. What is the probability that The Dude’s strike success rate is above 80%?\nReplace with your answer (i.e., don’t just show me R output here :))"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "STA6349 - Applied Bayesian Analysis - Fall 2025",
    "section": "",
    "text": "Welcome to Applied Bayesian Analysis for Fall 2025!\nThe initial development of this course was supported by the 2023 BATS program, funded by NSF IUSE: EHR program with award numbers 2215879, 2215920, and 2215709."
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#introduction",
    "href": "files/lectures/03-2-probability-distributions.html#introduction",
    "title": "Random Variables and their Distributions",
    "section": "Introduction",
    "text": "Introduction\n\nToday we will review the statistical distributions needed for this course.\nDiscrete distributions:\n\nBinomial\nPoisson\n\nContinuous distributions:\n\nUniform\nNormal\nGamma\nBeta"
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#basic-definitions",
    "href": "files/lectures/03-2-probability-distributions.html#basic-definitions",
    "title": "Random Variables and their Distributions",
    "section": "Basic Definitions",
    "text": "Basic Definitions\n\nDiscrete random variable: a variable that can assume only a finite or countably infinite number of distinct values.\nProbability distribution of a random variable: collection of probabilities for each value of the random variable.\nNotation:\n\nUppercase letter (e.g., Y) denotes a random variable.\nLowercase letter (e.g., y) denotes a particular value that the random variable may assume.\n\nThe specific observed value, y, is not random."
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#probability-distributions-for-discrete-rv",
    "href": "files/lectures/03-2-probability-distributions.html#probability-distributions-for-discrete-rv",
    "title": "Random Variables and their Distributions",
    "section": "Probability Distributions for Discrete RV",
    "text": "Probability Distributions for Discrete RV\n\nProbability function for \\boldsymbol Y: sum of the the probabilities of all sample points in S that are assigned the value y\n\nP[Y = y] = p(y): the probability that Y takes on the value y.\n\nProbability distribution for \\boldsymbol Y: a formula, table, or graph that provides p(y) \\ \\forall \\ y.\nTheorem:\n\nFor any discrete probability distribution, the following must be true:\n\n0 \\le p(y) \\le 1 \\  \\forall \\ y\n\\sum_y p(y) = 1 \\ \\forall \\ p(y) &gt; 0."
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#expected-values-for-discrete-rv",
    "href": "files/lectures/03-2-probability-distributions.html#expected-values-for-discrete-rv",
    "title": "Random Variables and their Distributions",
    "section": "Expected Values for Discrete RV",
    "text": "Expected Values for Discrete RV\n\nExpected value: Let Y be a discrete random variable with probability function p(y). Then the expected value of Y, E[Y], is defined to be\n\n\nE(Y) = \\sum_{y} y p(y)\n\n\nWhen p(y) is an accurate characterization of the population frequency distribution, then the expected value is the population mean.\n\n\nE[Y] = \\mu"
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#expected-values-for-discrete-rv-1",
    "href": "files/lectures/03-2-probability-distributions.html#expected-values-for-discrete-rv-1",
    "title": "Random Variables and their Distributions",
    "section": "Expected Values for Discrete RV",
    "text": "Expected Values for Discrete RV\n\nTheorem:\n\nLet Y be a discrete random variable with probability function p(y) and g(Y) be a real-valued function of Y (i.e., a transformed variable). Then the expected value of g(Y) is given by\n\n\n\nE[g(Y)] = \\sum_{y} g(y) p(y)"
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#expected-values-for-discrete-rv-2",
    "href": "files/lectures/03-2-probability-distributions.html#expected-values-for-discrete-rv-2",
    "title": "Random Variables and their Distributions",
    "section": "Expected Values for Discrete RV",
    "text": "Expected Values for Discrete RV\n\nVariance: if Y is a random variable with mean E[Y] = \\mu, the variance of a random variable Y is defined to be the expected value of (Y-\\mu)^2.\n\n\nV[Y] = E\\left[ (Y-\\mu)^2 \\right]\n\n\nIf p(y) is an accurate characterization of the population frequency distribution, then V(Y) is the population variance,\n\n\nV[Y] = \\sigma^2\n\n\nStandard deviation: the positive square root of V[Y]."
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#expected-values-for-discrete-rv-3",
    "href": "files/lectures/03-2-probability-distributions.html#expected-values-for-discrete-rv-3",
    "title": "Random Variables and their Distributions",
    "section": "Expected Values for Discrete RV",
    "text": "Expected Values for Discrete RV\n\nThere is an alternative (and easier) way to calculate the variance manually,\nTheorem:\n\nLet Y be a discrete random variable with probability function p(y) and mean E[Y] = \\mu. Then,\n\n\nV[Y] = \\sigma^2 = E\\left[(Y-\\mu)^2\\right] = E\\left[Y^2\\right] - \\mu^2"
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#expected-values-for-discrete-rv-4",
    "href": "files/lectures/03-2-probability-distributions.html#expected-values-for-discrete-rv-4",
    "title": "Random Variables and their Distributions",
    "section": "Expected Values for Discrete RV",
    "text": "Expected Values for Discrete RV\n\nThe probability distribution for a random variable Y is given below.\n\n\n\n\ny\np(y)\n\n\n\n\n0\n1/8\n\n\n1\n1/4\n\n\n2\n3/8\n\n\n3\n1/4\n\n\n\n\nFind the mean of Y."
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#expected-values-for-discrete-rv-5",
    "href": "files/lectures/03-2-probability-distributions.html#expected-values-for-discrete-rv-5",
    "title": "Random Variables and their Distributions",
    "section": "Expected Values for Discrete RV",
    "text": "Expected Values for Discrete RV\n\nThe probability distribution for a random variable Y is given below.\n\n\n\n\ny\np(y)\n\n\n\n\n0\n1/8\n\n\n1\n1/4\n\n\n2\n3/8\n\n\n3\n1/4\n\n\n\n\nFind the variance and standard deviation of Y."
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#binomial-probability-distribution",
    "href": "files/lectures/03-2-probability-distributions.html#binomial-probability-distribution",
    "title": "Random Variables and their Distributions",
    "section": "Binomial Probability Distribution",
    "text": "Binomial Probability Distribution\n\nBinomial experiment:\n\nThe experiment consists of a fixed number, n, of identical trials.\nEach trial results in one of two outcomes: success (S) or failure (F).\nThe probability of success on a single trial is equal to some value p and remains the same from trial to trial.\n\nThe probability of failure is equal to q = (1-p).\n\nThe trials are independent.\nThe random variable of interest is Y, the number of successes observed during the n trials."
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#binomial-probability-distribution-1",
    "href": "files/lectures/03-2-probability-distributions.html#binomial-probability-distribution-1",
    "title": "Random Variables and their Distributions",
    "section": "Binomial Probability Distribution",
    "text": "Binomial Probability Distribution\n\nA random variable Y is said to have a binomial distribution based on n trials with success probability p iff\n\n\np(y) = {n \\choose y}p^y q^{n-y}, \\text{ where } y = 0, 1, 2, ..., n, \\text{ and } 0 \\le p \\le1\n\n\nLet Y be a binomial random variable based on n trials and success probability p. Then\n\n\nE[Y] = \\mu = np \\ \\ \\ \\text{and} \\ \\ \\ V[Y] = \\sigma^2 = npq\n\n\nSee Wackerly pg. 107 for derivation."
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#binomial-probability-distribution-2",
    "href": "files/lectures/03-2-probability-distributions.html#binomial-probability-distribution-2",
    "title": "Random Variables and their Distributions",
    "section": "Binomial Probability Distribution",
    "text": "Binomial Probability Distribution\n\nWe can use R to find information related to the binomial distribution.\n\nP[X = x]: dbinom(x, size, prob)\nP[X \\le x]: pbinom(q, size, prob)\nP[X &gt; x]: pbinom(q, size, prob, lower.tail = FALSE)\n\nIn the functions,\n\nx or q is the value of X we are interested in\nsize is the sample size (n)\nprob is the probability of success, \\pi\nlower.tail has two options:\n\nTRUE (default) returns P[X \\le x]\nFALSE returns P[X &gt; x]"
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#binomial-probability-distribution-3",
    "href": "files/lectures/03-2-probability-distributions.html#binomial-probability-distribution-3",
    "title": "Random Variables and their Distributions",
    "section": "Binomial Probability Distribution",
    "text": "Binomial Probability Distribution\n\nThe manufacturer of a dairy drink wishes to compare a new formula (B) with that of the standard formula (A). Each of four judges perform a blinded taste test and report which glass he or she most enjoyed. Suppose that the two formulas are equally attractive.\n\nWhat is the probability distribution? \nWhat is the mean of the distribution? \nWhat is the variance of the distribution?"
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#binomial-probability-distribution-4",
    "href": "files/lectures/03-2-probability-distributions.html#binomial-probability-distribution-4",
    "title": "Random Variables and their Distributions",
    "section": "Binomial Probability Distribution",
    "text": "Binomial Probability Distribution\n\nThe manufacturer of a dairy drink wishes to compare a new formula (B) with that of the standard formula (A). Each of four judges perform a blinded taste test and report which glass he or she most enjoyed. Suppose that the two formulas are equally attractive.\nUse R to find:\n\nP[X = 2]\nP[X &gt; 2]\nP[X &lt; 4]"
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#binomial-probability-distribution-5",
    "href": "files/lectures/03-2-probability-distributions.html#binomial-probability-distribution-5",
    "title": "Random Variables and their Distributions",
    "section": "Binomial Probability Distribution",
    "text": "Binomial Probability Distribution\n\nThe manufacturer of a dairy drink wishes to compare a new formula (B) with that of the standard formula (A). Each of four judges perform a blinded taste test and report which glass he or she most enjoyed. Suppose that the two formulas are equally attractive.\nUse R to find:\n\nP[X = 2]\n\n\n\ndbinom(x = 2, size = 4, prob = 0.5)\n\n[1] 0.375"
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#binomial-probability-distribution-6",
    "href": "files/lectures/03-2-probability-distributions.html#binomial-probability-distribution-6",
    "title": "Random Variables and their Distributions",
    "section": "Binomial Probability Distribution",
    "text": "Binomial Probability Distribution\n\nThe manufacturer of a dairy drink wishes to compare a new formula (B) with that of the standard formula (A). Each of four judges perform a blinded taste test and report which glass he or she most enjoyed. Suppose that the two formulas are equally attractive.\nUse R to find:\n\nP[X &gt; 2]\n\n\n\npbinom(q = 2, size = 4, prob = 0.5, lower.tail = FALSE)\n\n[1] 0.3125"
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#binomial-probability-distribution-7",
    "href": "files/lectures/03-2-probability-distributions.html#binomial-probability-distribution-7",
    "title": "Random Variables and their Distributions",
    "section": "Binomial Probability Distribution",
    "text": "Binomial Probability Distribution\n\nThe manufacturer of a dairy drink wishes to compare a new formula (B) with that of the standard formula (A). Each of four judges perform a blinded taste test and report which glass he or she most enjoyed. Suppose that the two formulas are equally attractive.\nUse R to find:\n\nP[X &lt; 4] = P[X \\le 3]\n\n\n\npbinom(q = 3, size = 4, prob = 0.5)\n\n[1] 0.9375"
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#poisson-probability-distribution",
    "href": "files/lectures/03-2-probability-distributions.html#poisson-probability-distribution",
    "title": "Random Variables and their Distributions",
    "section": "Poisson Probability Distribution",
    "text": "Poisson Probability Distribution\n\nWe often use the Poisson distribution to model count data.\nA random variable Y is said to have a Poisson probability distribution iff\n\n\np(y) = \\frac{\\lambda^y}{y!}e^{-\\lambda}, \\text{ where } y=0,1,2,..., \\text{ and } \\lambda &gt; 0\n\n\nIf Y is a random variable with a Poisson distribution with parameter \\lambda, then\n\n\nE[Y] = \\mu = \\lambda \\text{ and } V[Y] = \\sigma^2 = \\lambda\n\n\nSee Wackerly pg. 134 for derivation."
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#poisson-probability-distribution-1",
    "href": "files/lectures/03-2-probability-distributions.html#poisson-probability-distribution-1",
    "title": "Random Variables and their Distributions",
    "section": "Poisson Probability Distribution",
    "text": "Poisson Probability Distribution\n\nWe can use R to find information related to the Poisson distribution.\n\nP[X = x]: dpois(x, lambda)\n\nP[X \\le x]: ppois(q, lambda)\n\nP[X &gt; x]: ppois(q, lambda, lower.tail = FALSE)\n\nIn the functions:\n\nx or q is the value of X we are interested in\n\nlambda is the rate of occurrence\nlower.tail has two options:\n\nTRUE (default) returns P[X \\le x]\n\nFALSE returns P[X &gt; x]"
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#poisson-probability-distribution-2",
    "href": "files/lectures/03-2-probability-distributions.html#poisson-probability-distribution-2",
    "title": "Random Variables and their Distributions",
    "section": "Poisson Probability Distribution",
    "text": "Poisson Probability Distribution\n\nCustomers arrive at a checkout counter in a department store according to a Poisson distribution at an average of seven per hour.\n\nWhat is the probability distribution? \nWhat is the mean of the distribution? \nWhat is the variance of the distribution?"
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#poisson-probability-distribution-3",
    "href": "files/lectures/03-2-probability-distributions.html#poisson-probability-distribution-3",
    "title": "Random Variables and their Distributions",
    "section": "Poisson Probability Distribution",
    "text": "Poisson Probability Distribution\n\nCustomers arrive at a checkout counter in a department store according to a Poisson distribution at an average of seven per hour. Use R to find the following probabilities.\n\nNo more than three customers arrive.\nAt least two customers arrive.\nExactly five customers arrive."
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#poisson-probability-distribution-4",
    "href": "files/lectures/03-2-probability-distributions.html#poisson-probability-distribution-4",
    "title": "Random Variables and their Distributions",
    "section": "Poisson Probability Distribution",
    "text": "Poisson Probability Distribution\n\nCustomers arrive at a checkout counter in a department store according to a Poisson distribution at an average of seven per hour. Use R to find the following probabilities.\n\nNo more than three customers arrive.\n\n\n\nppois(q = 3, lambda = 7)\n\n[1] 0.08176542"
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#poisson-probability-distribution-5",
    "href": "files/lectures/03-2-probability-distributions.html#poisson-probability-distribution-5",
    "title": "Random Variables and their Distributions",
    "section": "Poisson Probability Distribution",
    "text": "Poisson Probability Distribution\n\nCustomers arrive at a checkout counter in a department store according to a Poisson distribution at an average of seven per hour. Use R to find the following probabilities.\n\nAt least two customers arrive.\n\n\n\nppois(q = 1, lambda = 7, lower.tail = FALSE)\n\n[1] 0.9927049"
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#poisson-probability-distribution-6",
    "href": "files/lectures/03-2-probability-distributions.html#poisson-probability-distribution-6",
    "title": "Random Variables and their Distributions",
    "section": "Poisson Probability Distribution",
    "text": "Poisson Probability Distribution\n\nCustomers arrive at a checkout counter in a department store according to a Poisson distribution at an average of seven per hour. Use R to find the following probabilities.\n\nExactly five customers arrive.\n\n\n\ndpois(x = 5, lambda = 7)\n\n[1] 0.1277167"
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#probability-distributions-for-continuous-rv",
    "href": "files/lectures/03-2-probability-distributions.html#probability-distributions-for-continuous-rv",
    "title": "Random Variables and their Distributions",
    "section": "Probability Distributions for Continuous RV",
    "text": "Probability Distributions for Continuous RV\n\nThe distribution function of Y (any random varaible), denoted by F(y), is such that\n\nF(y) = P[Y \\le y] \\text{ for } -\\infty &lt; y &lt; \\infty\n\nTheorem: Properties of a Distribution Function\n\nIf F(y) is a distribution function, then\n\nF(-\\infty)   \\equiv \\underset{y \\to -\\infty}{\\lim} F(y) = 0\nF(\\infty)    \\equiv \\underset{y \\to \\infty}{\\lim} F(y) = 1\nF(y) is a nondecreasing function of y.\n\nIf y_1 and y_2 are any values such that y_1 &lt; y_2, then F(y_1) \\le F(y_2)."
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#probability-distributions-for-continuous-rv-1",
    "href": "files/lectures/03-2-probability-distributions.html#probability-distributions-for-continuous-rv-1",
    "title": "Random Variables and their Distributions",
    "section": "Probability Distributions for Continuous RV",
    "text": "Probability Distributions for Continuous RV\n\nContinuous random variable:\n\nA random variable Y with distribution function F(y) is said to be continuous if F(y) is continuous for -\\infty &lt; y &lt; \\infty.\n\nIf Y is a continuous random variable, then for any real number y, P[Y = y] = 0.\n\ni.e., we must remember to find the probability of an interval."
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#probability-distributions-for-continuous-rv-2",
    "href": "files/lectures/03-2-probability-distributions.html#probability-distributions-for-continuous-rv-2",
    "title": "Random Variables and their Distributions",
    "section": "Probability Distributions for Continuous RV",
    "text": "Probability Distributions for Continuous RV\n\nProbability density function:\n\nLet F(y) be the cumulative density function for a continuous random variable, Y. Then\n\n\np[Y = y] = f(y) = \\frac{dF(y)}{dy} = F'(y).\n\nTheorem: Properties of a Density Function\n\nIf f(y) is a density function for a continuous random variable, then\n\nf(y) \\ge 0 \\ \\forall y, \\ -\\infty &lt; y &lt; \\infty.\n\\int_{-\\infty}^{\\infty} f(y) dy = 1."
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#probability-distributions-for-continuous-rv-3",
    "href": "files/lectures/03-2-probability-distributions.html#probability-distributions-for-continuous-rv-3",
    "title": "Random Variables and their Distributions",
    "section": "Probability Distributions for Continuous RV",
    "text": "Probability Distributions for Continuous RV\n\nCumulative density function:\n\nLet f(y) be the probability density function for a continuous random variable, Y. Then\n\n\nP[Y \\le y] = F(y) = \\int_{-\\infty}^y f(t) dt, \\text{ for } y \\in {\\rm I\\!R}."
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#probability-distributions-for-continuous-rv-4",
    "href": "files/lectures/03-2-probability-distributions.html#probability-distributions-for-continuous-rv-4",
    "title": "Random Variables and their Distributions",
    "section": "Probability Distributions for Continuous RV",
    "text": "Probability Distributions for Continuous RV\n\nTheorem\n\nIf the random variable Y has density function f(y) and a &lt; b, then the probability that Y falls in the interval [a, b] is\n\n\n\nP[a \\le Y \\le b] = \\int_a^b f(y) dy."
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#expected-values-for-continuous-rv",
    "href": "files/lectures/03-2-probability-distributions.html#expected-values-for-continuous-rv",
    "title": "Random Variables and their Distributions",
    "section": "Expected Values for Continuous RV",
    "text": "Expected Values for Continuous RV\n\nExpected value:\n\nThe expected value of a continuous variable Y is\n\n\n\nE[Y] = \\int_{-\\infty}^{\\infty} y f(y) \\ dy\n\n\nThis is the continuous version of the expected value for a discrete random variable,\n\n\nE[Y] = \\sum_y y p(y)"
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#expected-values-for-continuous-rv-1",
    "href": "files/lectures/03-2-probability-distributions.html#expected-values-for-continuous-rv-1",
    "title": "Random Variables and their Distributions",
    "section": "Expected Values for Continuous RV",
    "text": "Expected Values for Continuous RV\n\nTheorem:\n\nLet g(Y) be a function of Y; then the expected value of g(Y) is given by\n\n\n\nE\\left[ g(Y) \\right] = \\int_{-\\infty}^{\\infty} g(y) f(y) \\ dy\n\n\nTheorem:\n\nLet c be a constant and let g(Y), g_1(Y), g_2(Y), …, g_k(Y) be functions of a continuous random variable, Y. Then the following results hold:\n\nE[c] = c\nE\\left[cg(Y)] = cE[g(Y)\\right]\nE\\left[g_1(Y)+...+g_k(Y)\\right] = E\\left[ g_1(Y) \\right] + ... + E\\left[ g_k(Y) \\right]"
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#uniform-probability-distribution",
    "href": "files/lectures/03-2-probability-distributions.html#uniform-probability-distribution",
    "title": "Random Variables and their Distributions",
    "section": "Uniform Probability Distribution",
    "text": "Uniform Probability Distribution\n\nUniform Distribution"
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#uniform-probability-distribution-1",
    "href": "files/lectures/03-2-probability-distributions.html#uniform-probability-distribution-1",
    "title": "Random Variables and their Distributions",
    "section": "Uniform Probability Distribution",
    "text": "Uniform Probability Distribution\n\nA random variable Y is said to have a uniform distribution iff\n\n\nf(y) = \\frac{1}{\\theta_2 - \\theta_1}, \\ \\theta_1 \\le y \\le \\theta_2\n\n\nIf \\theta_1 &lt; \\theta_2 and Y is a uniformly distributed r.v. on the interval (\\theta_1, \\theta_2), then\n\n\nE[Y] = \\mu = \\frac{\\theta_1+\\theta_2}{2} \\ \\ \\ \\text{and} \\ \\ \\ V[Y] = \\sigma^2 = \\frac{(\\theta_2-\\theta_1)^2}{12}\n\n\nSee Wackerly pg. 176 for derivation."
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#uniform-probability-distribution-2",
    "href": "files/lectures/03-2-probability-distributions.html#uniform-probability-distribution-2",
    "title": "Random Variables and their Distributions",
    "section": "Uniform Probability Distribution",
    "text": "Uniform Probability Distribution\n\nAn industrial psychologist has determined that it takes a worker between 9 and 15 minutes to complete a task on an automobile assembly line. If the time to complete the task is uniformly distributed over the interval 9 \\le y \\le 15, then determine:\n\nThe probability distribution. \nThe mean of the distribution. \nThe variance and standard deviation of the distribution."
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#uniform-probability-distribution-3",
    "href": "files/lectures/03-2-probability-distributions.html#uniform-probability-distribution-3",
    "title": "Random Variables and their Distributions",
    "section": "Uniform Probability Distribution",
    "text": "Uniform Probability Distribution\n\nWe can use R to find information related to the uniform distribution:\n\nP[X \\le x]: punif(q, min, max)\n\nP[X \\ge x]: punif(q, min, max, lower.tail = FALSE)\n\nIn the functions:\n\nq is the value of X we are interested in\n\nmin is the lower bound of the distribution\n\nmax is the upper bound of the distribution\n\nlower.tail has two options:\n\nTRUE (default) returns P[X \\le x]\n\nFALSE returns P[X \\ge x]"
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#uniform-probability-distribution-4",
    "href": "files/lectures/03-2-probability-distributions.html#uniform-probability-distribution-4",
    "title": "Random Variables and their Distributions",
    "section": "Uniform Probability Distribution",
    "text": "Uniform Probability Distribution\n\nAn industrial psychologist has determined that it takes a worker between 9 and 15 minutes to complete a task on an automobile assembly line. If the time to complete the task is uniformly distributed over the interval 9 \\le y \\le 15, then determine the following probabilities:\n\nA worker takes fewer than 13 minutes.\nA worker takes at least 11 minutes.\nA worker takes between 14 and 15 minutes."
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#uniform-probability-distribution-5",
    "href": "files/lectures/03-2-probability-distributions.html#uniform-probability-distribution-5",
    "title": "Random Variables and their Distributions",
    "section": "Uniform Probability Distribution",
    "text": "Uniform Probability Distribution\n\nAn industrial psychologist has determined that it takes a worker between 9 and 15 minutes to complete a task on an automobile assembly line. If the time to complete the task is uniformly distributed over the interval 9 \\le y \\le 15, then determine the following probabilities:\n\nA worker takes fewer than 13 minutes.\n\n\n\npunif(13, 9, 15)\n\n[1] 0.6666667"
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#uniform-probability-distribution-6",
    "href": "files/lectures/03-2-probability-distributions.html#uniform-probability-distribution-6",
    "title": "Random Variables and their Distributions",
    "section": "Uniform Probability Distribution",
    "text": "Uniform Probability Distribution\n\nAn industrial psychologist has determined that it takes a worker between 9 and 15 minutes to complete a task on an automobile assembly line. If the time to complete the task is uniformly distributed over the interval 9 \\le y \\le 15, then determine the following probabilities:\n\nA worker takes at least 11 minutes.\n\n\n\npunif(11, 9, 15, lower.tail = TRUE)\n\n[1] 0.3333333"
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#uniform-probability-distribution-7",
    "href": "files/lectures/03-2-probability-distributions.html#uniform-probability-distribution-7",
    "title": "Random Variables and their Distributions",
    "section": "Uniform Probability Distribution",
    "text": "Uniform Probability Distribution\n\nAn industrial psychologist has determined that it takes a worker between 9 and 15 minutes to complete a task on an automobile assembly line. If the time to complete the task is uniformly distributed over the interval 9 \\le y \\le 15, then determine the following probabilities:\n\nA worker takes between 14 and 15 minutes.\n\n\n\npunif(15, 9, 15) - punif(14, 9, 15)\n\n[1] 0.1666667"
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#normal-probability-distribution",
    "href": "files/lectures/03-2-probability-distributions.html#normal-probability-distribution",
    "title": "Random Variables and their Distributions",
    "section": "Normal Probability Distribution",
    "text": "Normal Probability Distribution\n\nNormal Distribution"
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#normal-probability-distribution-1",
    "href": "files/lectures/03-2-probability-distributions.html#normal-probability-distribution-1",
    "title": "Random Variables and their Distributions",
    "section": "Normal Probability Distribution",
    "text": "Normal Probability Distribution\n\nA random variable Y is said to have a normal distribution iff, for \\sigma &gt; 0 and -\\infty &lt; \\mu &lt; \\infty,\n\n\nf(y) = \\frac{1}{\\sigma \\sqrt{2\\pi}} e^{-(y-\\mu)^2/(2\\sigma^2)}\n\n\nIf Y is a random variable normally distributed with parameters \\mu and \\sigma, then\n\n\nE[Y] = \\mu =  \\ \\ \\ \\text{and} \\ \\ \\ V[Y] = \\sigma^2"
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#normal-probability-distribution-2",
    "href": "files/lectures/03-2-probability-distributions.html#normal-probability-distribution-2",
    "title": "Random Variables and their Distributions",
    "section": "Normal Probability Distribution",
    "text": "Normal Probability Distribution\n\nWe can use R to find information related to the normal distribution.\n\nP[X \\le x]: pnorm(q, mean, sd)\n\nP[X \\ge x]: pnorm(q, mean, sd, lower.tail = FALSE)\n\nIn the functions:\n\nq is the value of X we are interested in\n\nmean is the population mean \\mu\n\nsd is the standard deviation \\sigma\n\nlower.tail has two options:\n\nTRUE (default) returns P[X \\le x]\n\nFALSE returns P[X \\ge x]"
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#normal-probability-distribution-3",
    "href": "files/lectures/03-2-probability-distributions.html#normal-probability-distribution-3",
    "title": "Random Variables and their Distributions",
    "section": "Normal Probability Distribution",
    "text": "Normal Probability Distribution\n\nA random variable Y is said to have a standard normal distribution iff\n\n\nY \\sim N(\\mu=0,\\sigma=1)\n\n\nThe normal distribution is then simplified to\n\n\nf(y) = \\frac{1}{\\sqrt{2\\pi}} e^{-y^2/2}\n\n\nNote that in all cases of the normal distribution, we assume -\\infty &lt; y &lt; \\infty."
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#normal-probability-distribution-4",
    "href": "files/lectures/03-2-probability-distributions.html#normal-probability-distribution-4",
    "title": "Random Variables and their Distributions",
    "section": "Normal Probability Distribution",
    "text": "Normal Probability Distribution\n\nWhen using pnorm(), the default values for mean and sd are 1 and 0.\nThus, if we have the standard normal our R functions simplify to:\n\nP[Z \\le z]: pnorm(z)\n\nP[Z \\ge z]: pnorm(z, lower.tail = FALSE)\n\nIn the functions:\n\nq is the z-score value of interest\n\nlower.tail = TRUE returns P[Z \\le z]\n\nlower.tail = FALSE returns P[Z \\ge z]"
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#normal-probability-distribution-5",
    "href": "files/lectures/03-2-probability-distributions.html#normal-probability-distribution-5",
    "title": "Random Variables and their Distributions",
    "section": "Normal Probability Distribution",
    "text": "Normal Probability Distribution\n\nA geneticist working for a seed company develops a new carrot for growing in heavy clay soil. After measuring 5000 of these carrots, it can be said that the carrot length, Y, is normally distributed with \\mu = 11.5 cm and \\sigma = 1.15 cm. Determine\n\nThe probability distribution. \nThe mean of the distribution. \nThe variance and standard deviation of the distribution."
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#normal-probability-distribution-6",
    "href": "files/lectures/03-2-probability-distributions.html#normal-probability-distribution-6",
    "title": "Random Variables and their Distributions",
    "section": "Normal Probability Distribution",
    "text": "Normal Probability Distribution\n\nA geneticist working for a seed company develops a new carrot for growing in heavy clay soil. After measuring 5000 of these carrots, it can be said that the carrot length, Y, is normally distributed with \\mu = 11.5 cm and \\sigma = 1.15 cm.\n\nWhat is the probability that a carrot will be between 10 and 13 cm?\nWhat is the probability that a carrot will be less than 9 cm?\nWhat is the probability that a carrot will be 12 cm or larger?"
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#normal-probability-distribution-7",
    "href": "files/lectures/03-2-probability-distributions.html#normal-probability-distribution-7",
    "title": "Random Variables and their Distributions",
    "section": "Normal Probability Distribution",
    "text": "Normal Probability Distribution\n\nA geneticist working for a seed company develops a new carrot for growing in heavy clay soil. After measuring 5000 of these carrots, it can be said that the carrot length, Y, is normally distributed with \\mu = 11.5 cm and \\sigma = 1.15 cm.\n\nWhat is the probability that a carrot will be between 10 and 13 cm?\n\n\n\npnorm(q = 13, mean = 11.5, sd = 1.15) - pnorm(q = 10, mean = 11.5, sd = 1.15)\n\n[1] 0.807885"
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#normal-probability-distribution-8",
    "href": "files/lectures/03-2-probability-distributions.html#normal-probability-distribution-8",
    "title": "Random Variables and their Distributions",
    "section": "Normal Probability Distribution",
    "text": "Normal Probability Distribution\n\nA geneticist working for a seed company develops a new carrot for growing in heavy clay soil. After measuring 5000 of these carrots, it can be said that the carrot length, Y, is normally distributed with \\mu = 11.5 cm and \\sigma = 1.15 cm.\n\nWhat is the probability that a carrot will be less than 9 cm?\n\n\n\npnorm(q = 9, mean = 11.5, sd = 1.15)\n\n[1] 0.01485583"
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#normal-probability-distribution-9",
    "href": "files/lectures/03-2-probability-distributions.html#normal-probability-distribution-9",
    "title": "Random Variables and their Distributions",
    "section": "Normal Probability Distribution",
    "text": "Normal Probability Distribution\n\nA geneticist working for a seed company develops a new carrot for growing in heavy clay soil. After measuring 5000 of these carrots, it can be said that the carrot length, Y, is normally distributed with \\mu = 11.5 cm and \\sigma = 1.15 cm.\n\nWhat is the probability that a carrot will be 12 cm or larger?\n\n\n\npnorm(q = 12, mean = 11.5, sd = 1.15, lower.tail = FALSE)\n\n[1] 0.3318601"
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#gamma-probability-distribution",
    "href": "files/lectures/03-2-probability-distributions.html#gamma-probability-distribution",
    "title": "Random Variables and their Distributions",
    "section": "Gamma Probability Distribution",
    "text": "Gamma Probability Distribution\n\nGamma Distribution"
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#gamma-probability-distribution-1",
    "href": "files/lectures/03-2-probability-distributions.html#gamma-probability-distribution-1",
    "title": "Random Variables and their Distributions",
    "section": "Gamma Probability Distribution",
    "text": "Gamma Probability Distribution\n\nA random variable Y is said to have a gamma distribution with parameters \\alpha &gt; 0 and \\beta &gt; 0 iff,\n\n\nf(y) = \\frac{y^{\\alpha-1} e^{-y/\\beta}}{\\beta^{\\alpha} \\Gamma(\\alpha)}, \\ 0 \\le y &lt; \\infty\n\n\nNote that \\Gamma(\\alpha) = \\int_{0}^{\\infty} y^{\\alpha-1} e^{-y} \\ dy.\nIf Y has a gamma distribution with parameters \\alpha and \\beta, then\n\n\nE[Y] = \\mu = \\alpha\\beta \\ \\ \\ \\text{and} \\ \\ \\ V[Y] = \\sigma^2  = \\alpha\\beta^2\n\n\nSee Wackerly pg. 187 for derivation."
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#gamma-probability-distribution-2",
    "href": "files/lectures/03-2-probability-distributions.html#gamma-probability-distribution-2",
    "title": "Random Variables and their Distributions",
    "section": "Gamma Probability Distribution",
    "text": "Gamma Probability Distribution\n\nWe can use R to find information related to the Gamma distribution.\n\nP[X \\le x]: pgamma(q, shape, rate)\n\nP[X \\ge x]: pgamma(q, shape, rate, lower.tail = FALSE)\n\nIn the functions:\n\nq is the value of X we are interested in\n\nshape is the shape parameter, \\alpha\n\nscale is the scale parameter, \\beta\n\nAlternatively, can parameterize with rate = 1/\\beta, rate = 1 / scale\n\nlower.tail has two options:\n\nTRUE (default) returns P[X \\le x]\n\nFALSE returns P[X \\ge x]"
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#gamma-probability-distribution-3",
    "href": "files/lectures/03-2-probability-distributions.html#gamma-probability-distribution-3",
    "title": "Random Variables and their Distributions",
    "section": "Gamma Probability Distribution",
    "text": "Gamma Probability Distribution\n\nAnnual incomes for heads of household in an affluent section of a city have approximately a gamma distribution with \\alpha=32 and \\beta=2500. Determine:\n\nThe probability distribution. \nThe mean of the distribution. \nThe variance and standard deviation of the distribution."
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#gamma-probability-distribution-4",
    "href": "files/lectures/03-2-probability-distributions.html#gamma-probability-distribution-4",
    "title": "Random Variables and their Distributions",
    "section": "Gamma Probability Distribution",
    "text": "Gamma Probability Distribution\n\nAnnual incomes for heads of household in an affluent section of a city have approximately a gamma distribution with \\alpha=32 and \\beta=2500.\n\nWhat proportion have incomes in excess of $100,000?\nWhat proportion have incomes between $75,000 and $150,000?"
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#gamma-probability-distribution-5",
    "href": "files/lectures/03-2-probability-distributions.html#gamma-probability-distribution-5",
    "title": "Random Variables and their Distributions",
    "section": "Gamma Probability Distribution",
    "text": "Gamma Probability Distribution\n\nAnnual incomes for heads of household in a section of a city have approximately a gamma distribution with \\alpha=32 and \\beta=2500.\n\nWhat proportion have incomes in excess of $30,000?\n\n\n\npgamma(q = 100000, shape = 32, scale = 2500, lower.tail = FALSE)\n\n[1] 0.08552057"
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#gamma-probability-distribution-6",
    "href": "files/lectures/03-2-probability-distributions.html#gamma-probability-distribution-6",
    "title": "Random Variables and their Distributions",
    "section": "Gamma Probability Distribution",
    "text": "Gamma Probability Distribution\n\nAnnual incomes for heads of household in an affluent section of a city have approximately a gamma distribution with \\alpha=32 and \\beta=2500.\n\nWhat proportion have incomes between $75,000 and $150,000?\n\n\n\npgamma(q = 150000, shape = 32, scale = 2500) - pgamma(q = 75000, shape = 32, scale = 2500)\n\n[1] 0.6186147"
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#beta-probability-distribution",
    "href": "files/lectures/03-2-probability-distributions.html#beta-probability-distribution",
    "title": "Random Variables and their Distributions",
    "section": "Beta Probability Distribution",
    "text": "Beta Probability Distribution\n\nBeta Distribution"
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#beta-probability-distribution-1",
    "href": "files/lectures/03-2-probability-distributions.html#beta-probability-distribution-1",
    "title": "Random Variables and their Distributions",
    "section": "Beta Probability Distribution",
    "text": "Beta Probability Distribution\n\nA random variable Y is said to have a beta distribution with parameters \\alpha &gt; 0 and \\beta &gt; 0 iff,\n\n\nf(y) = \\frac{y^{\\alpha-1}(1-y)^{\\beta-1}}{B(\\alpha,\\beta)}, \\ 0 \\le y \\le 1\n\n\nNote: B(\\alpha,\\beta) = \\int_0^1 y^{\\alpha-1}(1-y)^{\\beta-1} \\ dy = \\frac{\\Gamma(\\alpha) \\Gamma(\\beta)}{\\Gamma(\\alpha+\\beta)}.\nIf Y has a beta distribution with parameters \\alpha &gt; 0 and \\beta &gt; 0, then\n\n\nE[Y] = \\mu = \\frac{\\alpha}{\\alpha+\\beta} \\ \\ \\ \\text{and} \\ \\ \\ V[Y] = \\sigma^2  = \\frac{\\alpha\\beta}{(\\alpha+\\beta)^2(\\alpha+\\beta+1)}"
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#beta-probability-distribution-2",
    "href": "files/lectures/03-2-probability-distributions.html#beta-probability-distribution-2",
    "title": "Random Variables and their Distributions",
    "section": "Beta Probability Distribution",
    "text": "Beta Probability Distribution\n\nWe can use R to find information related to the Beta distribution.\n\nP[X \\le x]: pbeta(q, shape1, shape2)\n\nP[X \\ge x]: pbeta(q, shape1, shape2, lower.tail = FALSE)\n\nIn the functions:\n\nq is the value of X we are interested in – must be in [0, 1]!\nshape1 is the first shape parameter, \\alpha\n\nshape2 is the second shape parameter, \\beta\n\nlower.tail has two options:\n\nTRUE (default) returns P[X \\le x]\n\nFALSE returns P[X \\ge x]"
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#beta-probability-distribution-3",
    "href": "files/lectures/03-2-probability-distributions.html#beta-probability-distribution-3",
    "title": "Random Variables and their Distributions",
    "section": "Beta Probability Distribution",
    "text": "Beta Probability Distribution\n\nIn a survey of cupcake preferences, 8 respondents liked the new cupcake flavor and 2 did not. We will model the proportion of all respondents who would like the cupcake flavor using a Beta distribution with \\alpha = 8 and \\beta = 2. Determine:\n\nThe probability distribution. \nThe mean of the distribution. \nThe variance and standard deviation of the distribution."
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#beta-probability-distribution-4",
    "href": "files/lectures/03-2-probability-distributions.html#beta-probability-distribution-4",
    "title": "Random Variables and their Distributions",
    "section": "Beta Probability Distribution",
    "text": "Beta Probability Distribution\n\nIn a survey of cupcake preferences, 8 respondents liked the new cupcake flavor and 2 did not. We will model the proportion of all respondents who would like the cupcake flavor using a Beta distribution with \\alpha = 8 and \\beta = 2. What is the probability that:\n\nfewer than 60% of respondents like the new flavor?\nmore than 90% of respondents like the new flavor?\nsomewhere between 70% and 90% of respondents like the new flavor?"
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#beta-probability-distribution-5",
    "href": "files/lectures/03-2-probability-distributions.html#beta-probability-distribution-5",
    "title": "Random Variables and their Distributions",
    "section": "Beta Probability Distribution",
    "text": "Beta Probability Distribution\n\nIn a survey of cupcake preferences, 8 respondents liked the new cupcake flavor and 2 did not. We will model the proportion of all respondents who would like the cupcake flavor using a Beta distribution with \\alpha = 8 and \\beta = 2. What is the probability that:\n\nfewer than 60% of respondents like the new flavor?\n\n\n\npbeta(q = 0.6, shape1 = 8, shape2 = 2)\n\n[1] 0.07054387"
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#beta-probability-distribution-6",
    "href": "files/lectures/03-2-probability-distributions.html#beta-probability-distribution-6",
    "title": "Random Variables and their Distributions",
    "section": "Beta Probability Distribution",
    "text": "Beta Probability Distribution\n\nIn a survey of cupcake preferences, 8 respondents liked the new cupcake flavor and 2 did not. We will model the proportion of all respondents who would like the cupcake flavor using a Beta distribution with \\alpha = 8 and \\beta = 2. What is the probability that:\n\nmore than 90% of respondents like the new flavor?\n\n\n\npbeta(q = 0.9, shape1 = 8, shape2 = 2, lower.tail = FALSE)\n\n[1] 0.225159"
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#beta-probability-distribution-7",
    "href": "files/lectures/03-2-probability-distributions.html#beta-probability-distribution-7",
    "title": "Random Variables and their Distributions",
    "section": "Beta Probability Distribution",
    "text": "Beta Probability Distribution\n\nIn a survey of cupcake preferences, 8 respondents liked the new cupcake flavor and 2 did not. We will model the proportion of all respondents who would like the cupcake flavor using a Beta distribution with \\alpha = 8 and \\beta = 2. What is the probability that:\n\nsomewhere between 70% and 90% of respondents like the new flavor?\n\n\n\npbeta(q = 0.9, shape1 = 8, shape2 = 2) - pbeta(q = 0.7, shape1 = 8, shape2 = 2)\n\n[1] 0.5788377"
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#wrap-up",
    "href": "files/lectures/03-2-probability-distributions.html#wrap-up",
    "title": "Random Variables and their Distributions",
    "section": "Wrap Up",
    "text": "Wrap Up\n\nYou now are ready to work on Assignment 1: Probability Distributions.\n\nDue Tuesday, September 16.\n.qmd file is available to download on Canvas.\n\nGoals:\n\nIdentify the applicable distribution.\nFind probabilities using said distributions.\n\nNext week:\n\nMonday: Thinking like a Bayesian\nWednesday: Beta-Binomial"
  },
  {
    "objectID": "files/lectures/Untitled.html#introduction",
    "href": "files/lectures/Untitled.html#introduction",
    "title": "Thinking Like a Bayesian",
    "section": "Introduction",
    "text": "Introduction\n\nBefore today:\n\nRefresher on probability theory\nWhat does each distribution do?\n\nbeta \\to outcomes limited to [0, 1]\nbinomial \\to binary outcomes\ngamma \\to continuous & positive outcomes; skewed right\nnormal \\to continuous outcomes; mound-shaped & symmetric distribution\nPoisson \\to count outcomes; skewed right\nuniform \\to each outcome has equal probability; rectangular distribution\n\n\nToday: building up Bayesian analysis concepts"
  },
  {
    "objectID": "files/lectures/Untitled.html#thinking-like-a-bayesian",
    "href": "files/lectures/Untitled.html#thinking-like-a-bayesian",
    "title": "Thinking Like a Bayesian",
    "section": "Thinking Like a Bayesian",
    "text": "Thinking Like a Bayesian\n\nBayesian analysis involves updating beliefs based on observed data."
  },
  {
    "objectID": "files/lectures/Untitled.html#thinking-like-a-bayesian-1",
    "href": "files/lectures/Untitled.html#thinking-like-a-bayesian-1",
    "title": "Thinking Like a Bayesian",
    "section": "Thinking Like a Bayesian",
    "text": "Thinking Like a Bayesian\n\nThis is the natural Bayesian knowledge-building process of:\n\nacknowledging your preconceptions (prior distribution),\nusing data (data distribution) to update your knowledge (posterior distribution), and\nrepeating (posterior distribution \\to new prior distribution)"
  },
  {
    "objectID": "files/lectures/Untitled.html#thinking-like-a-bayesian-2",
    "href": "files/lectures/Untitled.html#thinking-like-a-bayesian-2",
    "title": "Thinking Like a Bayesian",
    "section": "Thinking Like a Bayesian",
    "text": "Thinking Like a Bayesian\n\nBayesian and frequentist analyses share a common goal: to learn from data about the world around us.\n\nBoth Bayesian and frequentist analyses use data to fit models, make predictions, and evaluate hypotheses.\nWhen working with the same data, they will typically produce a similar set of conclusions.\n\nStatisticians typically identify as either a “Bayesian” or “frequentist” …\n\n🚫 We are not going to “take sides.”\n✅ We will see these as tools in our toolbox."
  },
  {
    "objectID": "files/lectures/Untitled.html#thinking-like-a-bayesian-3",
    "href": "files/lectures/Untitled.html#thinking-like-a-bayesian-3",
    "title": "Thinking Like a Bayesian",
    "section": "Thinking Like a Bayesian",
    "text": "Thinking Like a Bayesian\n\nBayesian probability: the relative plausibility of an event.\n\nConsiders prior belief."
  },
  {
    "objectID": "files/lectures/Untitled.html#thinking-like-a-bayesian-4",
    "href": "files/lectures/Untitled.html#thinking-like-a-bayesian-4",
    "title": "Thinking Like a Bayesian",
    "section": "Thinking Like a Bayesian",
    "text": "Thinking Like a Bayesian\n\nFrequentist probability: the long-run relative frequency of a repeatable event.\n\nDoes not consider prior belief."
  },
  {
    "objectID": "files/lectures/Untitled.html#thinking-like-a-bayesian-5",
    "href": "files/lectures/Untitled.html#thinking-like-a-bayesian-5",
    "title": "Thinking Like a Bayesian",
    "section": "Thinking Like a Bayesian",
    "text": "Thinking Like a Bayesian\n\nThe Bayesian framework depends upon prior information, data, and the balance between them.\n\nThe balance between the prior information and data is determined by the relative strength of each\n\n\n\n\nWhen we have little data, our posterior can rely more on prior knowledge.\nAs we collect more data, the prior can lose its influence."
  },
  {
    "objectID": "files/lectures/Untitled.html#thinking-like-a-bayesian-6",
    "href": "files/lectures/Untitled.html#thinking-like-a-bayesian-6",
    "title": "Thinking Like a Bayesian",
    "section": "Thinking Like a Bayesian",
    "text": "Thinking Like a Bayesian\n\nWe can also use this approach to combine analysis results."
  },
  {
    "objectID": "files/lectures/Untitled.html#thinking-like-a-bayesian-7",
    "href": "files/lectures/Untitled.html#thinking-like-a-bayesian-7",
    "title": "Thinking Like a Bayesian",
    "section": "Thinking Like a Bayesian",
    "text": "Thinking Like a Bayesian\n\nWe will use an example to work through Bayesian logic.\nThe Collins Dictionary named “fake news” the 2017 term of the year.\n\nFake, misleading, and biased news has proliferated along with online news and social media platforms which allow users to post articles with little quality control.\n\nWe want to flag articles as “real” or “fake.”\nWe’ll examine a sample of 150 articles which were posted on Facebook and fact checked by five BuzzFeed journalists (Shu et al. 2017)."
  },
  {
    "objectID": "files/lectures/Untitled.html#thinking-like-a-bayesian-8",
    "href": "files/lectures/Untitled.html#thinking-like-a-bayesian-8",
    "title": "Thinking Like a Bayesian",
    "section": "Thinking Like a Bayesian",
    "text": "Thinking Like a Bayesian\n\nInformation about each article is stored in the fake_news dataset in the bayesrules package.\n\n\nfake_news &lt;- bayesrules::fake_news\nprint(colnames(fake_news))\n\n [1] \"title\"                   \"text\"                   \n [3] \"url\"                     \"authors\"                \n [5] \"type\"                    \"title_words\"            \n [7] \"text_words\"              \"title_char\"             \n [9] \"text_char\"               \"title_caps\"             \n[11] \"text_caps\"               \"title_caps_percent\"     \n[13] \"text_caps_percent\"       \"title_excl\"             \n[15] \"text_excl\"               \"title_excl_percent\"     \n[17] \"text_excl_percent\"       \"title_has_excl\"         \n[19] \"anger\"                   \"anticipation\"           \n[21] \"disgust\"                 \"fear\"                   \n[23] \"joy\"                     \"sadness\"                \n[25] \"surprise\"                \"trust\"                  \n[27] \"negative\"                \"positive\"               \n[29] \"text_syllables\"          \"text_syllables_per_word\""
  },
  {
    "objectID": "files/lectures/Untitled.html#thinking-like-a-bayesian-9",
    "href": "files/lectures/Untitled.html#thinking-like-a-bayesian-9",
    "title": "Thinking Like a Bayesian",
    "section": "Thinking Like a Bayesian",
    "text": "Thinking Like a Bayesian\n\nWe could build a simple news filter which uses the following rule: since most articles are real, we should read and believe all articles.\n\nWhile this filter would solve the problem of disregarding real articles, we would read lots of fake news.\nIt also only takes into account the overall rates of, not the typical features of, real and fake news."
  },
  {
    "objectID": "files/lectures/Untitled.html#thinking-like-a-bayesian-10",
    "href": "files/lectures/Untitled.html#thinking-like-a-bayesian-10",
    "title": "Thinking Like a Bayesian",
    "section": "Thinking Like a Bayesian",
    "text": "Thinking Like a Bayesian\n\nSuppose that the most recent article posted to a social media platform is titled: The president has a funny secret!\n\nSome features of this title probably set off some red flags.\nFor example, the usage of an exclamation point might seem like an odd choice for a real news article.\n\nIn the dataset, what is the split of real and fake articles?"
  },
  {
    "objectID": "files/lectures/Untitled.html#thinking-like-a-bayesian-11",
    "href": "files/lectures/Untitled.html#thinking-like-a-bayesian-11",
    "title": "Thinking Like a Bayesian",
    "section": "Thinking Like a Bayesian",
    "text": "Thinking Like a Bayesian\n\nIn the dataset, what is the split of real and fake articles?\n\n\n\n\n  \n\n\n\n\nOur data backs up our instinct on the article,\n\n\n\n\n  \n\n\n\n\nIn this dataset, 26.67% (16 of 60) of fake news titles but only 2.22% (2 of 90) of real news titles use an exclamation point."
  },
  {
    "objectID": "files/lectures/Untitled.html#thinking-like-a-bayesian-12",
    "href": "files/lectures/Untitled.html#thinking-like-a-bayesian-12",
    "title": "Thinking Like a Bayesian",
    "section": "Thinking Like a Bayesian",
    "text": "Thinking Like a Bayesian\n\nWe now have two pieces of contradictory information.\n\nOur prior information suggested that incoming articles are most likely real.\nHowever, the exclamation point data is more consistent with fake news.\n\n\n\n\nThinking like Bayesians, we know that balancing both pieces of information is important in developing a posterior understanding of whether the article is fake."
  },
  {
    "objectID": "files/lectures/Untitled.html#building-a-bayesian-model",
    "href": "files/lectures/Untitled.html#building-a-bayesian-model",
    "title": "Thinking Like a Bayesian",
    "section": "Building a Bayesian Model",
    "text": "Building a Bayesian Model\n\nOur fake news analysis studies two variables:\n\nan article’s fake vs real status and\nits use of exclamation points.\n\nWe can represent the randomness in these variables using probability models.\nWe will now build:\n\na prior probability model for our prior understanding of whether the most recent article is fake;\na model for interpreting the exclamation point data; and, eventually,\na posterior probability model which summarizes the posterior plausibility that the article is fake."
  },
  {
    "objectID": "files/lectures/Untitled.html#building-a-bayesian-model-1",
    "href": "files/lectures/Untitled.html#building-a-bayesian-model-1",
    "title": "Thinking Like a Bayesian",
    "section": "Building a Bayesian Model",
    "text": "Building a Bayesian Model\n\nLet’s now formalize our prior understanding of whether the new article is fake.\nBased on our fake_news data, we saw that 40% of articles are fake and 60% are real.\n\nBefore reading the new article, there’s a 0.4 prior probability that it’s fake and a 0.6 prior probability it’s not.\n\n\nP\\left[B\\right] = 0.40 \\text{ and } P\\left[B\\right] = 0.40\n\nRemember that a valid probability model must:\n\naccount for all possible events (all articles must be fake or real);\nit assigns prior probabilities to each event; and\nthe probabilities sum to one."
  },
  {
    "objectID": "files/lectures/Untitled.html#building-a-bayesian-model-2",
    "href": "files/lectures/Untitled.html#building-a-bayesian-model-2",
    "title": "Thinking Like a Bayesian",
    "section": "Building a Bayesian Model",
    "text": "Building a Bayesian Model\n\n\n\n\n\n\n\n\ntitle_has_excl\nfake\nreal\n\n\n\n\nFALSE\n73.3% (44)\n97.8% (88)\n\n\nTRUE\n26.7% (16)\n2.2% (2)\n\n\n\n\n\n\n\n\nWe have the following conditional probabilities:\n\nIf an article is fake (B), then there’s a roughly 26.67% chance it uses exclamation points in the title.\nIf an article is real (B^c), then there’s only a roughly 2.22% chance it uses exclamation points."
  },
  {
    "objectID": "files/lectures/Untitled.html#building-a-bayesian-model-3",
    "href": "files/lectures/Untitled.html#building-a-bayesian-model-3",
    "title": "Thinking Like a Bayesian",
    "section": "Building a Bayesian Model",
    "text": "Building a Bayesian Model\n\nWe have the following conditional probabilities:\n\nIf an article is fake (B), then there’s a roughly 26.67% chance it uses exclamation points in the title.\nIf an article is real (B^c), then there’s only a roughly 2.22% chance it uses exclamation points.\n\nLooking at the probabilities, we can see that 26.67% of fake articles vs. 2.22% of real articles use exclamation points.\n\nExclamation point usage is much more likely among fake news than real news.\nWe have evidence that the article is fake."
  },
  {
    "objectID": "files/lectures/Untitled.html#building-a-bayesian-model-4",
    "href": "files/lectures/Untitled.html#building-a-bayesian-model-4",
    "title": "Thinking Like a Bayesian",
    "section": "Building a Bayesian Model",
    "text": "Building a Bayesian Model\n\nNote that we know that the incoming article used exclamation points (A), but we do not actually know if the article is fake (B or B^c).\nIn this case, we compared P[A|B] and P[A|B^c] to ascertain the relative likelihood of observing A under different scenarios.\n\nL\\left[B|A\\right] = P\\left[A|B\\right] \\text{ and } L\\left[B^c|A\\right] = P\\left[A|B^c\\right]"
  },
  {
    "objectID": "files/lectures/Untitled.html#building-a-bayesian-model-5",
    "href": "files/lectures/Untitled.html#building-a-bayesian-model-5",
    "title": "Thinking Like a Bayesian",
    "section": "Building a Bayesian Model",
    "text": "Building a Bayesian Model\n\n\n\nEvent\nB\nBc\nTotal\n\n\n\n\nPrior Probability\n0.4\n0.6\n1.0\n\n\nLikelihood\n0.2667\n0.0222\n0.2889\n\n\n\nL\\left[B|A\\right] = P\\left[A|B\\right] \\text{ and } L\\left[B^c|A\\right] = P\\left[A|B^c\\right]\n\nIt is important for us to note that the likelihood function is not a probability function.\n\nThis is a framework to compare the relative comparability of our exclamation point data with B and B^c."
  },
  {
    "objectID": "files/lectures/Untitled.html#building-a-bayesian-model-6",
    "href": "files/lectures/Untitled.html#building-a-bayesian-model-6",
    "title": "Thinking Like a Bayesian",
    "section": "Building a Bayesian Model",
    "text": "Building a Bayesian Model\n\n\n\nEvent\nB (fake)\nBc (real)\nTotal\n\n\n\n\nPrior Probability\n0.4\n0.6\n1.0\n\n\nLikelihood\n0.2667\n0.0222\n0.2889\n\n\n\n\nThe prior evidence suggested the article is most likely real,\n\nP[B] = 0.4 &lt; P[B^c] = 0.6\n\nThe data, however, is more consistent with the article being fake,\n\nL[B|A] = 0.2667 &gt; L[B^c|A] = 0.0222"
  },
  {
    "objectID": "files/lectures/Untitled.html#building-a-bayesian-model-7",
    "href": "files/lectures/Untitled.html#building-a-bayesian-model-7",
    "title": "Thinking Like a Bayesian",
    "section": "Building a Bayesian Model",
    "text": "Building a Bayesian Model\n\nWe can summarize our probabilities in a table,\n\n\n\n\n\nB\nB^c\nTotal\n\n\n\n\nA\n\n\n\n\n\nA^c\n\n\n\n\n\nTotal\n0.4\n0.6\n1\n\n\n\n\nAs found earlier, P[A|B] = 0.2667 and P[A|B^c]=0.0222.\n\n\n\\begin{align*}\nP[A \\cap B] &= P[A|B] \\times P[B] \\\\\n&= 0.2667 \\times 0.4 \\\\\n&= 0.1067\n\\end{align*}"
  },
  {
    "objectID": "files/lectures/Untitled.html#building-a-bayesian-model-8",
    "href": "files/lectures/Untitled.html#building-a-bayesian-model-8",
    "title": "Thinking Like a Bayesian",
    "section": "Building a Bayesian Model",
    "text": "Building a Bayesian Model\n\nHere’s what we know,\n\n\n\n\n\nB\nB^c\nTotal\n\n\n\n\nA\n0.1067\n\n\n\n\nA^c\n\n\n\n\n\nTotal\n0.4\n0.6\n1\n\n\n\n\nWe also know P[A|B] = 0.2667 and P[A|B^c]=0.0222.\n\n\n\\begin{align*}\nP[A^c \\cap B] &= P(A^c|B) \\times P[B]  \\\\\n&= (1-P[A|B]) \\times P[B] \\\\\n&= (1-0.2667) \\times 0.4 \\\\\n&= 0.2933\n\\end{align*}"
  },
  {
    "objectID": "files/lectures/Untitled.html#building-a-bayesian-model-9",
    "href": "files/lectures/Untitled.html#building-a-bayesian-model-9",
    "title": "Thinking Like a Bayesian",
    "section": "Building a Bayesian Model",
    "text": "Building a Bayesian Model\n\nHere’s what we know,\n\n\n\n\n\nB\nB^c\nTotal\n\n\n\n\nA\n0.1067\n\n\n\n\nA^c\n0.2933\n\n\n\n\nTotal\n0.4\n0.6\n1\n\n\n\n\nWe also know P[A|B] = 0.2667 and P[A|B^c]=0.0222.\n\n\n\\begin{align*}\nP[A \\cap B^c] &= P[A|B^c] \\times P[B^c]  \\\\\n&= 0.0222 \\times 0.6 \\\\\n&= 0.0133\n\\end{align*}"
  },
  {
    "objectID": "files/lectures/Untitled.html#building-a-bayesian-model-10",
    "href": "files/lectures/Untitled.html#building-a-bayesian-model-10",
    "title": "Thinking Like a Bayesian",
    "section": "Building a Bayesian Model",
    "text": "Building a Bayesian Model\n\nHere’s what we know,\n\n\n\n\n\nB\nB^c\nTotal\n\n\n\n\nA\n0.1067\n0.0133\n\n\n\nA^c\n0.2933\n\n\n\n\nTotal\n0.4\n0.6\n1\n\n\n\n\nWe also know P[A|B] = 0.2667 and P[A|B^c]=0.0222.\n\n\n\\begin{align*}\nP[A^c \\cap B^c] &= P[A^c|B^c] \\times P[B^c]  \\\\\n&= 0.9778 \\times 0.6 \\\\\n&= 0.5867\n\\end{align*}"
  },
  {
    "objectID": "files/lectures/Untitled.html#building-a-bayesian-model-11",
    "href": "files/lectures/Untitled.html#building-a-bayesian-model-11",
    "title": "Thinking Like a Bayesian",
    "section": "Building a Bayesian Model",
    "text": "Building a Bayesian Model\n\nHere’s what we know,\n\n\n\n\n\nB\nB^c\nTotal\n\n\n\n\nA\n0.1067\n0.0133\n\n\n\nA^c\n0.2933\n0.5867\n\n\n\nTotal\n0.4\n0.6\n1\n\n\n\n\nFinally,\n\n\n\\begin{align*}\n&P[A] = 0.1067 + 0.0133 = 0.12 \\\\\n&P[A^c] = 0.2933 + 0.5867 = 0.88\n\\end{align*}"
  },
  {
    "objectID": "files/lectures/Untitled.html#building-a-bayesian-model-12",
    "href": "files/lectures/Untitled.html#building-a-bayesian-model-12",
    "title": "Thinking Like a Bayesian",
    "section": "Building a Bayesian Model",
    "text": "Building a Bayesian Model\n\nUsing rules of probability, we have completed the table.\n\n\n\n\n\nB\nB^c\nTotal\n\n\n\n\nA\n0.1067\n0.0133\n0.12\n\n\nA^c\n0.2933\n0.5867\n0.88\n\n\nTotal\n0.4\n0.6\n1"
  },
  {
    "objectID": "files/lectures/Untitled.html#building-a-bayesian-model-13",
    "href": "files/lectures/Untitled.html#building-a-bayesian-model-13",
    "title": "Thinking Like a Bayesian",
    "section": "Building a Bayesian Model",
    "text": "Building a Bayesian Model\n\nWith more information, we can answer the question: what is the probability that the latest article is fake?\nWe will use the posterior probability, P[B|A], which is found using Bayes’ Rule.\nBayes’ Rule: For events A and B,\n\nP[B|A] = \\frac{P[A \\cap B]}{P[A]} = \\frac{P[B] \\times L[B|A]}{P[A]}\n\nBut really, we can think about it like this,\n\n\\text{posterior} = \\frac{\\text{prior} \\times \\text{likelihood}}{\\text{normalizing constant}}"
  },
  {
    "objectID": "files/lectures/Untitled.html#example-set-up",
    "href": "files/lectures/Untitled.html#example-set-up",
    "title": "Thinking Like a Bayesian",
    "section": "Example Set Up",
    "text": "Example Set Up\n\nIn 1996, Gary Kasparov played a six-game chess match against the IBM supercomputer Deep Blue.\n\nOf the six games, Kasparov won three, drew two, and lost one.\nThus, Kasparov won the overall match.\n\nKasparov and Deep Blue were to meet again for a six-game match in 1997.\nLet \\pi denote Kasparov’s chances of winning any particular game in the re-match.\n\nThus, \\pi is a measure of his overall skill relative to Deep Blue.\nGiven the complexity of chess, machines, and humans, \\pi is unknown and can vary over time.\n\ni.e., \\pi is a random variable."
  },
  {
    "objectID": "files/lectures/Untitled.html#example-set-up-1",
    "href": "files/lectures/Untitled.html#example-set-up-1",
    "title": "Thinking Like a Bayesian",
    "section": "Example Set Up",
    "text": "Example Set Up\n\nOur first step is to start with a prior model. This model\n\nIdentifies what values \\pi can take,\nassigns a prior weight or probability to each, and\nthese probabilities sum to 1.\n\nBased on what we were told, the prior model for \\pi in our example,\n\n\n\n\n\\pi\n0.2\n0.5\n0.8\nTotal\n\n\n\n\nf(\\pi)\n0.10\n0.25\n0.65\n1"
  },
  {
    "objectID": "files/lectures/Untitled.html#example-set-up-2",
    "href": "files/lectures/Untitled.html#example-set-up-2",
    "title": "Thinking Like a Bayesian",
    "section": "Example Set Up",
    "text": "Example Set Up\n\nBased on what we were told, the prior model for \\pi in our example,\n\n\n\n\n\\pi\n0.2\n0.5\n0.8\nTotal\n\n\n\n\nf(\\pi)\n0.10\n0.25\n0.65\n1\n\n\n\n\nNote that this is an incredibly simple model.\n\nThe win probability can technically be any number \\in [0, 1].\nHowever, this prior assumes that \\pi has a discrete set of possibilities: 20%, 50%, or 80%."
  },
  {
    "objectID": "files/lectures/Untitled.html#example-set-up-3",
    "href": "files/lectures/Untitled.html#example-set-up-3",
    "title": "Thinking Like a Bayesian",
    "section": "Example Set Up",
    "text": "Example Set Up\n\nIn the second step of our analysis, we collect and process data which can inform our understanding of \\pi.\nHere, Y = the number of the six games in the 1997 re-match that Kasparov wins.\n\nAs chess match outcome isn’t predetermined, Y is a random variable that can take any value in \\{1, 2, 3, 4, 5, 6\\}.\n\nNote that Y inherently depends upon \\pi.\n\nIf \\pi = 0.80, Y would also be high (on average).\nIf \\pi = 0.20, Y would also be low (on average).\n\nThus, we must model this dependence of Y on \\pi using a conditional probability model."
  },
  {
    "objectID": "files/lectures/Untitled.html#binomial-data-model",
    "href": "files/lectures/Untitled.html#binomial-data-model",
    "title": "Thinking Like a Bayesian",
    "section": "Binomial Data Model",
    "text": "Binomial Data Model\n\nWe must make two assumptions about the chess match:\n\nGames are independent (the outcome of one game does not influence the outcome of another).\nKasparov has an equal probability of winning any game in the match.\n\ni.e., probability of winning does not increase or decrease as the match goes on.\n\n\nWe will use a binomial model for this problem.\n\nIn our case,\n\n\nY|\\pi \\sim \\text{Bin}(6, \\pi)"
  },
  {
    "objectID": "files/lectures/Untitled.html#binomial-data-model-1",
    "href": "files/lectures/Untitled.html#binomial-data-model-1",
    "title": "Thinking Like a Bayesian",
    "section": "Binomial Data Model",
    "text": "Binomial Data Model\n\nLet’s assume \\pi = 0.8.\nThe probability that he would win all 6 games is approximately 26%.\n\nf(y=6|\\pi=0.8) = {6 \\choose 6} 0.8^6 (1-0.8)^{6-6}, \n\ndbinom(6, 6, 0.8)\n\n[1] 0.262144"
  },
  {
    "objectID": "files/lectures/Untitled.html#binomial-data-model-2",
    "href": "files/lectures/Untitled.html#binomial-data-model-2",
    "title": "Thinking Like a Bayesian",
    "section": "Binomial Data Model",
    "text": "Binomial Data Model\n\nLet’s assume \\pi = 0.8.\nThe probability that he would win none of the games is approximately 0%.\n\nf(y=0|\\pi=0.8) = {6 \\choose 0} 0.8^0 (1-0.8)^{6-0}, \n\ndbinom(0, 6, 0.8)\n\n[1] 6.4e-05"
  },
  {
    "objectID": "files/lectures/Untitled.html#binomial-data-model-3",
    "href": "files/lectures/Untitled.html#binomial-data-model-3",
    "title": "Thinking Like a Bayesian",
    "section": "Binomial Data Model",
    "text": "Binomial Data Model\n\nWe want to reproduce Figure 2.5 from the Bayes Rules! textbook (from Section 2.3.2).\n\n\n\nEach group will complete the graph for a specified value of \\pi.\n\nCampus: \\pi=0.2\nZoom 1: \\pi=0.5\nZoom 2: \\pi=0.8"
  },
  {
    "objectID": "files/lectures/Untitled.html#binomial-data-model-4",
    "href": "files/lectures/Untitled.html#binomial-data-model-4",
    "title": "Thinking Like a Bayesian",
    "section": "Binomial Data Model",
    "text": "Binomial Data Model\n\nNote that the Binomial gives us the theoretical model of the data we might observe.\n\nKasparov only won one of the six games against Deep Blue in 1997 (Y=1).\n\nNext step: how compatible this particular data is with the various possible \\pi?\n\nWhat is the likelihood of Kasparov winning Y=1 game under each possible \\pi?\n\nRecall, f(y|\\pi) = L(\\pi|Y=y). When Y=1,\n\n\n\\begin{align*}\nL(\\pi | y = 1) &= f(y=1|\\pi) \\\\\n&= {6 \\choose 1} \\pi^1 (1-\\pi)^{6-1} \\\\\n&= 6\\pi(1-\\pi)^5\n\\end{align*}\n\n\nNote that we do not expect all likelihoods to sum to 1."
  },
  {
    "objectID": "files/lectures/Untitled.html#binomial-data-model-5",
    "href": "files/lectures/Untitled.html#binomial-data-model-5",
    "title": "Thinking Like a Bayesian",
    "section": "Binomial Data Model",
    "text": "Binomial Data Model\n\nUse your results from earlier to tell me the resulting likelihood values.\n\n\n\n\n\n\n\n\n\n\n\\pi\n0.2\n0.5\n0.8\n\n\n\n\nL(\\pi|y=1)"
  },
  {
    "objectID": "files/lectures/Untitled.html#binomial-data-model-6",
    "href": "files/lectures/Untitled.html#binomial-data-model-6",
    "title": "Thinking Like a Bayesian",
    "section": "Binomial Data Model",
    "text": "Binomial Data Model\n\nUse your results from earlier to tell me the resulting likelihood values.\n\n\n\n\n\\pi\n0.2\n0.5\n0.8\n\n\n\n\nL(\\pi|y=1)\n0.3932\n0.0938\n0.0015\n\n\n\n\nAs we can see, the likelihoods do not sum to 1."
  },
  {
    "objectID": "files/lectures/Untitled.html#normalizing-constant",
    "href": "files/lectures/Untitled.html#normalizing-constant",
    "title": "Thinking Like a Bayesian",
    "section": "Normalizing Constant",
    "text": "Normalizing Constant\n\nBayes’ Rule requires three pieces of information:\n\nPrior\nLikelihood\nNormalizing constant\n\nNormalizing constant: ensures that the sum of all probabilities is equal to 1.\n\nIt can be a scalar or a function.\nEvery probability distribution that does not sum to 1 will have a normalizing constant."
  },
  {
    "objectID": "files/lectures/Untitled.html#normalizing-constant-1",
    "href": "files/lectures/Untitled.html#normalizing-constant-1",
    "title": "Thinking Like a Bayesian",
    "section": "Normalizing Constant",
    "text": "Normalizing Constant\n\nWe now must determine the total probability that Kasparov would win Y=1 games across all possible win probabilities \\pi, f(y=1).\n\n\n\\begin{align*}\nf(y=1) =& \\sum_{\\pi} L(\\pi |y=1)f(\\pi) \\\\\n=& L(\\pi=0.2|y=1)f(\\pi=0.2) + L(\\pi=0.5|y=1)f(\\pi=0.5) + \\\\\n& L(\\pi=0.8|y=1)f(\\pi=0.8)\n\\end{align*}\n\n\nWork with your group to find the normalizing constant."
  },
  {
    "objectID": "files/lectures/Untitled.html#normalizing-constant-2",
    "href": "files/lectures/Untitled.html#normalizing-constant-2",
    "title": "Thinking Like a Bayesian",
    "section": "Normalizing Constant",
    "text": "Normalizing Constant\n\nWe now must determine the total probability that Kasparov would win Y=1 games across all possible win probabilities \\pi, f(y=1).\n\n\n\\begin{align*}\nf(y=1) =& \\sum_{\\pi} L(\\pi |y=1)f(\\pi) \\\\\n=& L(\\pi=0.2|y=1)f(\\pi=0.2) + L(\\pi=0.5|y=1)f(\\pi=0.5) + \\\\\n& L(\\pi=0.8|y=1)f(\\pi=0.8) \\\\\n\\approx& 0.3932 \\cdot 0.10 + 0.0938 \\cdot 0.25 + 0.0015 \\cdot 0.65 \\\\\n\\approx& 0.0637\n\\end{align*}\n\n\nAcross all possible values of \\pi, there is about a 6% chance that Kasparov would have won only one game."
  },
  {
    "objectID": "files/lectures/Untitled.html#posterior-probability-model",
    "href": "files/lectures/Untitled.html#posterior-probability-model",
    "title": "Thinking Like a Bayesian",
    "section": "Posterior Probability Model",
    "text": "Posterior Probability Model\n\nNow recall,\n\n\\text{posterior} = \\frac{\\text{prior} \\times \\text{likelihood}}{\\text{normalizing constant}}\n\nIn our example, where y = 1,\n\nf(\\pi | y=1) = \\frac{f(\\pi) L(\\pi | y = 1)}{f(y=1)} \\  \\text{for} \\ \\pi \\in \\{ 0.2, 0.5, 0.8\\}\n\nWork with your group to find the posterior probabilities.\n\nYou will have one posterior probability for each value of \\pi."
  },
  {
    "objectID": "files/lectures/Untitled.html#posterior-probability-model-1",
    "href": "files/lectures/Untitled.html#posterior-probability-model-1",
    "title": "Thinking Like a Bayesian",
    "section": "Posterior Probability Model",
    "text": "Posterior Probability Model\n\nNote!! We do not have to calculate the normalizing constant!\nWe can note that f(Y=y) = 1/c.\nThen, we say that\n\n\n\\begin{align*}\nf(\\pi | y) &= \\frac{f(\\pi) L(\\pi|y)}{f(y)} \\\\\n& \\propto  f(\\pi) L(\\pi|y) \\\\\n\\\\\n\\text{posterior} &\\propto \\text{prior} \\cdot \\text{likelihood}\n\\end{align*}"
  },
  {
    "objectID": "files/lectures/Untitled.html#wrap-up",
    "href": "files/lectures/Untitled.html#wrap-up",
    "title": "Thinking Like a Bayesian",
    "section": "Wrap Up",
    "text": "Wrap Up\n\nToday we learned how to, in general, approach Bayesian analysis.\nOn Thursday, we will formalize what we observed today and learn about the conjugate families.\n\nBeta-binomial\nGamma-Poisson\nNormal Normal\n\nFor the rest of class, please work on Assignment 2.\n\nSee Canvas for the starter .qmd file."
  },
  {
    "objectID": "files/lectures/Untitled.html#homework",
    "href": "files/lectures/Untitled.html#homework",
    "title": "Thinking Like a Bayesian",
    "section": "Homework",
    "text": "Homework\n\n1.3\n1.4\n1.8\n2.4\n2.9"
  },
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "Abbreviated Syllabus",
    "section": "",
    "text": "Note: this is an abbreviated syllabus. The full syllabus is available through Classmate and/or Canvas.\n\nInstructor Information\n\nDr. Samantha Seals\nOffice: 4/344\n\n\n\nMeeting Times and Location\n\nMW 4:00 pm–5:15 pm\nPhysical classroom: 4/406\nZoom classroom: see Canvas or full syllabus for link\n\n\n\nOffice Hours\n\nMonday: 9:00 am-11:00 am\nTuesday: 2:00 pm-4:00 pm\nWednesday: 9:00 am-11:00 am\nOther times by appointment\n\n\n\nGrading and Evaluation\nThe course grade will be determined as follows:\n\nAssignments (25%): Every module will finish with an assignment to demonstrate the knowledge gained. TThe resulting .html file should be submitted to the designated Assignment on Canvas by 11:59 pm on the specified date (see Canvas).\nProject (50%): Students in this course will be collaborating with students in GEO6118 - Research Design. The final research project will be submitted along with smaller pieces throughout the semester.\nFinal Exam (25%): The final exam will be a take home final.\n\n\n\nLate Policy\nAssigments have due dates, however, the dropboxes will not close until the end of the semester. All students are automatically granted “extensions” without question.\nNote that if there is not a submission when I go to grade (after the initial deadline), I will assign a zero (0) and request that you submit the assignment when you are able to. This is only for record keeping purposes. There is no penalty for submitting late and a full grade will be given upon review of your submission.\nExtensions are not available for project pieces or the final exam."
  }
]