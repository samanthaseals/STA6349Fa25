[
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Tentative Schedule",
    "section": "",
    "text": "Week 1: August 25-29, 2025\n\n\n\n\n\n\nDr. Seals out sick :(\n\n\n\n\n\n\n\n\n\n\nWeek 2: September 1-5, 2025\n\n\n\n\n\n\nMeeting 1:\n\nLabor Day holiday - campus closed.\n\nMeeting 2:\n\nTopic(s):\n\nVery Brief Review of Probability\n\nSlides:\n\n.html: view slides here\n.qmd: see underlying code here\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 3: September 8-12, 2025\n\n\n\n\n\n\nMeeting 1:\n\nSchedule a meeting with your EES collaborator.\nWorksheet: see Canvas.\n\nMeeting 2:\n\nTopic(s):\n\nDiscrete distributions\n\nSlides:\n\n.html: view slides here\n.qmd: see underlying code here\n\n\nAssignment 1: Probability Distributions - due September 16\n\n.html: view assignment here\n.qmd: see underlying code here\n\n\n\n\n\n\n\n\n\n\n\nWeek 4: September 15-19, 2025\n\n\n\n\n\n\nMeeting 1:\n\nTopic(s):\n\nContinuous distributions\n\nSlides:\n\n.html: view slides here\n.qmd: see underlying code here\n\n\nAssignment 1: Probability Distributions - due September 21\n\n.html: view assignment here\n.qmd: see underlying code here\n\nMeeting 2:\n\nTopic(s):\n\nThinking like a Bayesian\nBayes’ Rule/Theorem\n\nSlides:\n\n.html: view slides here\n.qmd: see underlying code here\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 5: September 22-26, 2025\n\n\n\n\n\n\nMeeting 1:\n\nTopic(s):\n\nBeta-Binomial\n\nSlides:\n\n.html: view slides here\n.qmd: see underlying code here\n\n\nMeeting 2:\n\nTopic(s):\n\nBalance and Sequentiality\n\nSlides:\n\n.html: view slides here\n.qmd: see underlying code here\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 6: September 29-October 3, 2025\n\n\n\n\n\n\nMeeting 1:\n\nTopic(s):\n\nGamma-Poisson\nNormal-Normal\n\nSlides:\n\n.html:\n.qmd:\n\n\nMeeting 2:\n\nTopic(s):\n\nApproximating the posterior\nMCMC\n\nSlides:\n\n.html:\n.qmd:\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 7: October 6-10, 2025\n\n\n\n\n\n\nMeeting 1:\n\nTopic(s):\n\nPosterior inference\nPosterior prediction\n\nSlides:\n\n.html:\n.qmd:\n\n\nAssignment 2: Basic Bayesian Analysis\n\n.html:\n.qmd:\n\n\n\n\n\n\n\n\n\n\n\nWeek 8: October 13-17, 2025\n\n\n\n\n\n\nMeeting 1:\n\nColumbus Day holiday - campus closed.\n\nMeeting 2:\n\nTopic(s):\n\nRegression under the normal distribution\n\nSlides:\n\n.html:\n.qmd:\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 9: October 20-24, 2025\n\n\n\n\n\n\nMeeting 1:\n\nTopic(s):\n\nEvaluating the model\n\nSlides:\n\n.html:\n.qmd:\n\n\nMeeting 2:\n\nTopic(s):\n\nPoisson/negative binomial regressions\nLogistic regression\n\nSlides:\n\n.html:\n.qmd:\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 10: October 27-31, 2025\n\n\n\n\n\n\nMeeting 2:\n\nTopic(s):\n\nHow to “read” research papers outside of statistics.\n\nProject 2a: Reviewing literature (complete in class)\n\n.html:\n.qmd:\n\nProject 2b: Reviewing literature (complete at home)\n\n.html:\n.qmd:\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 11: November 3-7, 2025\n\n\n\n\n\n\nMeeting 1:\n\nAssignment 3: Regression Analysis\n\n.html:\n.qmd:\n\nCheck in with Project 2b\n\nMeeting 2:\n\nMeet with your EES collaborator on your own to discuss methodology.\nYour annotations (Project 2b) will be completed, which will help guide the discussion.\nProject 2: Discussion worksheet\n\n\n\n\n\n\n\n\n\n\n\nWeek 12: November 10-14, 2025\n\n\n\n\n\n\nMeeting 1:\n\nWork on analysis\n\nMeeting 2:\n\nWork on analysis\nProject 3: Analysis worksheet/check in\n\n\n\n\n\n\n\n\n\n\n\nWeek 13: November 17-21, 2025\n\n\n\n\n\n\nMeeting 1:\n\nTopic(s):\n\nGeneral guidelines for writing about statistical methodology.\n\nEthical considerations.\n\nGeneral guidelines for providing results.\n\nTables.\nVisualizations.\nWritten summaries.\n\n\nSlides:\n\n.html:\n.qmd:\n\nProject 4a: Methods paragraph\nProject 4b: Tables\nProject 4c: Graphs\n\nMeeting 2:\n\n2 minute presentation of your methodology/results\n\n\n\n\n\n\n\n\n\n\n\nWeek 14: November 24-28, 2025\n\n\n\n\n\n\nMeeting 1:\n\nMeet with your EES collaborator on your own to discuss results.\nProject 5: Worksheet for discussion\n\nMeeting 2:\n\nThanksgiving holiday - campus closed.\n\n\n\n\n\n\n\n\n\n\n\nWeek 15: December 1-5, 2025\n\n\n\n\n\n\nMeeting 1:\n\nNo meeting.\nTake home final opens.\n\nMeeting 2:\n\nNo meeting.\nFinal draft of research due.\n\n\n\n\n\n\n\n\n\n\n\nWeek 16: Exam Week!\n\n\n\n\n\n\nFinal exam due this week."
  },
  {
    "objectID": "files/lectures/01-1-probability-theory.html#introduction",
    "href": "files/lectures/01-1-probability-theory.html#introduction",
    "title": "Overview of Probability Theory",
    "section": "Introduction",
    "text": "Introduction\n\nToday we will review very basic probability rules to help us build towards analysis under the Bayeisan framework.\nWe will discuss\n\nBasic terminology\nBasic properties\nTypes of probabilities\nTypes of events"
  },
  {
    "objectID": "files/lectures/01-1-probability-theory.html#terminology-for-probability",
    "href": "files/lectures/01-1-probability-theory.html#terminology-for-probability",
    "title": "Overview of Probability Theory",
    "section": "Terminology for Probability",
    "text": "Terminology for Probability\n\nExperiment: A process that results in one and only one of many possible observations.\nSimple outcomes: The possible results of our experiment.\nSample space: Collection of possible outcomes of the experiment.\nEvent: A collection of one or more of the outcomes of the experiment.\nExample: rolling a die once.\n\nOutcome: the result of the die.\nSample space: {1, 2, 3, 4, 5, 6}\nEvents: rolling an odd; rolling a multiple of 3; rolling a 3 or better."
  },
  {
    "objectID": "files/lectures/01-1-probability-theory.html#properties-of-probability",
    "href": "files/lectures/01-1-probability-theory.html#properties-of-probability",
    "title": "Overview of Probability Theory",
    "section": "Properties of Probability",
    "text": "Properties of Probability\n\nThere are two properties of probability that we must keep at the forefront of our mind.\nFirst, probability always falls between 0 and 1. Mathematically,\n\n0 \\le P[E_i] \\le 1\n\nWhat does p=0 imply?\nWhat does p=0.5 imply?\nWhat does p=1 imply?"
  },
  {
    "objectID": "files/lectures/01-1-probability-theory.html#properties-of-probability-1",
    "href": "files/lectures/01-1-probability-theory.html#properties-of-probability-1",
    "title": "Overview of Probability Theory",
    "section": "Properties of Probability",
    "text": "Properties of Probability\n\nThere are two properties of probability that we must keep at the forefront of our mind.\nSecond, the sum of all simple events for an experiment is always 1. Mathematically,\n\n\\sum_{i=1}^n P[E_i] = P[E_1] + ... + P[E_n] = 1\n\nIf there are 2 events and we know P[E_1]=0.7, what is P[E_2]? \nIf there are 4 events and we know P[E_1]=P[E_2]=0.1, P[E_3]=0.6, what is P[E_4]?"
  },
  {
    "objectID": "files/lectures/01-1-probability-theory.html#assigning-probabilities",
    "href": "files/lectures/01-1-probability-theory.html#assigning-probabilities",
    "title": "Overview of Probability Theory",
    "section": "Assigning Probabilities",
    "text": "Assigning Probabilities\n\nHow do we assign probabilities?\n\nSubjective probability\nClassicial probability rule\nRelative frequency"
  },
  {
    "objectID": "files/lectures/01-1-probability-theory.html#subjective-probability",
    "href": "files/lectures/01-1-probability-theory.html#subjective-probability",
    "title": "Overview of Probability Theory",
    "section": "Subjective Probability",
    "text": "Subjective Probability\n\nSubjective probability is the probability assigned to an event based on subjective judgement, experience, information, and belief.\nExamples:\n\nP[UWF wins national championship]\nP[tomato plant eaten by hornworms]\nP[A in this course]"
  },
  {
    "objectID": "files/lectures/01-1-probability-theory.html#classical-probability",
    "href": "files/lectures/01-1-probability-theory.html#classical-probability",
    "title": "Overview of Probability Theory",
    "section": "Classical Probability",
    "text": "Classical Probability\n\nLet A be an event for an experiment with equally likely outcomes,\n\nP[A] = \\frac{\\text{Number of outcomes favorable to $A$}}{\\text{Total number of outcomes for the experiment}}\n\nExamples:\n\nP[2 heads on 3 coin tosses]\nP[at least 2 heads on 4 coin tosses]\nP[even when rolling die]"
  },
  {
    "objectID": "files/lectures/01-1-probability-theory.html#relative-frequency",
    "href": "files/lectures/01-1-probability-theory.html#relative-frequency",
    "title": "Overview of Probability Theory",
    "section": "Relative Frequency",
    "text": "Relative Frequency\n\nIf an experiment is repeated n times and an event A is observed f times, then\n\nP[A] = \\frac{f}{n} = \\frac{\\text{Frequency of $A$}}{\\text{Sample size}}\n\nExample:\n\nP[car is a lemon] given 10/500 sampled cars from a factory are lemons.\nP[person is a homeowner] given 730/1000 sampled individuals own a home."
  },
  {
    "objectID": "files/lectures/01-1-probability-theory.html#contingency-tables",
    "href": "files/lectures/01-1-probability-theory.html#contingency-tables",
    "title": "Overview of Probability Theory",
    "section": "Contingency Tables",
    "text": "Contingency Tables\n\nSuppose 100 employees at Target were asked whether they are in favor of or against extending store hours during the holiday season.\n\n\n\n\n\n\n\n\nDepartment\n\n\nIn Favor\n\n\nAgainst\n\n\nTotal\n\n\n\n\nElectronics\n\n\n12\n\n\n8\n\n\n20\n\n\n\n\nClothing\n\n\n18\n\n\n12\n\n\n30\n\n\n\n\nGrocery\n\n\n25\n\n\n15\n\n\n40\n\n\n\n\nCustomer Service\n\n\n5\n\n\n5\n\n\n10\n\n\n\n\nTotal\n\n\n60\n\n\n40\n\n\n100"
  },
  {
    "objectID": "files/lectures/01-1-probability-theory.html#marginal-probability",
    "href": "files/lectures/01-1-probability-theory.html#marginal-probability",
    "title": "Overview of Probability Theory",
    "section": "Marginal Probability",
    "text": "Marginal Probability\n\nA marginal probability is the probability of a single event occurring without considering any other variables.\n\nIn a contingency table, marginal probabilities are found outside the body of the table.\n\nIt tells us the likelihood of one category happening overall, regardless of how it combines (or interacts) with other categories.\nIn our Target example:\n\nWhat is the probability that a randomly selected employee is in favor?\nWhat is the probability that randomly selected employee works in the Grocery department?"
  },
  {
    "objectID": "files/lectures/01-1-probability-theory.html#marginal-probability-1",
    "href": "files/lectures/01-1-probability-theory.html#marginal-probability-1",
    "title": "Overview of Probability Theory",
    "section": "Marginal Probability",
    "text": "Marginal Probability\n\nRecall,\n\n\n\n\n\n\n\n\nDepartment\n\n\nIn Favor\n\n\nAgainst\n\n\nTotal\n\n\n\n\nElectronics\n\n\n12\n\n\n8\n\n\n20\n\n\n\n\nClothing\n\n\n18\n\n\n12\n\n\n30\n\n\n\n\nGrocery\n\n\n25\n\n\n15\n\n\n40\n\n\n\n\nCustomer Service\n\n\n5\n\n\n5\n\n\n10\n\n\n\n\nTotal\n\n\n60\n\n\n40\n\n\n100\n\n\n\n\n\n\n\nWhat is the probability that a randomly selected employee is in favor?\nWhat is the probability that randomly selected employee works in the Grocery department?"
  },
  {
    "objectID": "files/lectures/01-1-probability-theory.html#joint-probability",
    "href": "files/lectures/01-1-probability-theory.html#joint-probability",
    "title": "Overview of Probability Theory",
    "section": "Joint Probability",
    "text": "Joint Probability\n\nThe joint probability is the probability that two events happen at the same time.\n\nIn a contingency table, joint probabilities are found inside the body of the table.\n\nIt tells us the likelihood that a randomly selected observation falls into both categories simultaneously.\nIn our Target example:\n\nWhat is the probability that an employee is in the Grocery department and in favor of extended hours?\nWhat is the probability that an employee is in Electronics and against extended hours?"
  },
  {
    "objectID": "files/lectures/01-1-probability-theory.html#joint-probability-1",
    "href": "files/lectures/01-1-probability-theory.html#joint-probability-1",
    "title": "Overview of Probability Theory",
    "section": "Joint Probability",
    "text": "Joint Probability\n\nRecall,\n\n\n\n\n\n\n\n\nDepartment\n\n\nIn Favor\n\n\nAgainst\n\n\nTotal\n\n\n\n\nElectronics\n\n\n12\n\n\n8\n\n\n20\n\n\n\n\nClothing\n\n\n18\n\n\n12\n\n\n30\n\n\n\n\nGrocery\n\n\n25\n\n\n15\n\n\n40\n\n\n\n\nCustomer Service\n\n\n5\n\n\n5\n\n\n10\n\n\n\n\nTotal\n\n\n60\n\n\n40\n\n\n100\n\n\n\n\n\n\n\nWhat is the probability that an employee is in the Grocery department and in favor of extended hours?\nWhat is the probability that an employee is in Electronics and against extended hours?"
  },
  {
    "objectID": "files/lectures/01-1-probability-theory.html#conditional-probability",
    "href": "files/lectures/01-1-probability-theory.html#conditional-probability",
    "title": "Overview of Probability Theory",
    "section": "Conditional Probability",
    "text": "Conditional Probability\n\nConditional probability is the probability that one event occurs given that we already know another event has occurred.\n\n“What is the probability of Event A if we know Event B is true?”\nIn a contingency table, conditional probabilities are found by limiting yourself to a specific row or column of the table, then finding the corresponding probability.\n\nIn our Target example,\n\nWhat is the probability that an employee is in favor of extended hours given that they work in the Grocery department?\nWhat is the probability that an employee works in the Electronics department given that they are against extended hours?"
  },
  {
    "objectID": "files/lectures/01-1-probability-theory.html#conditional-probability-1",
    "href": "files/lectures/01-1-probability-theory.html#conditional-probability-1",
    "title": "Overview of Probability Theory",
    "section": "Conditional Probability",
    "text": "Conditional Probability\n\nRecall,\n\n\n\n\n\n\n\n\nDepartment\n\n\nIn Favor\n\n\nAgainst\n\n\nTotal\n\n\n\n\nElectronics\n\n\n12\n\n\n8\n\n\n20\n\n\n\n\nClothing\n\n\n18\n\n\n12\n\n\n30\n\n\n\n\nGrocery\n\n\n25\n\n\n15\n\n\n40\n\n\n\n\nCustomer Service\n\n\n5\n\n\n5\n\n\n10\n\n\n\n\nTotal\n\n\n60\n\n\n40\n\n\n100\n\n\n\n\n\n\n\nWhat is the probability that an employee is in favor of extended hours given that they work in the Grocery department?\nWhat is the probability that an employee works in the Electronics department given that they are against extended hours?"
  },
  {
    "objectID": "files/lectures/01-1-probability-theory.html#mutually-exclusive-events",
    "href": "files/lectures/01-1-probability-theory.html#mutually-exclusive-events",
    "title": "Overview of Probability Theory",
    "section": "Mutually Exclusive Events",
    "text": "Mutually Exclusive Events\n\nEvents that cannot occur together are mutually exclusive or disjoint.\nConsider rolling a single die:\n\nA = an even number = {2, 4, 6}\nB = an odd number = {1, 3, 5}\nC = a number less than 5 = {1, 2, 3, 4}\n\nAre events A and B mutually exclusive?\nAre events A and C mutually exclusive?"
  },
  {
    "objectID": "files/lectures/01-1-probability-theory.html#mutually-exclusive-events-1",
    "href": "files/lectures/01-1-probability-theory.html#mutually-exclusive-events-1",
    "title": "Overview of Probability Theory",
    "section": "Mutually Exclusive Events",
    "text": "Mutually Exclusive Events\n\nConsider rolling a single die:\n\nA = an even number = {2, 4, 6}\nB = an odd number = {1, 3, 5}\nC = a number less than 5 = {1, 2, 3, 4}\n\nAre events A and B mutually exclusive?"
  },
  {
    "objectID": "files/lectures/01-1-probability-theory.html#mutually-exclusive-events-2",
    "href": "files/lectures/01-1-probability-theory.html#mutually-exclusive-events-2",
    "title": "Overview of Probability Theory",
    "section": "Mutually Exclusive Events",
    "text": "Mutually Exclusive Events\n\nConsider rolling a single die:\n\nA = an even number = {2, 4, 6}\nB = an odd number = {1, 3, 5}\nC = a number less than 5 = {1, 2, 3, 4}\n\nAre events A and C mutually exclusive?"
  },
  {
    "objectID": "files/lectures/01-1-probability-theory.html#independent-events",
    "href": "files/lectures/01-1-probability-theory.html#independent-events",
    "title": "Overview of Probability Theory",
    "section": "Independent Events",
    "text": "Independent Events\n\nTwo events are said to be independent if the occurrence of one event does not affect the probability of the occurrence of the other event.\nMathematically,\n\n P[A|B] = P[A] \\text{ or } P[B|A] = P[B]\n\nIn our Target example, is department independent of being in favor of extended hours?\n\nWe are being asked to examine P[department|favor] or P[favor|department]."
  },
  {
    "objectID": "files/lectures/01-1-probability-theory.html#independent-events-1",
    "href": "files/lectures/01-1-probability-theory.html#independent-events-1",
    "title": "Overview of Probability Theory",
    "section": "Independent Events",
    "text": "Independent Events\n\nRecall,\n\n\n\n\n\n\n\n\nDepartment\n\n\nIn Favor\n\n\nAgainst\n\n\nTotal\n\n\n\n\nElectronics\n\n\n12\n\n\n8\n\n\n20\n\n\n\n\nClothing\n\n\n18\n\n\n12\n\n\n30\n\n\n\n\nGrocery\n\n\n25\n\n\n15\n\n\n40\n\n\n\n\nCustomer Service\n\n\n5\n\n\n5\n\n\n10\n\n\n\n\nTotal\n\n\n60\n\n\n40\n\n\n100\n\n\n\n\n\n\n\nWe need to examine P[department|favor] or P[favor|department]. We say that they are independent if\n\nP[department|favor] = P[department]\nP[favor|department] = P[favor]"
  },
  {
    "objectID": "files/lectures/01-1-probability-theory.html#complementary-events",
    "href": "files/lectures/01-1-probability-theory.html#complementary-events",
    "title": "Overview of Probability Theory",
    "section": "Complementary Events",
    "text": "Complementary Events\n\nThe complement of A is the event that includes all the outcomes that are not in A.\n\n\n\n\n\nMathematically,\n\n\n\\begin{align*}\nP[A] &+ P[A^c] = 1 \\\\\nP[A] &= 1 - P[A^c] \\\\\nP[A^c] &= 1 - P[A]\n\\end{align*}"
  },
  {
    "objectID": "files/lectures/01-1-probability-theory.html#complementary-events-1",
    "href": "files/lectures/01-1-probability-theory.html#complementary-events-1",
    "title": "Overview of Probability Theory",
    "section": "Complementary Events",
    "text": "Complementary Events\n\nRecall,\n\n\n\n\n\n\n\n\nDepartment\n\n\nIn Favor\n\n\nAgainst\n\n\nTotal\n\n\n\n\nElectronics\n\n\n12\n\n\n8\n\n\n20\n\n\n\n\nClothing\n\n\n18\n\n\n12\n\n\n30\n\n\n\n\nGrocery\n\n\n25\n\n\n15\n\n\n40\n\n\n\n\nCustomer Service\n\n\n5\n\n\n5\n\n\n10\n\n\n\n\nTotal\n\n\n60\n\n\n40\n\n\n100\n\n\n\n\n\n\n\nFind P[Electronicsc].\nFind P[Electronicsc | Against]."
  },
  {
    "objectID": "files/lectures/01-1-probability-theory.html#wrap-up",
    "href": "files/lectures/01-1-probability-theory.html#wrap-up",
    "title": "Overview of Probability Theory",
    "section": "Wrap Up",
    "text": "Wrap Up\n\nToday we have reviewed probability basics.\n\nNote that this is no replacement for Advanced Probability :)\n\nWe just need to understand general concepts to move foward.\nNext week:\n\nMonday: no class meeting – instead, you will meet with your EES collaborator (at an agreed upon time/date).\n\nPairing document with contact information is linked on Discord. We now have a “EES project” channel.\n\nWednesday: probability distributions"
  },
  {
    "objectID": "files/lectures/04-2-thinking-bayesian.html#introduction",
    "href": "files/lectures/04-2-thinking-bayesian.html#introduction",
    "title": "Thinking Like a Bayesian",
    "section": "Introduction",
    "text": "Introduction\n\nBefore today:\n\nRefresher on probability theory\nWhat does each distribution do?\n\nbeta \\to outcomes limited to [0, 1]\nbinomial \\to binary outcomes\ngamma \\to continuous & positive outcomes; skewed right\nnormal \\to continuous outcomes; mound-shaped & symmetric distribution\nPoisson \\to count outcomes; skewed right\nuniform \\to each outcome has equal probability; rectangular distribution\n\n\nToday: building up Bayesian analysis concepts"
  },
  {
    "objectID": "files/lectures/04-2-thinking-bayesian.html#thinking-like-a-bayesian",
    "href": "files/lectures/04-2-thinking-bayesian.html#thinking-like-a-bayesian",
    "title": "Thinking Like a Bayesian",
    "section": "Thinking Like a Bayesian",
    "text": "Thinking Like a Bayesian\n\nBayesian analysis involves updating beliefs based on observed data."
  },
  {
    "objectID": "files/lectures/04-2-thinking-bayesian.html#thinking-like-a-bayesian-1",
    "href": "files/lectures/04-2-thinking-bayesian.html#thinking-like-a-bayesian-1",
    "title": "Thinking Like a Bayesian",
    "section": "Thinking Like a Bayesian",
    "text": "Thinking Like a Bayesian\n\nThis is the natural Bayesian knowledge-building process of:\n\nacknowledging your preconceptions (prior distribution),\nusing data (data distribution) to update your knowledge (posterior distribution), and\nrepeating (posterior distribution \\to new prior distribution)"
  },
  {
    "objectID": "files/lectures/04-2-thinking-bayesian.html#thinking-like-a-bayesian-2",
    "href": "files/lectures/04-2-thinking-bayesian.html#thinking-like-a-bayesian-2",
    "title": "Thinking Like a Bayesian",
    "section": "Thinking Like a Bayesian",
    "text": "Thinking Like a Bayesian\n\nBayesian and frequentist analyses share a common goal: to learn from data about the world around us.\n\nBoth Bayesian and frequentist analyses use data to fit models, make predictions, and evaluate hypotheses.\nWhen working with the same data, they will typically produce a similar set of conclusions.\n\nStatisticians typically identify as either a “Bayesian” or “frequentist” …\n\n🚫 We are not going to “take sides.”\n✅ We will see these as tools in our toolbox."
  },
  {
    "objectID": "files/lectures/04-2-thinking-bayesian.html#thinking-like-a-bayesian-3",
    "href": "files/lectures/04-2-thinking-bayesian.html#thinking-like-a-bayesian-3",
    "title": "Thinking Like a Bayesian",
    "section": "Thinking Like a Bayesian",
    "text": "Thinking Like a Bayesian\n\nBayesian probability: the relative plausibility of an event.\n\nConsiders prior belief."
  },
  {
    "objectID": "files/lectures/04-2-thinking-bayesian.html#thinking-like-a-bayesian-4",
    "href": "files/lectures/04-2-thinking-bayesian.html#thinking-like-a-bayesian-4",
    "title": "Thinking Like a Bayesian",
    "section": "Thinking Like a Bayesian",
    "text": "Thinking Like a Bayesian\n\nFrequentist probability: the long-run relative frequency of a repeatable event.\n\nDoes not consider prior belief."
  },
  {
    "objectID": "files/lectures/04-2-thinking-bayesian.html#thinking-like-a-bayesian-5",
    "href": "files/lectures/04-2-thinking-bayesian.html#thinking-like-a-bayesian-5",
    "title": "Thinking Like a Bayesian",
    "section": "Thinking Like a Bayesian",
    "text": "Thinking Like a Bayesian\n\nThe Bayesian framework depends upon prior information, data, and the balance between them.\n\nThe balance between the prior information and data is determined by the relative strength of each\n\n\n\n\nWhen we have little data, our posterior can rely more on prior knowledge.\nAs we collect more data, the prior can lose its influence."
  },
  {
    "objectID": "files/lectures/04-2-thinking-bayesian.html#thinking-like-a-bayesian-6",
    "href": "files/lectures/04-2-thinking-bayesian.html#thinking-like-a-bayesian-6",
    "title": "Thinking Like a Bayesian",
    "section": "Thinking Like a Bayesian",
    "text": "Thinking Like a Bayesian\n\nWe can also use this approach to combine analysis results."
  },
  {
    "objectID": "files/lectures/04-2-thinking-bayesian.html#thinking-like-a-bayesian-7",
    "href": "files/lectures/04-2-thinking-bayesian.html#thinking-like-a-bayesian-7",
    "title": "Thinking Like a Bayesian",
    "section": "Thinking Like a Bayesian",
    "text": "Thinking Like a Bayesian\n\nWe will use an example to work through Bayesian logic.\nThe Collins Dictionary named “fake news” the 2017 term of the year.\n\nFake, misleading, and biased news has proliferated along with online news and social media platforms which allow users to post articles with little quality control.\n\nWe want to flag articles as “real” or “fake.”\nWe’ll examine a sample of 150 articles which were posted on Facebook and fact checked by five BuzzFeed journalists (Shu et al. 2017)."
  },
  {
    "objectID": "files/lectures/04-2-thinking-bayesian.html#thinking-like-a-bayesian-8",
    "href": "files/lectures/04-2-thinking-bayesian.html#thinking-like-a-bayesian-8",
    "title": "Thinking Like a Bayesian",
    "section": "Thinking Like a Bayesian",
    "text": "Thinking Like a Bayesian\n\nInformation about each article is stored in the fake_news dataset in the bayesrules package.\n\n\nfake_news &lt;- bayesrules::fake_news\nprint(colnames(fake_news))\n\n [1] \"title\"                   \"text\"                   \n [3] \"url\"                     \"authors\"                \n [5] \"type\"                    \"title_words\"            \n [7] \"text_words\"              \"title_char\"             \n [9] \"text_char\"               \"title_caps\"             \n[11] \"text_caps\"               \"title_caps_percent\"     \n[13] \"text_caps_percent\"       \"title_excl\"             \n[15] \"text_excl\"               \"title_excl_percent\"     \n[17] \"text_excl_percent\"       \"title_has_excl\"         \n[19] \"anger\"                   \"anticipation\"           \n[21] \"disgust\"                 \"fear\"                   \n[23] \"joy\"                     \"sadness\"                \n[25] \"surprise\"                \"trust\"                  \n[27] \"negative\"                \"positive\"               \n[29] \"text_syllables\"          \"text_syllables_per_word\""
  },
  {
    "objectID": "files/lectures/04-2-thinking-bayesian.html#thinking-like-a-bayesian-9",
    "href": "files/lectures/04-2-thinking-bayesian.html#thinking-like-a-bayesian-9",
    "title": "Thinking Like a Bayesian",
    "section": "Thinking Like a Bayesian",
    "text": "Thinking Like a Bayesian\n\nWe could build a simple news filter which uses the following rule: since most articles are real, we should read and believe all articles.\n\nWhile this filter would solve the problem of disregarding real articles, we would read lots of fake news.\nIt also only takes into account the overall rates of, not the typical features of, real and fake news."
  },
  {
    "objectID": "files/lectures/04-2-thinking-bayesian.html#thinking-like-a-bayesian-10",
    "href": "files/lectures/04-2-thinking-bayesian.html#thinking-like-a-bayesian-10",
    "title": "Thinking Like a Bayesian",
    "section": "Thinking Like a Bayesian",
    "text": "Thinking Like a Bayesian\n\nSuppose that the most recent article posted to a social media platform is titled: The president has a funny secret!\n\nSome features of this title probably set off some red flags.\nFor example, the usage of an exclamation point might seem like an odd choice for a real news article.\n\nIn the dataset, what is the split of real and fake articles?"
  },
  {
    "objectID": "files/lectures/04-2-thinking-bayesian.html#thinking-like-a-bayesian-11",
    "href": "files/lectures/04-2-thinking-bayesian.html#thinking-like-a-bayesian-11",
    "title": "Thinking Like a Bayesian",
    "section": "Thinking Like a Bayesian",
    "text": "Thinking Like a Bayesian\n\nIn the dataset, what is the split of real and fake articles?\n\n\n\n\n  \n\n\n\n\nOur data backs up our instinct on the article,\n\n\n\n\n  \n\n\n\n\nIn this dataset, 26.67% (16 of 60) of fake news titles but only 2.22% (2 of 90) of real news titles use an exclamation point."
  },
  {
    "objectID": "files/lectures/04-2-thinking-bayesian.html#thinking-like-a-bayesian-12",
    "href": "files/lectures/04-2-thinking-bayesian.html#thinking-like-a-bayesian-12",
    "title": "Thinking Like a Bayesian",
    "section": "Thinking Like a Bayesian",
    "text": "Thinking Like a Bayesian\n\nWe now have two pieces of contradictory information.\n\nOur prior information suggested that incoming articles are most likely real.\nHowever, the exclamation point data is more consistent with fake news.\n\n\n\n\nThinking like Bayesians, we know that balancing both pieces of information is important in developing a posterior understanding of whether the article is fake."
  },
  {
    "objectID": "files/lectures/04-2-thinking-bayesian.html#building-a-bayesian-model",
    "href": "files/lectures/04-2-thinking-bayesian.html#building-a-bayesian-model",
    "title": "Thinking Like a Bayesian",
    "section": "Building a Bayesian Model",
    "text": "Building a Bayesian Model\n\nOur fake news analysis studies two variables:\n\nan article’s fake vs real status and\nits use of exclamation points.\n\nWe can represent the randomness in these variables using probability models.\nWe will now build:\n\na prior probability model for our prior understanding of whether the most recent article is fake;\na model for interpreting the exclamation point data; and, eventually,\na posterior probability model which summarizes the posterior plausibility that the article is fake."
  },
  {
    "objectID": "files/lectures/04-2-thinking-bayesian.html#building-a-bayesian-model-1",
    "href": "files/lectures/04-2-thinking-bayesian.html#building-a-bayesian-model-1",
    "title": "Thinking Like a Bayesian",
    "section": "Building a Bayesian Model",
    "text": "Building a Bayesian Model\n\nLet’s now formalize our prior understanding of whether the new article is fake.\nBased on our fake_news data, we saw that 40% of articles are fake and 60% are real.\n\nBefore reading the new article, there’s a 0.4 prior probability that it’s fake and a 0.6 prior probability it’s not.\n\n\nP\\left[B\\right] = 0.40 \\text{ and } P\\left[B^c\\right] = 0.60\n\nRemember that a valid probability model must:\n\naccount for all possible events (all articles must be fake or real);\nit assigns prior probabilities to each event; and\nthe probabilities sum to one."
  },
  {
    "objectID": "files/lectures/04-2-thinking-bayesian.html#building-a-bayesian-model-2",
    "href": "files/lectures/04-2-thinking-bayesian.html#building-a-bayesian-model-2",
    "title": "Thinking Like a Bayesian",
    "section": "Building a Bayesian Model",
    "text": "Building a Bayesian Model\n\n\n\n\n\n\n\n\ntitle_has_excl\nfake\nreal\n\n\n\n\nFALSE\n73.3% (44)\n97.8% (88)\n\n\nTRUE\n26.7% (16)\n2.2% (2)\n\n\n\n\n\n\n\n\nWe have the following conditional probabilities:\n\nIf an article is fake (B), then there’s a roughly 26.67% chance it uses exclamation points in the title.\nIf an article is real (B^c), then there’s only a roughly 2.22% chance it uses exclamation points."
  },
  {
    "objectID": "files/lectures/04-2-thinking-bayesian.html#building-a-bayesian-model-3",
    "href": "files/lectures/04-2-thinking-bayesian.html#building-a-bayesian-model-3",
    "title": "Thinking Like a Bayesian",
    "section": "Building a Bayesian Model",
    "text": "Building a Bayesian Model\n\nWe have the following conditional probabilities:\n\nIf an article is fake (B), then there’s a roughly 26.67% chance it uses exclamation points in the title.\nIf an article is real (B^c), then there’s only a roughly 2.22% chance it uses exclamation points.\n\nLooking at the probabilities, we can see that 26.67% of fake articles vs. 2.22% of real articles use exclamation points.\n\nExclamation point usage is much more likely among fake news than real news.\nWe have evidence that the article is fake."
  },
  {
    "objectID": "files/lectures/04-2-thinking-bayesian.html#building-a-bayesian-model-4",
    "href": "files/lectures/04-2-thinking-bayesian.html#building-a-bayesian-model-4",
    "title": "Thinking Like a Bayesian",
    "section": "Building a Bayesian Model",
    "text": "Building a Bayesian Model\n\nNote that we know that the incoming article used exclamation points (A), but we do not actually know if the article is fake (B or B^c).\nIn this case, we compared P[A|B] and P[A|B^c] to ascertain the relative likelihood of observing A under different scenarios.\n\nL\\left[B|A\\right] = P\\left[A|B\\right] \\text{ and } L\\left[B^c|A\\right] = P\\left[A|B^c\\right]"
  },
  {
    "objectID": "files/lectures/04-2-thinking-bayesian.html#building-a-bayesian-model-5",
    "href": "files/lectures/04-2-thinking-bayesian.html#building-a-bayesian-model-5",
    "title": "Thinking Like a Bayesian",
    "section": "Building a Bayesian Model",
    "text": "Building a Bayesian Model\n\n\n\nEvent\nB\nBc\nTotal\n\n\n\n\nPrior Probability\n0.4\n0.6\n1.0\n\n\nLikelihood\n0.2667\n0.0222\n0.2889\n\n\n\nL\\left[B|A\\right] = P\\left[A|B\\right] \\text{ and } L\\left[B^c|A\\right] = P\\left[A|B^c\\right]\n\nIt is important for us to note that the likelihood function is not a probability function.\n\nThis is a framework to compare the relative comparability of our exclamation point data with B and B^c."
  },
  {
    "objectID": "files/lectures/04-2-thinking-bayesian.html#building-a-bayesian-model-6",
    "href": "files/lectures/04-2-thinking-bayesian.html#building-a-bayesian-model-6",
    "title": "Thinking Like a Bayesian",
    "section": "Building a Bayesian Model",
    "text": "Building a Bayesian Model\n\n\n\nEvent\nB (fake)\nBc (real)\nTotal\n\n\n\n\nPrior Probability\n0.4\n0.6\n1.0\n\n\nLikelihood\n0.2667\n0.0222\n0.2889\n\n\n\n\nThe prior evidence suggested the article is most likely real,\n\nP[B] = 0.4 &lt; P[B^c] = 0.6\n\nThe data, however, is more consistent with the article being fake,\n\nL[B|A] = 0.2667 &gt; L[B^c|A] = 0.0222"
  },
  {
    "objectID": "files/lectures/04-2-thinking-bayesian.html#building-a-bayesian-model-7",
    "href": "files/lectures/04-2-thinking-bayesian.html#building-a-bayesian-model-7",
    "title": "Thinking Like a Bayesian",
    "section": "Building a Bayesian Model",
    "text": "Building a Bayesian Model\n\nWe can summarize our probabilities in a table,\n\n\n\n\n\nB\nB^c\nTotal\n\n\n\n\nA\n\n\n\n\n\nA^c\n\n\n\n\n\nTotal\n0.4\n0.6\n1\n\n\n\n\nAs found earlier, P[A|B] = 0.2667 and P[A|B^c]=0.0222.\n\n\n\\begin{align*}\nP[A \\cap B] &= P[A|B] \\times P[B] \\\\\n&= 0.2667 \\times 0.4 \\\\\n&= 0.1067\n\\end{align*}"
  },
  {
    "objectID": "files/lectures/04-2-thinking-bayesian.html#building-a-bayesian-model-8",
    "href": "files/lectures/04-2-thinking-bayesian.html#building-a-bayesian-model-8",
    "title": "Thinking Like a Bayesian",
    "section": "Building a Bayesian Model",
    "text": "Building a Bayesian Model\n\nHere’s what we know,\n\n\n\n\n\nB\nB^c\nTotal\n\n\n\n\nA\n0.1067\n\n\n\n\nA^c\n\n\n\n\n\nTotal\n0.4\n0.6\n1\n\n\n\n\nWe also know P[A|B] = 0.2667 and P[A|B^c]=0.0222.\n\n\n\\begin{align*}\nP[A^c \\cap B] &= P(A^c|B) \\times P[B]  \\\\\n&= (1-P[A|B]) \\times P[B] \\\\\n&= (1-0.2667) \\times 0.4 \\\\\n&= 0.2933\n\\end{align*}"
  },
  {
    "objectID": "files/lectures/04-2-thinking-bayesian.html#building-a-bayesian-model-9",
    "href": "files/lectures/04-2-thinking-bayesian.html#building-a-bayesian-model-9",
    "title": "Thinking Like a Bayesian",
    "section": "Building a Bayesian Model",
    "text": "Building a Bayesian Model\n\nHere’s what we know,\n\n\n\n\n\nB\nB^c\nTotal\n\n\n\n\nA\n0.1067\n\n\n\n\nA^c\n0.2933\n\n\n\n\nTotal\n0.4\n0.6\n1\n\n\n\n\nWe also know P[A|B] = 0.2667 and P[A|B^c]=0.0222.\n\n\n\\begin{align*}\nP[A \\cap B^c] &= P[A|B^c] \\times P[B^c]  \\\\\n&= 0.0222 \\times 0.6 \\\\\n&= 0.0133\n\\end{align*}"
  },
  {
    "objectID": "files/lectures/04-2-thinking-bayesian.html#building-a-bayesian-model-10",
    "href": "files/lectures/04-2-thinking-bayesian.html#building-a-bayesian-model-10",
    "title": "Thinking Like a Bayesian",
    "section": "Building a Bayesian Model",
    "text": "Building a Bayesian Model\n\nHere’s what we know,\n\n\n\n\n\nB\nB^c\nTotal\n\n\n\n\nA\n0.1067\n0.0133\n\n\n\nA^c\n0.2933\n\n\n\n\nTotal\n0.4\n0.6\n1\n\n\n\n\nWe also know P[A|B] = 0.2667 and P[A|B^c]=0.0222.\n\n\n\\begin{align*}\nP[A^c \\cap B^c] &= P[A^c|B^c] \\times P[B^c]  \\\\\n&= 0.9778 \\times 0.6 \\\\\n&= 0.5867\n\\end{align*}"
  },
  {
    "objectID": "files/lectures/04-2-thinking-bayesian.html#building-a-bayesian-model-11",
    "href": "files/lectures/04-2-thinking-bayesian.html#building-a-bayesian-model-11",
    "title": "Thinking Like a Bayesian",
    "section": "Building a Bayesian Model",
    "text": "Building a Bayesian Model\n\nHere’s what we know,\n\n\n\n\n\nB\nB^c\nTotal\n\n\n\n\nA\n0.1067\n0.0133\n\n\n\nA^c\n0.2933\n0.5867\n\n\n\nTotal\n0.4\n0.6\n1\n\n\n\n\nFinally,\n\n\n\\begin{align*}\n&P[A] = 0.1067 + 0.0133 = 0.12 \\\\\n&P[A^c] = 0.2933 + 0.5867 = 0.88\n\\end{align*}"
  },
  {
    "objectID": "files/lectures/04-2-thinking-bayesian.html#building-a-bayesian-model-12",
    "href": "files/lectures/04-2-thinking-bayesian.html#building-a-bayesian-model-12",
    "title": "Thinking Like a Bayesian",
    "section": "Building a Bayesian Model",
    "text": "Building a Bayesian Model\n\nUsing rules of probability, we have completed the table.\n\n\n\n\n\nB\nB^c\nTotal\n\n\n\n\nA\n0.1067\n0.0133\n0.12\n\n\nA^c\n0.2933\n0.5867\n0.88\n\n\nTotal\n0.4\n0.6\n1"
  },
  {
    "objectID": "files/lectures/04-2-thinking-bayesian.html#building-a-bayesian-model-13",
    "href": "files/lectures/04-2-thinking-bayesian.html#building-a-bayesian-model-13",
    "title": "Thinking Like a Bayesian",
    "section": "Building a Bayesian Model",
    "text": "Building a Bayesian Model\n\nWith more information, we can answer the question: what is the probability that the latest article is fake?\nWe will use the posterior probability, P[B|A], which is found using Bayes’ Rule.\nBayes’ Rule: For events A and B,\n\nP[B|A] = \\frac{P[A \\cap B]}{P[A]} = \\frac{P[B] \\times L[B|A]}{P[A]}\n\nBut really, we can think about it like this,\n\n\\text{posterior} = \\frac{\\text{prior} \\times \\text{likelihood}}{\\text{normalizing constant}}"
  },
  {
    "objectID": "files/lectures/04-2-thinking-bayesian.html#example-set-up",
    "href": "files/lectures/04-2-thinking-bayesian.html#example-set-up",
    "title": "Thinking Like a Bayesian",
    "section": "Example Set Up",
    "text": "Example Set Up\n\nIn 1996, Gary Kasparov played a six-game chess match against the IBM supercomputer Deep Blue.\n\nOf the six games, Kasparov won three, drew two, and lost one.\nThus, Kasparov won the overall match.\n\nKasparov and Deep Blue were to meet again for a six-game match in 1997.\nLet \\pi denote Kasparov’s chances of winning any particular game in the re-match.\n\nThus, \\pi is a measure of his overall skill relative to Deep Blue.\nGiven the complexity of chess, machines, and humans, \\pi is unknown and can vary over time.\n\ni.e., \\pi is a random variable."
  },
  {
    "objectID": "files/lectures/04-2-thinking-bayesian.html#example-set-up-1",
    "href": "files/lectures/04-2-thinking-bayesian.html#example-set-up-1",
    "title": "Thinking Like a Bayesian",
    "section": "Example Set Up",
    "text": "Example Set Up\n\nOur first step is to start with a prior model. This model\n\nIdentifies what values \\pi can take,\nassigns a prior weight or probability to each, and\nthese probabilities sum to 1.\n\nBased on what we were told, the prior model for \\pi in our example,\n\n\n\n\n\\pi\n0.2\n0.5\n0.8\nTotal\n\n\n\n\nf(\\pi)\n0.10\n0.25\n0.65\n1"
  },
  {
    "objectID": "files/lectures/04-2-thinking-bayesian.html#example-set-up-2",
    "href": "files/lectures/04-2-thinking-bayesian.html#example-set-up-2",
    "title": "Thinking Like a Bayesian",
    "section": "Example Set Up",
    "text": "Example Set Up\n\nBased on what we were told, the prior model for \\pi in our example,\n\n\n\n\n\\pi\n0.2\n0.5\n0.8\nTotal\n\n\n\n\nf(\\pi)\n0.10\n0.25\n0.65\n1\n\n\n\n\nNote that this is an incredibly simple model.\n\nThe win probability can technically be any number \\in [0, 1].\nHowever, this prior assumes that \\pi has a discrete set of possibilities: 20%, 50%, or 80%."
  },
  {
    "objectID": "files/lectures/04-2-thinking-bayesian.html#example-set-up-3",
    "href": "files/lectures/04-2-thinking-bayesian.html#example-set-up-3",
    "title": "Thinking Like a Bayesian",
    "section": "Example Set Up",
    "text": "Example Set Up\n\nIn the second step of our analysis, we collect and process data which can inform our understanding of \\pi.\nHere, Y = the number of the six games in the 1997 re-match that Kasparov wins.\n\nAs chess match outcome isn’t predetermined, Y is a random variable that can take any value in \\{0, 1, 2, 3, 4, 5, 6\\}.\n\nNote that Y inherently depends upon \\pi.\n\nIf \\pi = 0.80, Y would also be high (on average).\nIf \\pi = 0.20, Y would also be low (on average).\n\nThus, we must model this dependence of Y on \\pi using a conditional probability model."
  },
  {
    "objectID": "files/lectures/04-2-thinking-bayesian.html#binomial-data-model",
    "href": "files/lectures/04-2-thinking-bayesian.html#binomial-data-model",
    "title": "Thinking Like a Bayesian",
    "section": "Binomial Data Model",
    "text": "Binomial Data Model\n\nWe must make two assumptions about the chess match:\n\nGames are independent (the outcome of one game does not influence the outcome of another).\nKasparov has an equal probability of winning any game in the match.\n\ni.e., probability of winning does not increase or decrease as the match goes on.\n\n\nWe will use a binomial model for this problem.\n\nIn our case,\n\n\nY|\\pi \\sim \\text{Bin}(6, \\pi)"
  },
  {
    "objectID": "files/lectures/04-2-thinking-bayesian.html#binomial-data-model-1",
    "href": "files/lectures/04-2-thinking-bayesian.html#binomial-data-model-1",
    "title": "Thinking Like a Bayesian",
    "section": "Binomial Data Model",
    "text": "Binomial Data Model\n\nLet’s assume \\pi = 0.8.\nThe probability that he would win all 6 games is approximately 26%.\n\nf(y=6|\\pi=0.8) = {6 \\choose 6} 0.8^6 (1-0.8)^{6-6}, \n\ndbinom(6, 6, 0.8)\n\n[1] 0.262144"
  },
  {
    "objectID": "files/lectures/04-2-thinking-bayesian.html#binomial-data-model-2",
    "href": "files/lectures/04-2-thinking-bayesian.html#binomial-data-model-2",
    "title": "Thinking Like a Bayesian",
    "section": "Binomial Data Model",
    "text": "Binomial Data Model\n\nLet’s assume \\pi = 0.8.\nThe probability that he would win none of the games is approximately 0%.\n\nf(y=0|\\pi=0.8) = {6 \\choose 0} 0.8^0 (1-0.8)^{6-0}, \n\ndbinom(0, 6, 0.8)\n\n[1] 6.4e-05"
  },
  {
    "objectID": "files/lectures/04-2-thinking-bayesian.html#binomial-data-model-3",
    "href": "files/lectures/04-2-thinking-bayesian.html#binomial-data-model-3",
    "title": "Thinking Like a Bayesian",
    "section": "Binomial Data Model",
    "text": "Binomial Data Model\n\nWe want to reproduce Figure 2.5 from the Bayes Rules! textbook (from Section 2.3.2).\n\n\n\nEach group will complete the graph for a specified value of \\pi.\n\nCampus: \\pi=0.2\nZoom 1: \\pi=0.5\nZoom 2: \\pi=0.8"
  },
  {
    "objectID": "files/lectures/04-2-thinking-bayesian.html#binomial-data-model-4",
    "href": "files/lectures/04-2-thinking-bayesian.html#binomial-data-model-4",
    "title": "Thinking Like a Bayesian",
    "section": "Binomial Data Model",
    "text": "Binomial Data Model\n\nNote that the Binomial gives us the theoretical model of the data we might observe.\n\nKasparov only won one of the six games against Deep Blue in 1997 (Y=1).\n\nNext step: how compatible this particular data is with the various possible \\pi?\n\nWhat is the likelihood of Kasparov winning Y=1 game under each possible \\pi?\n\nRecall, f(y|\\pi) = L(\\pi|Y=y). When Y=1,\n\n\n\\begin{align*}\nL(\\pi | y = 1) &= f(y=1|\\pi) \\\\\n&= {6 \\choose 1} \\pi^1 (1-\\pi)^{6-1} \\\\\n&= 6\\pi(1-\\pi)^5\n\\end{align*}\n\n\nNote that we do not expect all likelihoods to sum to 1."
  },
  {
    "objectID": "files/lectures/04-2-thinking-bayesian.html#binomial-data-model-5",
    "href": "files/lectures/04-2-thinking-bayesian.html#binomial-data-model-5",
    "title": "Thinking Like a Bayesian",
    "section": "Binomial Data Model",
    "text": "Binomial Data Model\n\nUse your results from earlier to tell me the resulting likelihood values.\n\n\n\n\n\n\n\n\n\n\n\\pi\n0.2\n0.5\n0.8\n\n\n\n\nL(\\pi|y=1)"
  },
  {
    "objectID": "files/lectures/04-2-thinking-bayesian.html#binomial-data-model-6",
    "href": "files/lectures/04-2-thinking-bayesian.html#binomial-data-model-6",
    "title": "Thinking Like a Bayesian",
    "section": "Binomial Data Model",
    "text": "Binomial Data Model\n\nUse your results from earlier to tell me the resulting likelihood values.\n\n\n\n\n\\pi\n0.2\n0.5\n0.8\n\n\n\n\nL(\\pi|y=1)\n0.3932\n0.0938\n0.0015\n\n\n\n\nAs we can see, the likelihoods do not sum to 1."
  },
  {
    "objectID": "files/lectures/04-2-thinking-bayesian.html#normalizing-constant",
    "href": "files/lectures/04-2-thinking-bayesian.html#normalizing-constant",
    "title": "Thinking Like a Bayesian",
    "section": "Normalizing Constant",
    "text": "Normalizing Constant\n\nBayes’ Rule requires three pieces of information:\n\nPrior\nLikelihood\nNormalizing constant\n\nNormalizing constant: ensures that the sum of all probabilities is equal to 1.\n\nIt can be a scalar or a function.\nEvery probability distribution that does not sum to 1 will have a normalizing constant."
  },
  {
    "objectID": "files/lectures/04-2-thinking-bayesian.html#normalizing-constant-1",
    "href": "files/lectures/04-2-thinking-bayesian.html#normalizing-constant-1",
    "title": "Thinking Like a Bayesian",
    "section": "Normalizing Constant",
    "text": "Normalizing Constant\n\nWe now must determine the total probability that Kasparov would win Y=1 games across all possible win probabilities \\pi, f(y=1).\n\n\n\\begin{align*}\nf(y=1) =& \\sum_{\\pi} L(\\pi |y=1)f(\\pi) \\\\\n=& L(\\pi=0.2|y=1)f(\\pi=0.2) + L(\\pi=0.5|y=1)f(\\pi=0.5) + \\\\\n& L(\\pi=0.8|y=1)f(\\pi=0.8)\n\\end{align*}\n\n\nWork with your group to find the normalizing constant."
  },
  {
    "objectID": "files/lectures/04-2-thinking-bayesian.html#normalizing-constant-2",
    "href": "files/lectures/04-2-thinking-bayesian.html#normalizing-constant-2",
    "title": "Thinking Like a Bayesian",
    "section": "Normalizing Constant",
    "text": "Normalizing Constant\n\nWe now must determine the total probability that Kasparov would win Y=1 games across all possible win probabilities \\pi, f(y=1).\n\n\n\\begin{align*}\nf(y=1) =& \\sum_{\\pi} L(\\pi |y=1)f(\\pi) \\\\\n=& L(\\pi=0.2|y=1)f(\\pi=0.2) + L(\\pi=0.5|y=1)f(\\pi=0.5) + \\\\\n& L(\\pi=0.8|y=1)f(\\pi=0.8) \\\\\n\\approx& 0.3932 \\cdot 0.10 + 0.0938 \\cdot 0.25 + 0.0015 \\cdot 0.65 \\\\\n\\approx& 0.0637\n\\end{align*}\n\n\nAcross all possible values of \\pi, there is about a 6% chance that Kasparov would have won only one game."
  },
  {
    "objectID": "files/lectures/04-2-thinking-bayesian.html#posterior-probability-model",
    "href": "files/lectures/04-2-thinking-bayesian.html#posterior-probability-model",
    "title": "Thinking Like a Bayesian",
    "section": "Posterior Probability Model",
    "text": "Posterior Probability Model\n\nNow recall,\n\n\\text{posterior} = \\frac{\\text{prior} \\times \\text{likelihood}}{\\text{normalizing constant}}\n\nIn our example, where y = 1,\n\nf(\\pi | y=1) = \\frac{f(\\pi) L(\\pi | y = 1)}{f(y=1)} \\  \\text{for} \\ \\pi \\in \\{ 0.2, 0.5, 0.8\\}\n\nWork with your group to find the posterior probabilities.\n\nYou will have one posterior probability for each value of \\pi."
  },
  {
    "objectID": "files/lectures/04-2-thinking-bayesian.html#posterior-probability-model-1",
    "href": "files/lectures/04-2-thinking-bayesian.html#posterior-probability-model-1",
    "title": "Thinking Like a Bayesian",
    "section": "Posterior Probability Model",
    "text": "Posterior Probability Model\n\nNote!! We do not have to calculate the normalizing constant!\nWe can note that f(Y=y) = 1/c.\nThen, we say that\n\n\n\\begin{align*}\nf(\\pi | y) &= \\frac{f(\\pi) L(\\pi|y)}{f(y)} \\\\\n& \\propto  f(\\pi) L(\\pi|y) \\\\\n\\\\\n\\text{posterior} &\\propto \\text{prior} \\cdot \\text{likelihood}\n\\end{align*}"
  },
  {
    "objectID": "files/lectures/04-2-thinking-bayesian.html#wrap-up",
    "href": "files/lectures/04-2-thinking-bayesian.html#wrap-up",
    "title": "Thinking Like a Bayesian",
    "section": "Wrap Up",
    "text": "Wrap Up\n\nToday we learned how to, in general, approach Bayesian analysis.\nNext week, we will formalize what we observed today and learn about the conjugate families.\n\nBeta-binomial\nGamma-Poisson\nNormal Normal"
  },
  {
    "objectID": "files/lectures/04-2-thinking-bayesian.html#homework-practice",
    "href": "files/lectures/04-2-thinking-bayesian.html#homework-practice",
    "title": "Thinking Like a Bayesian",
    "section": "Homework / Practice",
    "text": "Homework / Practice\n\nFrom the Bayes Rules! textbook:\n\n1.3\n1.4\n1.8\n2.4\n2.9"
  },
  {
    "objectID": "files/lectures/05-2-balance-squentiality.html#introduction",
    "href": "files/lectures/05-2-balance-squentiality.html#introduction",
    "title": "Balance and Sequentiality in Bayesian Analysis",
    "section": "Introduction",
    "text": "Introduction\n\nOn Monday, we talked about the Beta-Binomial model for binary outcomes with an unknown probability of success, \\pi.\nWe will now discuss sequentality in Bayesian analyses.\nWorking example:\n\nIn Alison Bechdel’s 1985 comic strip The Rule, a character states that they only see a movie if it satisfies the following three rules (Bechdel 1986):\n\nthe movie has to have at least two women in it;\nthese two women talk to each other; and\nthey talk about something besides a man.\n\nThinking of movies you’ve watched, what percentage of all recent movies do you think pass the Bechdel test? Is it closer to 10%, 50%, 80%, or 100%?"
  },
  {
    "objectID": "files/lectures/05-2-balance-squentiality.html#introduction-example",
    "href": "files/lectures/05-2-balance-squentiality.html#introduction-example",
    "title": "Balance and Sequentiality in Bayesian Analysis",
    "section": "Introduction: Example",
    "text": "Introduction: Example\n\nLet \\pi, a random value between 0 and 1, denote the unknown proportion of recent movies that pass the Bechdel test.\nThree friends (feminist, clueless, and optimist) have some prior ideas about \\pi.\n\nReflecting upon movies that he has seen in the past, the feminist understands that the majority lack strong women characters.\nThe clueless doesn’t really recall the movies they’ve seen, and so are unsure whether passing the Bechdel test is common or uncommon.\nLastly, the optimist thinks that the Bechdel test is a really low bar for the representation of women in film, and thus assumes almost all movies pass the test."
  },
  {
    "objectID": "files/lectures/05-2-balance-squentiality.html#introduction-example-1",
    "href": "files/lectures/05-2-balance-squentiality.html#introduction-example-1",
    "title": "Balance and Sequentiality in Bayesian Analysis",
    "section": "Introduction: Example",
    "text": "Introduction: Example\n\nGraph the following priors:\n\n\nplot_beta(alpha = 1, beta = 1)\nplot_beta(alpha = 5, beta = 11)\nplot_beta(alpha = 14, beta = 1)\n\n\nWhich prior belongs to each friend?\n\nReflecting upon movies that he has seen in the past, the feminist understands that the majority lack strong women characters.\nThe clueless doesn’t really recall the movies they’ve seen, and so are unsure whether passing the Bechdel test is common or uncommon.\nLastly, the optimist thinks that the Bechdel test is a really low bar for the representation of women in film, and thus assumes almost all movies pass the test."
  },
  {
    "objectID": "files/lectures/05-2-balance-squentiality.html#introduction-example-2",
    "href": "files/lectures/05-2-balance-squentiality.html#introduction-example-2",
    "title": "Balance and Sequentiality in Bayesian Analysis",
    "section": "Introduction: Example",
    "text": "Introduction: Example\n\nThe clueless doesn’t really recall the movies they’ve seen, and so are unsure whether passing the Bechdel test is common or uncommon."
  },
  {
    "objectID": "files/lectures/05-2-balance-squentiality.html#introduction-example-3",
    "href": "files/lectures/05-2-balance-squentiality.html#introduction-example-3",
    "title": "Balance and Sequentiality in Bayesian Analysis",
    "section": "Introduction: Example",
    "text": "Introduction: Example\n\nReflecting upon movies that he has seen in the past, the feminist understands that the majority lack strong women characters."
  },
  {
    "objectID": "files/lectures/05-2-balance-squentiality.html#introduction-example-4",
    "href": "files/lectures/05-2-balance-squentiality.html#introduction-example-4",
    "title": "Balance and Sequentiality in Bayesian Analysis",
    "section": "Introduction: Example",
    "text": "Introduction: Example\n\nLastly, the optimist thinks that the Bechdel test is a really low bar for the representation of women in film, and thus assumes almost all movies pass the test."
  },
  {
    "objectID": "files/lectures/05-2-balance-squentiality.html#introduction-example-5",
    "href": "files/lectures/05-2-balance-squentiality.html#introduction-example-5",
    "title": "Balance and Sequentiality in Bayesian Analysis",
    "section": "Introduction: Example",
    "text": "Introduction: Example\n\nThe analysts agree to review a sample of n recent movies and record Y, the number that pass the Bechdel test.\n\nBecause the outcome is yes/no, the binomial distribution is appropriate for the data distribution.\nWe aren’t sure what the population proportion, \\pi, is, so we will not restrict it to a fixed value.\n\nBecause we know \\pi \\in [0, 1], the beta distribution is appropriate for the prior distribution.\n\n\n\n\n\\begin{align*}\nY|\\pi &\\sim \\text{Bin}(n, \\pi) \\\\\n\\pi &\\sim \\text{Beta}(\\alpha, \\beta)\n\\end{align*}"
  },
  {
    "objectID": "files/lectures/05-2-balance-squentiality.html#introduction-example-6",
    "href": "files/lectures/05-2-balance-squentiality.html#introduction-example-6",
    "title": "Balance and Sequentiality in Bayesian Analysis",
    "section": "Introduction: Example",
    "text": "Introduction: Example\n\nBecause we know \\pi \\in [0, 1], the beta distribution is appropriate for the prior distribution.\n\n\n\\begin{align*}\nY|\\pi &\\sim \\text{Bin}(n, \\pi) \\\\\n\\pi &\\sim \\text{Beta}(\\alpha, \\beta)\n\\end{align*}\n\n\nFrom the previous chapter, we know that this results in the following posterior distribution\n\n\n\\pi | (Y=y) \\sim \\text{Beta}(\\alpha+y, \\beta+n-y)"
  },
  {
    "objectID": "files/lectures/05-2-balance-squentiality.html#introduction-example-7",
    "href": "files/lectures/05-2-balance-squentiality.html#introduction-example-7",
    "title": "Balance and Sequentiality in Bayesian Analysis",
    "section": "Introduction: Example",
    "text": "Introduction: Example\n\nWait!!\n\nEveryone gets their own prior?\n… is there a “correct” prior?\n…… is the Bayesian world always this subjective?"
  },
  {
    "objectID": "files/lectures/05-2-balance-squentiality.html#introduction-example-8",
    "href": "files/lectures/05-2-balance-squentiality.html#introduction-example-8",
    "title": "Balance and Sequentiality in Bayesian Analysis",
    "section": "Introduction: Example",
    "text": "Introduction: Example\n\nMore clearly defined questions that we can actually answer:\n\nTo what extent might different priors lead the analysts to three different posterior conclusions about the Bechdel test?\n\nHow might this depend upon the sample size and outcomes of the movie data they collect?\n\nTo what extent will the analysts’ posterior understandings evolve as they collect more and more data?\nWill they ever come to agreement about the representation of women in film?!"
  },
  {
    "objectID": "files/lectures/05-2-balance-squentiality.html#different-priors-to-different-posteriors",
    "href": "files/lectures/05-2-balance-squentiality.html#different-priors-to-different-posteriors",
    "title": "Balance and Sequentiality in Bayesian Analysis",
    "section": "Different Priors \\to Different Posteriors",
    "text": "Different Priors \\to Different Posteriors\n\n\nThe differing prior means show disagreement about whether \\pi is closer to 0 or 1.\nThe differing levels of prior variability show that the analysts have different degrees of certainty in their prior information."
  },
  {
    "objectID": "files/lectures/05-2-balance-squentiality.html#different-priors-to-different-posteriors-1",
    "href": "files/lectures/05-2-balance-squentiality.html#different-priors-to-different-posteriors-1",
    "title": "Balance and Sequentiality in Bayesian Analysis",
    "section": "Different Priors \\to Different Posteriors",
    "text": "Different Priors \\to Different Posteriors\n\n\nInformative prior: reflects specific information about the unknown variable with high certainty, i.e., low variability."
  },
  {
    "objectID": "files/lectures/05-2-balance-squentiality.html#different-priors-to-different-posteriors-2",
    "href": "files/lectures/05-2-balance-squentiality.html#different-priors-to-different-posteriors-2",
    "title": "Balance and Sequentiality in Bayesian Analysis",
    "section": "Different Priors \\to Different Posteriors",
    "text": "Different Priors \\to Different Posteriors\n\n\nVague or diffuse prior: reflects little specific information about the unknown variable.\n\nA flat prior, which assigns equal prior plausibility to all possible values of the variable, is a special case.\nThis is effectively saying “🤷.”"
  },
  {
    "objectID": "files/lectures/05-2-balance-squentiality.html#different-priors-to-different-posteriors-3",
    "href": "files/lectures/05-2-balance-squentiality.html#different-priors-to-different-posteriors-3",
    "title": "Balance and Sequentiality in Bayesian Analysis",
    "section": "Different Priors \\to Different Posteriors",
    "text": "Different Priors \\to Different Posteriors\n\nOkay, great - we have different priors.\n\nHow do the different priors affect the posterior?\n\nWe have data from FiveThirtyEight, reporting results of the Bechdel test."
  },
  {
    "objectID": "files/lectures/05-2-balance-squentiality.html#different-priors-to-different-posteriors-4",
    "href": "files/lectures/05-2-balance-squentiality.html#different-priors-to-different-posteriors-4",
    "title": "Balance and Sequentiality in Bayesian Analysis",
    "section": "Different Priors \\to Different Posteriors",
    "text": "Different Priors \\to Different Posteriors\n\nSo how many pass the test in this sample?\n\n\nbechdel20 %&gt;% tabyl(binary) %&gt;% adorn_totals(\"row\")"
  },
  {
    "objectID": "files/lectures/05-2-balance-squentiality.html#different-priors-to-different-posteriors-5",
    "href": "files/lectures/05-2-balance-squentiality.html#different-priors-to-different-posteriors-5",
    "title": "Balance and Sequentiality in Bayesian Analysis",
    "section": "Different Priors \\to Different Posteriors",
    "text": "Different Priors \\to Different Posteriors\n\nLet’s look at the graphs of just the prior and likelihood.\n\n\nplot_beta_binomial(alpha = 5, beta = 11, y = 9, n = 20, posterior = FALSE) + theme_bw()\nplot_beta_binomial(alpha = 1, beta = 1, y = 9, n = 20, posterior = FALSE) + theme_bw()\nplot_beta_binomial(alpha = 14, beta = 1, y = 9, n = 20, posterior = FALSE) + theme_bw()\n\n\nQuestions to think about:\n\nWhose posterior do you anticipate will look the most like the scaled likelihood?\nWhose do you anticipate will look the least like the scaled likelihood?"
  },
  {
    "objectID": "files/lectures/05-2-balance-squentiality.html#different-priors-to-different-posteriors-6",
    "href": "files/lectures/05-2-balance-squentiality.html#different-priors-to-different-posteriors-6",
    "title": "Balance and Sequentiality in Bayesian Analysis",
    "section": "Different Priors \\to Different Posteriors",
    "text": "Different Priors \\to Different Posteriors\n\nLet’s look at the graphs of just the prior and likelihood."
  },
  {
    "objectID": "files/lectures/05-2-balance-squentiality.html#different-priors-to-different-posteriors-7",
    "href": "files/lectures/05-2-balance-squentiality.html#different-priors-to-different-posteriors-7",
    "title": "Balance and Sequentiality in Bayesian Analysis",
    "section": "Different Priors \\to Different Posteriors",
    "text": "Different Priors \\to Different Posteriors\n\nLet’s look at the graphs of just the prior and likelihood."
  },
  {
    "objectID": "files/lectures/05-2-balance-squentiality.html#different-priors-to-different-posteriors-8",
    "href": "files/lectures/05-2-balance-squentiality.html#different-priors-to-different-posteriors-8",
    "title": "Balance and Sequentiality in Bayesian Analysis",
    "section": "Different Priors \\to Different Posteriors",
    "text": "Different Priors \\to Different Posteriors\n\nLet’s look at the graphs of just the prior and likelihood."
  },
  {
    "objectID": "files/lectures/05-2-balance-squentiality.html#different-priors-to-different-posteriors-9",
    "href": "files/lectures/05-2-balance-squentiality.html#different-priors-to-different-posteriors-9",
    "title": "Balance and Sequentiality in Bayesian Analysis",
    "section": "Different Priors \\to Different Posteriors",
    "text": "Different Priors \\to Different Posteriors\n\nFind the posterior distributions. (i.e., What are the updated parameters?)\n\n\n\n\n\n\nAnalyst\n\n\nPrior\n\n\nPosterior\n\n\n\n\n\n\nthe feminist\n\n\nBeta(5, 11)\n\n\nBeta(14, 22)\n\n\n\n\nthe clueless\n\n\nBeta(1, 1)\n\n\nBeta(10, 12)\n\n\n\n\nthe optimist\n\n\nBeta(14, 1)\n\n\nBeta(23, 12)\n\n\n\n\n\n\nLet’s now explore what the posteriors look like.\n\n\nplot_beta_binomial(alpha = 5, beta = 11, y = 9, n = 20) + theme_bw()\nplot_beta_binomial(alpha = 1, beta = 1, y = 9, n = 20) + theme_bw()\nplot_beta_binomial(alpha = 14, beta = 1, y = 9, n = 20) + theme_bw()"
  },
  {
    "objectID": "files/lectures/05-2-balance-squentiality.html#different-priors-to-different-posteriors-10",
    "href": "files/lectures/05-2-balance-squentiality.html#different-priors-to-different-posteriors-10",
    "title": "Balance and Sequentiality in Bayesian Analysis",
    "section": "Different Priors \\to Different Posteriors",
    "text": "Different Priors \\to Different Posteriors\n\nLet’s now explore what the posteriors look like."
  },
  {
    "objectID": "files/lectures/05-2-balance-squentiality.html#different-priors-to-different-posteriors-11",
    "href": "files/lectures/05-2-balance-squentiality.html#different-priors-to-different-posteriors-11",
    "title": "Balance and Sequentiality in Bayesian Analysis",
    "section": "Different Priors \\to Different Posteriors",
    "text": "Different Priors \\to Different Posteriors\n\nLet’s now explore what the posteriors look like."
  },
  {
    "objectID": "files/lectures/05-2-balance-squentiality.html#different-priors-to-different-posteriors-12",
    "href": "files/lectures/05-2-balance-squentiality.html#different-priors-to-different-posteriors-12",
    "title": "Balance and Sequentiality in Bayesian Analysis",
    "section": "Different Priors \\to Different Posteriors",
    "text": "Different Priors \\to Different Posteriors\n\nLet’s now explore what the posteriors look like."
  },
  {
    "objectID": "files/lectures/05-2-balance-squentiality.html#different-priors-to-different-posteriors-13",
    "href": "files/lectures/05-2-balance-squentiality.html#different-priors-to-different-posteriors-13",
    "title": "Balance and Sequentiality in Bayesian Analysis",
    "section": "Different Priors \\to Different Posteriors",
    "text": "Different Priors \\to Different Posteriors\n\nIn addition to priors affecting our posterior distributions… the data also affects it.\nLet’s now consider three new analysts: they all share the optimistic Beta(14, 1) for \\pi, however, they have access to different data.\n\nMorteza reviews n = 13 movies from the year 1991, among which Y=6 (about 46%) pass the Bechdel.\nNadide reviews n = 63 movies from the year 2001, among which Y=29 (about 46%) pass the Bechdel.\nUrsula reviews n = 99 movies from the year 2013, among which Y=46 (about 46%) pass the Bechdel.\n\nHow will the different data affect the posterior distributions?"
  },
  {
    "objectID": "files/lectures/05-2-balance-squentiality.html#different-priors-to-different-posteriors-14",
    "href": "files/lectures/05-2-balance-squentiality.html#different-priors-to-different-posteriors-14",
    "title": "Balance and Sequentiality in Bayesian Analysis",
    "section": "Different Priors \\to Different Posteriors",
    "text": "Different Priors \\to Different Posteriors\n\nHow will the different data affect the posterior distributions?\n\n\nplot_beta_binomial(alpha = 14, beta = 1, y = 6, n = 13, posterior = FALSE) + theme_bw()\nplot_beta_binomial(alpha = 14, beta = 1, y = 29, n = 63, posterior = FALSE) + theme_bw()\nplot_beta_binomial(alpha = 14, beta = 1, y = 46, n = 99, posterior = FALSE) + theme_bw()\n\n\nWhich posterior is the most in sync with their data?\nWhich posterior is the least in sync with their data?"
  },
  {
    "objectID": "files/lectures/05-2-balance-squentiality.html#different-priors-to-different-posteriors-15",
    "href": "files/lectures/05-2-balance-squentiality.html#different-priors-to-different-posteriors-15",
    "title": "Balance and Sequentiality in Bayesian Analysis",
    "section": "Different Priors \\to Different Posteriors",
    "text": "Different Priors \\to Different Posteriors\n\nHow will the different data affect the posterior distributions?"
  },
  {
    "objectID": "files/lectures/05-2-balance-squentiality.html#different-priors-to-different-posteriors-16",
    "href": "files/lectures/05-2-balance-squentiality.html#different-priors-to-different-posteriors-16",
    "title": "Balance and Sequentiality in Bayesian Analysis",
    "section": "Different Priors \\to Different Posteriors",
    "text": "Different Priors \\to Different Posteriors\n\nHow will the different data affect the posterior distributions?"
  },
  {
    "objectID": "files/lectures/05-2-balance-squentiality.html#different-priors-to-different-posteriors-17",
    "href": "files/lectures/05-2-balance-squentiality.html#different-priors-to-different-posteriors-17",
    "title": "Balance and Sequentiality in Bayesian Analysis",
    "section": "Different Priors \\to Different Posteriors",
    "text": "Different Priors \\to Different Posteriors\n\nHow will the different data affect the posterior distributions?"
  },
  {
    "objectID": "files/lectures/05-2-balance-squentiality.html#different-priors-to-different-posteriors-18",
    "href": "files/lectures/05-2-balance-squentiality.html#different-priors-to-different-posteriors-18",
    "title": "Balance and Sequentiality in Bayesian Analysis",
    "section": "Different Priors \\to Different Posteriors",
    "text": "Different Priors \\to Different Posteriors\n\nFind the posterior distributions. (i.e., What are the updated parameters?)\n\nRecall that all use the Beta(14, 1) prior.\n\n\n\n\n\n\n\n\nAnalyst\n\n\n\n\nData\n\n\n\n\nPosterior\n\n\n\n\n\n\n\nMorteza\n\n\nY=6 of n=13\n\n\nBeta(20, 8)\n\n\n\n\nNadide\n\n\nY=29 of n=63\n\n\nBeta(45, 35)\n\n\n\n\nUrsula\n\n\nY=46 of n=99\n\n\nBeta(60, 54)\n\n\n\n\n\n\nLet’s also explore what the posteriors look like.\n\n\nplot_beta_binomial(alpha = 14, beta = 1, y = 6, n = 13) + theme_bw() \nplot_beta_binomial(alpha = 14, beta = 1, y = 29, n = 63) + theme_bw()\nplot_beta_binomial(alpha = 14, beta = 1, y = 46, n = 99) + theme_bw()"
  },
  {
    "objectID": "files/lectures/05-2-balance-squentiality.html#different-priors-to-different-posteriors-19",
    "href": "files/lectures/05-2-balance-squentiality.html#different-priors-to-different-posteriors-19",
    "title": "Balance and Sequentiality in Bayesian Analysis",
    "section": "Different Priors \\to Different Posteriors",
    "text": "Different Priors \\to Different Posteriors\n\nLet’s explore what the posteriors look like."
  },
  {
    "objectID": "files/lectures/05-2-balance-squentiality.html#different-priors-to-different-posteriors-20",
    "href": "files/lectures/05-2-balance-squentiality.html#different-priors-to-different-posteriors-20",
    "title": "Balance and Sequentiality in Bayesian Analysis",
    "section": "Different Priors \\to Different Posteriors",
    "text": "Different Priors \\to Different Posteriors\n\nLet’s explore what the posteriors look like."
  },
  {
    "objectID": "files/lectures/05-2-balance-squentiality.html#different-priors-to-different-posteriors-21",
    "href": "files/lectures/05-2-balance-squentiality.html#different-priors-to-different-posteriors-21",
    "title": "Balance and Sequentiality in Bayesian Analysis",
    "section": "Different Priors \\to Different Posteriors",
    "text": "Different Priors \\to Different Posteriors\n\nLet’s explore what the posteriors look like."
  },
  {
    "objectID": "files/lectures/05-2-balance-squentiality.html#different-priors-to-different-posteriors-22",
    "href": "files/lectures/05-2-balance-squentiality.html#different-priors-to-different-posteriors-22",
    "title": "Balance and Sequentiality in Bayesian Analysis",
    "section": "Different Priors \\to Different Posteriors",
    "text": "Different Priors \\to Different Posteriors\n\nWhat did we observe?\n\nAs n \\to \\infty, variance in the likelihood \\to 0.\n\nIn Morteza’s small sample of 13 movies, the likelihood function is wide.\nIn Ursula’s larger sample size of 99 movies, the likelihood function is narrower.\n\nWe see that the narrower the likelihood, the more influence the data holds over the posterior."
  },
  {
    "objectID": "files/lectures/05-2-balance-squentiality.html#striking-a-balance",
    "href": "files/lectures/05-2-balance-squentiality.html#striking-a-balance",
    "title": "Balance and Sequentiality in Bayesian Analysis",
    "section": "Striking a Balance",
    "text": "Striking a Balance\n\n\nOverall message: no matter the strength of and discrepancies among their prior understanding of \\pi, analysts will come to a common posterior understanding in light of strong data."
  },
  {
    "objectID": "files/lectures/05-2-balance-squentiality.html#striking-a-balance-1",
    "href": "files/lectures/05-2-balance-squentiality.html#striking-a-balance-1",
    "title": "Balance and Sequentiality in Bayesian Analysis",
    "section": "Striking a Balance",
    "text": "Striking a Balance\n\nThe posterior can either favor the data or the prior.\n\nThe rate at which the posterior balance tips in favor of the data depends upon the prior.\n\nLeft to right on the graph, the sample size increases from n=13 to n=99 movies, while preserving the proportion that pass (\\approx 0.46).\n\nThe likelihood’s insistence and the data’s influence over the posterior increase with sample size.\nThis also means that the influence of our prior understanding diminishes as we gather new data.\n\nTop to bottom on the graph, priors move from informative (Beta(14,1)) to vague (Beta(1,1)).\n\nNaturally, the more informative the prior, the greater its influence on the posterior."
  },
  {
    "objectID": "files/lectures/05-2-balance-squentiality.html#introduction-sequentiality",
    "href": "files/lectures/05-2-balance-squentiality.html#introduction-sequentiality",
    "title": "Balance and Sequentiality in Bayesian Analysis",
    "section": "Introduction: Sequentiality",
    "text": "Introduction: Sequentiality\n\nLet’s now turn our thinking to - okay, we’ve updated our beliefs… but now we have new data!\nThe evolution in our posterior understanding happens incrementally, as we accumulate new data.\n\nScientists’ understanding of climate change has evolved over the span of decades as they gain new information.\nPresidential candidates’ understanding of their chances of winning an election evolve over months as new poll results become available."
  },
  {
    "objectID": "files/lectures/05-2-balance-squentiality.html#introduction-sequentiality-1",
    "href": "files/lectures/05-2-balance-squentiality.html#introduction-sequentiality-1",
    "title": "Balance and Sequentiality in Bayesian Analysis",
    "section": "Introduction: Sequentiality",
    "text": "Introduction: Sequentiality\n\nLet’s revisit Milgram’s behavioral study of obedience from Chapter 3. Recall, \\pi represents the proportion of people that will obey authority, even if it means bringing harm to others.\nPrior to Milgram’s experiments, our fictional psychologist expected that few people would obey authority in the face of harming another: \\pi \\sim \\text{Beta}(1,10).\nNow, suppose that the psychologist collected the data incrementally, day by day, over a three-day period.\nFind the following posterior distributions, each building off the last:\n\nDay 0: \\text{Beta}(1,10).\nDay 1: Y=1 out of n=10.\nDay 2: Y=17 out of n=20.\nDay 3: Y=8 out of n=10."
  },
  {
    "objectID": "files/lectures/05-2-balance-squentiality.html#introduction-sequentiality-2",
    "href": "files/lectures/05-2-balance-squentiality.html#introduction-sequentiality-2",
    "title": "Balance and Sequentiality in Bayesian Analysis",
    "section": "Introduction: Sequentiality",
    "text": "Introduction: Sequentiality\n\nFind the following posterior distributions, each building off the last:\n\nDay 0: \\text{Beta}(1,10).\nDay 1: Y=1 out of n=10: \\text{Beta}(1,10) \\to \\text{Beta}(2, 19).\nDay 2: Y=17 out of n=20: \\text{Beta}(2, 19) \\to \\text{Beta}(19, 22).\nDay 3: Y=8 out of n=10: \\text{Beta}(19, 22) \\to \\text{Beta}(27, 24).\n\nRecall from Chapter 3, our posterior was \\text{Beta}(27,24)!"
  },
  {
    "objectID": "files/lectures/05-2-balance-squentiality.html#sequential-bayesian-analysis-or-bayesian-learning",
    "href": "files/lectures/05-2-balance-squentiality.html#sequential-bayesian-analysis-or-bayesian-learning",
    "title": "Balance and Sequentiality in Bayesian Analysis",
    "section": "Sequential Bayesian Analysis or Bayesian Learning",
    "text": "Sequential Bayesian Analysis or Bayesian Learning\n\nIn a sequential Bayesian analysis, a posterior model is updated incrementally as more data come in.\n\nWith each new piece of data, the previous posterior model reflecting our understanding prior to observing this data becomes the new prior model.\n\nThis is why we love Bayesian!\n\nWe evolve our thinking as new data come in.\n\nThese types of sequential analyses also uphold two fundamental properties:\n\nThe final posterior model is data order invariant,\n\nThe final posterior only depends upon the cumulative data."
  },
  {
    "objectID": "files/lectures/05-2-balance-squentiality.html#sequential-bayesian-analysis-or-bayesian-learning-1",
    "href": "files/lectures/05-2-balance-squentiality.html#sequential-bayesian-analysis-or-bayesian-learning-1",
    "title": "Balance and Sequentiality in Bayesian Analysis",
    "section": "Sequential Bayesian Analysis or Bayesian Learning",
    "text": "Sequential Bayesian Analysis or Bayesian Learning\n\nIn order:\n\nDay 0: \\text{Beta}(1,10).\nDay 1: Y=1 out of n=10: \\text{Beta}(1,10) \\to \\text{Beta}(2, 19).\nDay 2: Y=17 out of n=20: \\text{Beta}(2, 19) \\to \\text{Beta}(19, 22).\nDay 3: Y=8 out of n=10: \\text{Beta}(19, 22) \\to \\text{Beta}(27, 24).\n\nOut of order:\n\nDay 0: \\text{Beta}(1,10).\nDay 3: Y=8 out of n=10: \\text{Beta}(1,10) \\to \\text{Beta}(9, 12).\nDay 2: Y=17 out of n=20: \\text{Beta}(9, 12) \\to \\text{Beta}(26, 15).\nDay 1: Y=1 out of n=10: \\text{Beta}(26, 15) \\to \\text{Beta}(27, 24)."
  },
  {
    "objectID": "files/lectures/05-2-balance-squentiality.html#sequential-bayesian-analysis-or-bayesian-learning-2",
    "href": "files/lectures/05-2-balance-squentiality.html#sequential-bayesian-analysis-or-bayesian-learning-2",
    "title": "Balance and Sequentiality in Bayesian Analysis",
    "section": "Sequential Bayesian Analysis or Bayesian Learning",
    "text": "Sequential Bayesian Analysis or Bayesian Learning"
  },
  {
    "objectID": "files/lectures/05-2-balance-squentiality.html#example-mario-kart",
    "href": "files/lectures/05-2-balance-squentiality.html#example-mario-kart",
    "title": "Balance and Sequentiality in Bayesian Analysis",
    "section": "Example: Mario Kart",
    "text": "Example: Mario Kart\n\nIn Mario Kart 8 Deluxe, item boxes give different items depending on race position. To reduce the “position bias,” only item boxes opened while the racer was in mid-pack (positions 4–10) were recorded.\nYou want to estimate the probability that an item box yields a Red Shell. When playing the Special Cup, only 31 red shells were seen in 114 boxes opened by mid-pack racers.\nFind the posterior distribution under two priors:\n\nFlat/uninformative prior, Beta(1,1).\nBeta(\\alpha, \\beta) of your choosing.\n\nHow different are the posterior distributions?"
  },
  {
    "objectID": "files/lectures/05-2-balance-squentiality.html#example-mario-kart-1",
    "href": "files/lectures/05-2-balance-squentiality.html#example-mario-kart-1",
    "title": "Balance and Sequentiality in Bayesian Analysis",
    "section": "Example: Mario Kart",
    "text": "Example: Mario Kart\n\nSuppose that we know the individual data points for the Special Cup:\n\nCloudtop Cruise, of 28 boxes, 6 had a red shell.\nBone-Dry Dunes, of 27 boxes, 8 had a red shell.\nBowser’s Castle, of 29 boxes, 12 had a red shell.\nRainbow Road, of 30 boxes, 5 had a red shell.\n\nProve sequentiality to yourself.\n\nAnalyze the data race-by-race in this order: Cloudtop Cruise, Bone-Dry Dunes, Bowser’s Castle, and Rainbow Road.\nAnalyze the data race-by-race in this order: Rainbow Road, Bowser’s Castle, Cloudtop Cruise, and Bone-Dry Dunes."
  },
  {
    "objectID": "files/lectures/05-2-balance-squentiality.html#wrap-up",
    "href": "files/lectures/05-2-balance-squentiality.html#wrap-up",
    "title": "Balance and Sequentiality in Bayesian Analysis",
    "section": "Wrap Up",
    "text": "Wrap Up\n\nToday we have discussed balance and sequentiality.\nRemember that order of data inclusion does not matter – we will end up with the same posterior.\nWe have seen that prior specification “matters” but there will not be a large difference in the posterior distribution when priors are more similar to one another.\nNext week:\n\nGamma-Poisson\nNormal-Normal\nWhat to do with the posterior distribution."
  },
  {
    "objectID": "files/lectures/05-2-balance-squentiality.html#homework-practice",
    "href": "files/lectures/05-2-balance-squentiality.html#homework-practice",
    "title": "Balance and Sequentiality in Bayesian Analysis",
    "section": "Homework / Practice",
    "text": "Homework / Practice\n\nFrom the Bayes Rules! textbook:\n\n4.3\n4.4\n4.6\n4.9\n4.15\n4.16\n4.17\n4.18\n4.19"
  },
  {
    "objectID": "files/assignments/assignment1.html",
    "href": "files/assignments/assignment1.html",
    "title": "Assignment 1: Probability Distributions",
    "section": "",
    "text": "Note: do not read too much into the following situations. The purpose is to get comfortable identifying distributions and finding corresponding probabilities.\nSituation 1: Walter is known for his intense personality during bowling league nights. Whether it’s a foot foul, a scoring dispute, or just general frustration with life, he frequently loses his temper and yells loudly at his teammates or opponents. Based on observations over several league nights, it has been determined that Walter loses his temper at an average rate of 4 outbursts per hour. His outbursts are unpredictable and can happen at any point during the game, but they seem to occur independently of each other.\na. What is the random variable of interest? (What is being measured?)\nReplace with your answer\nb. What distribution is appropriate and why?\nReplace with your answer\nc. What are the parameters of the distribution?\nReplace with your answer\nd. What is the probability that Walter will not have an outburst at all?\nReplace with your answer (i.e., don’t just show me R output here :))\ne. What is the probability that Walter has more than 10 outbursts during league night?\nReplace with your answer (i.e., don’t just show me R output here :))\nScenario 2: Donny often struggles to find parking near the bowling alley, especially on busy league nights. Sometimes he gets lucky and finds a spot fairly quickly, but other times he ends up circling the block several times before he can finally park. His parking times are generally right-skewed as it is much more common for it to take a shorter amount of time… but every now and then it can take quite a while. Over time, his friends have noticed a pattern: it usually takes him a few minutes, but there is a decent chance it could take significantly longer.\na. What is the random variable of interest? (What is being measured?)\nReplace with your answer\nb. What distribution is appropriate and why?\nReplace with your answer\nc. What are the parameters of the distribution? (Hint: for this situation, they are 3 and 4 (in that order)).\nReplace with your answer\nd. What is the probability that Donny will take more than 15 minutes to find a spot?\nReplace with your answer (i.e., don’t just show me R output here :))\ne. What is the probability that Donny will find a parking space quickly – i.e., that he parks within 5 minutes of arriving?\nReplace with your answer (i.e., don’t just show me R output here :))\nSituation 3: Maude is famous for her avant-garde dance performances that leave audiences both puzzled and mesmerized. Her performances at the local art venue are known for their spontaneity and lack of strict timing. Observers have noted that the duration of her performances can last anywhere between 15 and 30 minutes, with any length within this interval being equally likely. There’s no predicting exactly how long Maude will choose to perform on any given night… sometimes it’s a brief expression and sometimes it stretches to the edge of the audience’s patience.\na. What is the random variable of interest? (What is being measured?)\nReplace with your answer\nb. What distribution is appropriate and why?\nReplace with your answer\nc. What are the parameters of the distribution?\nReplace with your answer\nd. What is the probability that Maude’s performance will be on the shorter end – i.e., it lasts somewhere between 15 and 16 minutes?\nReplace with your answer (i.e., don’t just show me R output here :))\ne. What is the probability that Maude’s performance will be on the longer end – i.e., it lasts somewhere between 25 and 30 minutes?\nReplace with your answer (i.e., don’t just show me R output here :))\nScenario 4. The Dude always orders a White Russian when he’s at the bowling alley. However, the bowling alley bar is not always well-stocked. Each time The Dude places an order, there is only a 60% chance that the bartender has all the necessary ingredients on hand to make the drink (they are frequently out of cream). Throughout one particularly long evening of bowling and philosophical debates, The Dude orders 8 White Russians. He hopes that most of them will be successfully made.\na. What is the random variable of interest? (What is being measured?)\nReplace with your answer\nb. What distribution is appropriate and why?\nReplace with your answer\nc. What are the parameters of the distribution?\nReplace with your answer\nd. What is the probability that at least 5 drinks will be successfully made?\nReplace with your answer (i.e., don’t just show me R output here :))\ne. What is the probability that all 8 drinks are successfully made?\nReplace with your answer (i.e., don’t just show me R output here :))\nScenario 5: The Dude’s bowling games tend to last around 45 minutes, give or take. While the exact length can vary from game to game, it’s rare for his games to be extremely short or unusually long. Most of the time, his game lengths cluster fairly tightly around that 45-minute mark, though occasionally he’ll have a quicker match or one that drags on a little longer if the vibe calls for it, to the tune of \\pm 5 minutes.\na. What is the random variable of interest? (What is being measured?)\nReplace with your answer\nb. What distribution is appropriate and why?\nReplace with your answer\nc. What are the parameters of the distribution?\nReplace with your answer\nd. What is the probability that the game will last exactly 45 minutes?\nReplace with your answer (i.e., don’t just show me R output here :))\ne. What is the probability that the game will really drag on – that it lasts somewhere between 50 and 60 minutes?\nReplace with your answer (i.e., don’t just show me R output here :))\nScenario 6: After many laid-back bowling sessions (and several White Russians), The Dude has started keeping track of how often he lands a strike. He knows he’s not perfect, but he’s developed a pretty good feel for his overall strike success rate. It seems like his strike percentage tends to hang out somewhere between 60% and 80% most nights, though there’s still some uncertainty. Sometimes he’s on fire, sometimes he’s a little off, but overall he’s confident his long-term strike rate leans more toward success than failure.\na. What is the random variable of interest? (What is being measured?)\nReplace with your answer\nb. What distribution is appropriate and why?\nReplace with your answer\nc. What are the parameters of the distribution? (Hint: for this example, they are 7 and 3 (in that order)).\nReplace with your answer\nd. What is the probability that The Dude’s strike success rate is less than 50%?\nReplace with your answer (i.e., don’t just show me R output here :))\ne. What is the probability that The Dude’s strike success rate is above 80%?\nReplace with your answer (i.e., don’t just show me R output here :))"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "STA6349 - Applied Bayesian Analysis - Fall 2025",
    "section": "",
    "text": "Welcome to Applied Bayesian Analysis for Fall 2025!\nThe initial development of this course was supported by the 2023 BATS program, funded by NSF IUSE: EHR program with award numbers 2215879, 2215920, and 2215709."
  },
  {
    "objectID": "files/lectures/05-1-beta-binomial.html#introduction-beta-binomial-model",
    "href": "files/lectures/05-1-beta-binomial.html#introduction-beta-binomial-model",
    "title": "Beta-Binomial Model",
    "section": "Introduction: Beta-Binomial Model",
    "text": "Introduction: Beta-Binomial Model\n\nLast week, we learned how to think like a Bayesian.\n\nToday, we will formalize the model we muddled through last time.\n\nThis is called the Beta-Binomial model.\n\nThe Beta distribution is the prior.\nThe Binomial distribution is the data distribution (or the likeihood).\nThe posterior also follows a Beta distribution.\n\nConjugate family: When the prior and posterior are the same named distribution, but different parameters."
  },
  {
    "objectID": "files/lectures/05-1-beta-binomial.html#example-set-up",
    "href": "files/lectures/05-1-beta-binomial.html#example-set-up",
    "title": "Beta-Binomial Model",
    "section": "Example Set Up",
    "text": "Example Set Up\n\nConsider the following scenario.\n\n“Michelle” has decided to run for president and you’re her campaign manager for the state of Florida.\nAs such, you’ve conducted 30 different polls throughout the election season.\nThough Michelle’s support has hovered around 45%, she polled at around 35% in the dreariest days and around 55% in the best days on the campaign trail."
  },
  {
    "objectID": "files/lectures/05-1-beta-binomial.html#example-set-up-1",
    "href": "files/lectures/05-1-beta-binomial.html#example-set-up-1",
    "title": "Beta-Binomial Model",
    "section": "Example Set Up",
    "text": "Example Set Up\n\nPast polls provide prior information about \\pi, the proportion of Floridians that currently support Michelle.\n\nIn fact, we can reorganize this information into a formal prior probability model of \\pi.\n\nIn a previous problem, we assumed that \\pi could only be 0.2, 0.5, or 0.8, the corresponding chances of which were defined by a discrete probability model.\n\nHowever, in the reality of Michelle’s election support, \\pi \\in [0, 1].\n\nWe can reflect this reality and conduct a Bayesian analysis by constructing a continuous prior probability model of \\pi."
  },
  {
    "objectID": "files/lectures/05-1-beta-binomial.html#example-set-up-2",
    "href": "files/lectures/05-1-beta-binomial.html#example-set-up-2",
    "title": "Beta-Binomial Model",
    "section": "Example Set Up",
    "text": "Example Set Up\n\n\n\n\nA reasonable prior is represented by the curve on the right.\n\nNotice that this curve preserves the overall information and variability in the past polls, i.e., Michelle’s support, \\pi can be anywhere between 0 and 1, but is most likely around 0.45."
  },
  {
    "objectID": "files/lectures/05-1-beta-binomial.html#example-set-up-3",
    "href": "files/lectures/05-1-beta-binomial.html#example-set-up-3",
    "title": "Beta-Binomial Model",
    "section": "Example Set Up",
    "text": "Example Set Up\n\nIncorporating this more nuanced, continuous view of Michelle’s support, \\pi, will require some new tools.\n\nNo matter if our parameter \\pi is continuous or discrete, the posterior model of \\pi will combine insights from the prior and data.\n\\pi isn’t the only variable of interest that lives on [0,1].\n\nMaybe we’re interested in modeling the proportion of people that use public transit, the proportion of trains that are delayed, the proportion of people that prefer cats to dogs, etc.\n\nThe Beta-Binomial model provides the tools we need to study the proportion of interest, \\pi, in each of these settings."
  },
  {
    "objectID": "files/lectures/05-1-beta-binomial.html#beta-prior",
    "href": "files/lectures/05-1-beta-binomial.html#beta-prior",
    "title": "Beta-Binomial Model",
    "section": "Beta Prior",
    "text": "Beta Prior\n\n\n\n\nIn building the Bayesian election model of Michelle’s election support among Floridians, \\pi, we begin with the prior.\n\nOur continuous prior probability model of \\pi is specified by the probability density function (pdf).\n\nWhat values can \\pi take and which are more plausible than others?"
  },
  {
    "objectID": "files/lectures/05-1-beta-binomial.html#beta-prior-1",
    "href": "files/lectures/05-1-beta-binomial.html#beta-prior-1",
    "title": "Beta-Binomial Model",
    "section": "Beta Prior",
    "text": "Beta Prior\n\nLet \\pi be a random variable, where \\pi \\in [0, 1].\nThe variability in \\pi may be captured by a Beta model with shape hyperparameters \\alpha &gt; 0 and \\beta &gt; 0,\n\nhyperparameter: a parameter used in a prior model.\n\n\n\\pi \\sim \\text{Beta}(\\alpha, \\beta),"
  },
  {
    "objectID": "files/lectures/05-1-beta-binomial.html#beta-prior-shapes",
    "href": "files/lectures/05-1-beta-binomial.html#beta-prior-shapes",
    "title": "Beta-Binomial Model",
    "section": "Beta Prior: Shapes",
    "text": "Beta Prior: Shapes\n\nLet’s explore the shape of the Beta:\n\n\n\nplot_beta(1, 5) + theme_bw() + ggtitle(\"Beta(1, 5)\")"
  },
  {
    "objectID": "files/lectures/05-1-beta-binomial.html#beta-prior-shapes-1",
    "href": "files/lectures/05-1-beta-binomial.html#beta-prior-shapes-1",
    "title": "Beta-Binomial Model",
    "section": "Beta Prior: Shapes",
    "text": "Beta Prior: Shapes\n\nLet’s explore the shape of the Beta:\n\n\n\nplot_beta(1, 2) + theme_bw() + ggtitle(\"Beta(1, 2)\")"
  },
  {
    "objectID": "files/lectures/05-1-beta-binomial.html#beta-prior-shapes-2",
    "href": "files/lectures/05-1-beta-binomial.html#beta-prior-shapes-2",
    "title": "Beta-Binomial Model",
    "section": "Beta Prior: Shapes",
    "text": "Beta Prior: Shapes\n\nLet’s explore the shape of the Beta:\n\n\n\nplot_beta(3, 7) + theme_bw() + ggtitle(\"Beta(3, 7)\")"
  },
  {
    "objectID": "files/lectures/05-1-beta-binomial.html#beta-prior-shapes-3",
    "href": "files/lectures/05-1-beta-binomial.html#beta-prior-shapes-3",
    "title": "Beta-Binomial Model",
    "section": "Beta Prior: Shapes",
    "text": "Beta Prior: Shapes\n\nLet’s explore the shape of the Beta:\n\n\n\nplot_beta(1, 1) + theme_bw() + ggtitle(\"Beta(1, 1)\")"
  },
  {
    "objectID": "files/lectures/05-1-beta-binomial.html#beta-prior-shapes-4",
    "href": "files/lectures/05-1-beta-binomial.html#beta-prior-shapes-4",
    "title": "Beta-Binomial Model",
    "section": "Beta Prior: Shapes",
    "text": "Beta Prior: Shapes\n\nYour turn!\nHow would you describe the typical behavior of a:\n\nBeta(\\alpha, \\beta) variable, \\pi, when \\alpha=\\beta?\nBeta(\\alpha, \\beta) variable, \\pi, when \\alpha&gt;\\beta?\nBeta(\\alpha, \\beta) variable, \\pi, when \\alpha&lt;\\beta?\n\nFor which model is there greater variability in the plausible values of \\pi, Beta(20, 20) or Beta(5, 5)?"
  },
  {
    "objectID": "files/lectures/05-1-beta-binomial.html#beta-prior-shapes-5",
    "href": "files/lectures/05-1-beta-binomial.html#beta-prior-shapes-5",
    "title": "Beta-Binomial Model",
    "section": "Beta Prior: Shapes",
    "text": "Beta Prior: Shapes\n\nHow would you describe the typical behavior of a Beta(\\alpha, \\beta) variable, \\pi, when \\alpha=\\beta?"
  },
  {
    "objectID": "files/lectures/05-1-beta-binomial.html#beta-prior-shapes-6",
    "href": "files/lectures/05-1-beta-binomial.html#beta-prior-shapes-6",
    "title": "Beta-Binomial Model",
    "section": "Beta Prior: Shapes",
    "text": "Beta Prior: Shapes\n\nHow would you describe the typical behavior of a Beta(\\alpha, \\beta) variable, \\pi, when \\alpha&gt;\\beta?"
  },
  {
    "objectID": "files/lectures/05-1-beta-binomial.html#beta-prior-shapes-7",
    "href": "files/lectures/05-1-beta-binomial.html#beta-prior-shapes-7",
    "title": "Beta-Binomial Model",
    "section": "Beta Prior: Shapes",
    "text": "Beta Prior: Shapes\n\nHow would you describe the typical behavior of a Beta(\\alpha, \\beta) variable, \\pi, when \\alpha&lt;\\beta?"
  },
  {
    "objectID": "files/lectures/05-1-beta-binomial.html#beta-prior-shapes-8",
    "href": "files/lectures/05-1-beta-binomial.html#beta-prior-shapes-8",
    "title": "Beta-Binomial Model",
    "section": "Beta Prior: Shapes",
    "text": "Beta Prior: Shapes\n\nFor which model is there greater variability in the plausible values of \\pi, Beta(20, 20) or Beta(5, 5)?"
  },
  {
    "objectID": "files/lectures/05-1-beta-binomial.html#tuning-the-beta-prior",
    "href": "files/lectures/05-1-beta-binomial.html#tuning-the-beta-prior",
    "title": "Beta-Binomial Model",
    "section": "Tuning the Beta Prior",
    "text": "Tuning the Beta Prior\n\nWe can tune the shape hyperparameters (\\alpha and \\beta) to reflect our prior information about Michelle’s election support, \\pi.\nIn our example, we saw that she polled between 25 and 65 percentage points, with an average of 45 percentage points.\n\nWe want our Beta(\\alpha, \\beta) to have similar patterns, so we should pick \\alpha and \\beta such that \\pi is around 0.45.\n\n\n\nE[\\pi] = \\frac{\\alpha}{\\alpha+\\beta} \\approx 0.45\n\n\nUsing algebra, we can tune, and find\n\n\\alpha \\approx \\frac{9}{11} \\beta"
  },
  {
    "objectID": "files/lectures/05-1-beta-binomial.html#tuning-the-beta-prior-1",
    "href": "files/lectures/05-1-beta-binomial.html#tuning-the-beta-prior-1",
    "title": "Beta-Binomial Model",
    "section": "Tuning the Beta Prior",
    "text": "Tuning the Beta Prior\n\nYour turn!\n\nGraph the following and determine which is best for the example.\n\n\n\nplot_beta(9, 11) + theme_bw()\nplot_beta(27, 33) + theme_bw()\nplot_beta(45, 55) + theme_bw()\n\n\nRecall, this is what we are going for:"
  },
  {
    "objectID": "files/lectures/05-1-beta-binomial.html#tuning-the-beta-prior-2",
    "href": "files/lectures/05-1-beta-binomial.html#tuning-the-beta-prior-2",
    "title": "Beta-Binomial Model",
    "section": "Tuning the Beta Prior",
    "text": "Tuning the Beta Prior\n\n\nplot_beta(9, 11) + theme_bw() + ggtitle(\"Beta(9, 11)\")"
  },
  {
    "objectID": "files/lectures/05-1-beta-binomial.html#tuning-the-beta-prior-3",
    "href": "files/lectures/05-1-beta-binomial.html#tuning-the-beta-prior-3",
    "title": "Beta-Binomial Model",
    "section": "Tuning the Beta Prior",
    "text": "Tuning the Beta Prior\n\n\nplot_beta(27, 33) + theme_bw() + ggtitle(\"Beta(27, 33)\")"
  },
  {
    "objectID": "files/lectures/05-1-beta-binomial.html#tuning-the-beta-prior-4",
    "href": "files/lectures/05-1-beta-binomial.html#tuning-the-beta-prior-4",
    "title": "Beta-Binomial Model",
    "section": "Tuning the Beta Prior",
    "text": "Tuning the Beta Prior\n\n\nplot_beta(45, 55) + theme_bw() + ggtitle(\"Beta(45, 55)\")"
  },
  {
    "objectID": "files/lectures/05-1-beta-binomial.html#tuning-the-beta-prior-5",
    "href": "files/lectures/05-1-beta-binomial.html#tuning-the-beta-prior-5",
    "title": "Beta-Binomial Model",
    "section": "Tuning the Beta Prior",
    "text": "Tuning the Beta Prior\n\nNow that we have a prior, we “know” some things.\n\n\\pi \\sim \\text{Beta}(45, 55)\n\nFrom the properties of the beta distribution,\n\n\n\\begin{equation*}\n\\begin{aligned}\nE[\\pi] &= \\frac{\\alpha}{\\alpha + \\beta} & \\text{ and } & \\text{ } & \\text{ }  \\\\\n&=\\frac{45}{45+55} \\\\\n&= 0.45\n\\end{aligned}\n\\begin{aligned}\n\\text{var}[\\pi] &= \\frac{\\alpha\\beta}{(\\alpha+\\beta)^2(\\alpha+\\beta+1)} \\\\\n&= \\frac{(45)(55)}{(45+55)^2(45+55+1)} \\\\\n&= 0.0025\n\\end{aligned}\n\\end{equation*}"
  },
  {
    "objectID": "files/lectures/05-1-beta-binomial.html#binomial-data-model",
    "href": "files/lectures/05-1-beta-binomial.html#binomial-data-model",
    "title": "Beta-Binomial Model",
    "section": "Binomial Data Model",
    "text": "Binomial Data Model\n\nA new poll of n = 50 Floridians recorded Y, the number that support Michelle.\n\nThe results depend upon \\pi (as \\pi increases, Y tends to increase).\n\nTo model the dependence of Y on \\pi, we assume\n\nvoters answer the poll independently of one another;\nthe probability that any polled voter supports your candidate Michelle is \\pi\n\nThis is a binomial event, Y|\\pi \\sim \\text{Bin}(50, \\pi), with conditional pmf, f(y|\\pi) defined for y \\in \\{0, 1, ..., 50\\}\n\nf(y|\\pi) = P[Y = y|\\pi] = {50 \\choose y} \\pi^y (1-\\pi)^{50-y}"
  },
  {
    "objectID": "files/lectures/05-1-beta-binomial.html#binomial-data-model-1",
    "href": "files/lectures/05-1-beta-binomial.html#binomial-data-model-1",
    "title": "Beta-Binomial Model",
    "section": "Binomial Data Model",
    "text": "Binomial Data Model\n\nThe conditional pmf, f(y|\\pi), gives us answers to a hypothetical question:\n\nIf Michelle’s support were given some value of \\pi, then how many of the 50 polled voters (Y=y) might we expect to suppport her?\n\nLet’s look at this graphically:\n\n\nbinom_prob &lt;- tibble(n_success = 1:sample_size,\n                     prob = dbinom(n_success, size=sample_size, prob=pi_value))\n\nbinom_prob %&gt;%\n  ggplot(aes(x=n_success,y=prob))+\n  geom_col(width=0.2)+\n  labs(x= \"Number of Successes\",\n       y= \"Probability\") +\n  theme_bw()"
  },
  {
    "objectID": "files/lectures/05-1-beta-binomial.html#binomial-data-model-2",
    "href": "files/lectures/05-1-beta-binomial.html#binomial-data-model-2",
    "title": "Beta-Binomial Model",
    "section": "Binomial Data Model",
    "text": "Binomial Data Model"
  },
  {
    "objectID": "files/lectures/05-1-beta-binomial.html#binomial-data-model-3",
    "href": "files/lectures/05-1-beta-binomial.html#binomial-data-model-3",
    "title": "Beta-Binomial Model",
    "section": "Binomial Data Model",
    "text": "Binomial Data Model\n\nIt is observed that Y=30 of the n=50 polled voters support Michelle.\nWe now want to find the likelihood function – remember that we treat Y=30 as the observed data and \\pi as unknown,\n\n\n\\begin{align*}\nf(y|\\pi) &= {50 \\choose y} \\pi^y (1-\\pi)^{50-y} \\\\\nL(\\pi|y=30) &= {50 \\choose 30} \\pi^{30} (1-\\pi)^{20}\n\\end{align*}\n\n\nThis is valid for \\pi \\in [0, 1]."
  },
  {
    "objectID": "files/lectures/05-1-beta-binomial.html#binomial-data-model-4",
    "href": "files/lectures/05-1-beta-binomial.html#binomial-data-model-4",
    "title": "Beta-Binomial Model",
    "section": "Binomial Data Model",
    "text": "Binomial Data Model\n\nWhat is the likelihood of 30/50 voters supporting Michelle?\n\n\ndbinom(30, 50, pi)\n\n\nYou try this for \\pi = \\{0.25, 0.50, 0.75\\}.\n\n\ndbinom(30, 50, 0.25)\ndbinom(30, 50, 0.5)\ndbinom(30, 50, 0.75)"
  },
  {
    "objectID": "files/lectures/05-1-beta-binomial.html#binomial-data-model-5",
    "href": "files/lectures/05-1-beta-binomial.html#binomial-data-model-5",
    "title": "Beta-Binomial Model",
    "section": "Binomial Data Model",
    "text": "Binomial Data Model\n\nWhat is the likelihood of 30/50 voters supporting Michelle?\n\n\ndbinom(30, 50, 0.25)\n\n[1] 1.29633e-07\n\ndbinom(30, 50, 0.5)\n\n[1] 0.04185915\n\ndbinom(30, 50, 0.75)\n\n[1] 0.007654701"
  },
  {
    "objectID": "files/lectures/05-1-beta-binomial.html#binomial-data-model-6",
    "href": "files/lectures/05-1-beta-binomial.html#binomial-data-model-6",
    "title": "Beta-Binomial Model",
    "section": "Binomial Data Model",
    "text": "Binomial Data Model\n\nChallenge!\nCreate a graph showing what happens to the likelihood for different values of \\pi.\n\ni.e., have \\pi on the x-axis and likelihood on the y-axis.\n\nTo get you started,\n\n\ngraph &lt;- tibble(pi = seq(0, 1, 0.001)) %&gt;%\n  mutate(likelihood = dbinom(30, 50, pi))"
  },
  {
    "objectID": "files/lectures/05-1-beta-binomial.html#binomial-data-model-7",
    "href": "files/lectures/05-1-beta-binomial.html#binomial-data-model-7",
    "title": "Beta-Binomial Model",
    "section": "Binomial Data Model",
    "text": "Binomial Data Model\n\nCreate a graph showing what happens to the likelihood for different values of \\pi.\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhere is the maximum?"
  },
  {
    "objectID": "files/lectures/05-1-beta-binomial.html#binomial-data-model-8",
    "href": "files/lectures/05-1-beta-binomial.html#binomial-data-model-8",
    "title": "Beta-Binomial Model",
    "section": "Binomial Data Model",
    "text": "Binomial Data Model\n\nWhere is the maximum?"
  },
  {
    "objectID": "files/lectures/05-1-beta-binomial.html#the-beta-posterior-model",
    "href": "files/lectures/05-1-beta-binomial.html#the-beta-posterior-model",
    "title": "Beta-Binomial Model",
    "section": "The Beta Posterior Model",
    "text": "The Beta Posterior Model\n\nLooking at just the prior and the data distributions,\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe prior is a bit more pessimistic about Michelle’s election support than the data obtained from the latest poll."
  },
  {
    "objectID": "files/lectures/05-1-beta-binomial.html#the-beta-posterior-model-1",
    "href": "files/lectures/05-1-beta-binomial.html#the-beta-posterior-model-1",
    "title": "Beta-Binomial Model",
    "section": "The Beta Posterior Model",
    "text": "The Beta Posterior Model\n\nNow including the posterior,\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe can see that the posterior model of \\pi is continuous and \\in [0, 1].\nThe shape of the posterior appears to also have a Beta(\\alpha, \\beta) model.\n\nThe shape parameters (\\alpha and \\beta) have been updated."
  },
  {
    "objectID": "files/lectures/05-1-beta-binomial.html#the-beta-posterior-model-2",
    "href": "files/lectures/05-1-beta-binomial.html#the-beta-posterior-model-2",
    "title": "Beta-Binomial Model",
    "section": "The Beta Posterior Model",
    "text": "The Beta Posterior Model\n\nIf we were to collect more information about Michelle’s support, we would use the current posterior as the new prior, then update our posterior.\n\nHow do we know what the updated parameters are?\n\n\n\nsummarize_beta_binomial(alpha = 45, beta = 55, y = 30, n = 50)"
  },
  {
    "objectID": "files/lectures/05-1-beta-binomial.html#the-beta-posterior-model-3",
    "href": "files/lectures/05-1-beta-binomial.html#the-beta-posterior-model-3",
    "title": "Beta-Binomial Model",
    "section": "The Beta Posterior Model",
    "text": "The Beta Posterior Model\n\nWe used Michelle’s election support to understand the Beta-Binomial model.\nLet’s now generalize it for any appropriate situation.\n\n\n\\begin{align*}\nY|\\pi &\\sim \\text{Bin}(n, \\pi) \\\\\n\\pi &\\sim \\text{Beta}(\\alpha, \\beta) \\\\\n\\pi | (Y=y) &\\sim \\text{Beta}(\\alpha+y, \\beta+n-y)\n\\end{align*}\n\n\nWe can see that the posterior distribution reveals the influence of the prior (\\alpha and \\beta) and data (y and n)."
  },
  {
    "objectID": "files/lectures/05-1-beta-binomial.html#the-beta-posterior-model-4",
    "href": "files/lectures/05-1-beta-binomial.html#the-beta-posterior-model-4",
    "title": "Beta-Binomial Model",
    "section": "The Beta Posterior Model",
    "text": "The Beta Posterior Model\n\nUnder this updated distribution,\n\n\n\\pi | (Y=y) \\sim \\text{Beta}(\\alpha+y, \\beta+n-y)\n\n\nwe have updated moments:\n\n\n\\begin{align*}\nE[\\pi | Y = y] &= \\frac{\\alpha + y}{\\alpha + \\beta + n} \\\\\n\\text{Var}[\\pi|Y=y] &= \\frac{(\\alpha+y)(\\beta+n-y)}{(\\alpha+\\beta+n)^2(\\alpha+\\beta+1)}\n\\end{align*}"
  },
  {
    "objectID": "files/lectures/05-1-beta-binomial.html#the-beta-posterior-model-5",
    "href": "files/lectures/05-1-beta-binomial.html#the-beta-posterior-model-5",
    "title": "Beta-Binomial Model",
    "section": "The Beta Posterior Model",
    "text": "The Beta Posterior Model\n\nLet’s pause and think about this from a theoretical standpoint.\nThe Beta distribution is a conjugate prior for the likelihood.\n\nConjugate prior: the posterior is from the same model family as the prior.\n\nRecall the Beta prior, f(\\pi),\n\n L(\\pi|y) = {n \\choose y} \\pi^y (1-\\pi)^{n-y} \n\nand the likelihood function, L(\\pi|y).\n\n f(\\pi) = \\frac{\\Gamma(\\alpha+\\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)} \\pi^{\\alpha-1}(1-\\alpha)^{\\beta-1}"
  },
  {
    "objectID": "files/lectures/05-1-beta-binomial.html#the-beta-posterior-model-6",
    "href": "files/lectures/05-1-beta-binomial.html#the-beta-posterior-model-6",
    "title": "Beta-Binomial Model",
    "section": "The Beta Posterior Model",
    "text": "The Beta Posterior Model\n\nWe can put the prior and likelihood together to create the posterior,\n\n\n\\begin{align*}\nf(\\pi|y) &\\propto f(\\pi)L(\\pi|y) \\\\\n&= \\frac{\\Gamma(\\alpha+\\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)} \\pi^{\\alpha-1}(1-\\pi)^{\\beta-1} \\times {n \\choose y} \\pi^y (1-\\pi)^{n-1} \\\\\n&\\propto \\pi^{(\\alpha+y)-1} (1-\\pi)^{(\\beta+n-y)-1}\n\\end{align*}\n\n\nThis is the same structure as the normalized Beta(\\alpha+y, \\beta+n-y),\n\nf(\\pi|y) = \\frac{\\Gamma(\\alpha+\\beta+n)}{\\Gamma(\\alpha+y) \\Gamma(\\beta+n-y)} \\pi^{(\\alpha+y)-1} (1-\\pi)^{(\\beta+n-y)-1}"
  },
  {
    "objectID": "files/lectures/05-1-beta-binomial.html#beta-binomial-example",
    "href": "files/lectures/05-1-beta-binomial.html#beta-binomial-example",
    "title": "Beta-Binomial Model",
    "section": "Beta-Binomial: Example",
    "text": "Beta-Binomial: Example\n\nIn Mario Kart 8 Deluxe, item boxes give different items depending on race position. To reduce the “position bias,” only item boxes opened while the racer was in mid-pack (positions 4–10) were recorded.\nYou want to estimate the probability that an item box yields a Red Shell. When playing the Special Cup, only 31 red shells were seen in 114 boxes opened by mid-pack racers.\nFind the posterior distribution under two priors:\n\nFlat/uninformative prior, Beta(1,1).\nBeta(\\alpha, \\beta) of your choosing."
  },
  {
    "objectID": "files/lectures/05-1-beta-binomial.html#wrap-up-beta-binomial-model",
    "href": "files/lectures/05-1-beta-binomial.html#wrap-up-beta-binomial-model",
    "title": "Beta-Binomial Model",
    "section": "Wrap Up: Beta-Binomial Model",
    "text": "Wrap Up: Beta-Binomial Model\n\nWe have built the Beta-Binomial model for \\pi, an unknown proportion.\n\n\n\\begin{equation*}\n\\begin{aligned}\nY|\\pi &\\sim \\text{Bin}(n,\\pi) \\\\\n\\pi &\\sim \\text{Beta}(\\alpha,\\beta) &\n\\end{aligned}\n\\Rightarrow\n\\begin{aligned}\n&& \\pi | (Y=y) &\\sim \\text{Beta}(\\alpha+y, \\beta+n-y) \\\\\n\\end{aligned}\n\\end{equation*}\n\n\nThe prior model, f(\\pi), is given by Beta(\\alpha,\\beta).\nThe data model, f(Y|\\pi), is given by Bin(n,\\pi).\nThe likelihood function, L(\\pi|y), is obtained by plugging y into the Binomial pmf.\nThe posterior model is a Beta distribution with updated parameters \\alpha+y and \\beta+n-y."
  },
  {
    "objectID": "files/lectures/05-1-beta-binomial.html#homework-practice",
    "href": "files/lectures/05-1-beta-binomial.html#homework-practice",
    "title": "Beta-Binomial Model",
    "section": "Homework / Practice",
    "text": "Homework / Practice\n\nFrom the Bayes Rules! textbook:\n\n3.3\n3.9\n3.10\n3.18"
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#introduction",
    "href": "files/lectures/03-2-probability-distributions.html#introduction",
    "title": "Random Variables and their Distributions",
    "section": "Introduction",
    "text": "Introduction\n\nToday we will review the statistical distributions needed for this course.\nDiscrete distributions:\n\nBinomial\nPoisson\n\nContinuous distributions:\n\nUniform\nNormal\nGamma\nBeta"
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#basic-definitions",
    "href": "files/lectures/03-2-probability-distributions.html#basic-definitions",
    "title": "Random Variables and their Distributions",
    "section": "Basic Definitions",
    "text": "Basic Definitions\n\nDiscrete random variable: a variable that can assume only a finite or countably infinite number of distinct values.\nProbability distribution of a random variable: collection of probabilities for each value of the random variable.\nNotation:\n\nUppercase letter (e.g., Y) denotes a random variable.\nLowercase letter (e.g., y) denotes a particular value that the random variable may assume.\n\nThe specific observed value, y, is not random."
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#probability-distributions-for-discrete-rv",
    "href": "files/lectures/03-2-probability-distributions.html#probability-distributions-for-discrete-rv",
    "title": "Random Variables and their Distributions",
    "section": "Probability Distributions for Discrete RV",
    "text": "Probability Distributions for Discrete RV\n\nProbability function for \\boldsymbol Y: sum of the the probabilities of all sample points in S that are assigned the value y\n\nP[Y = y] = p(y): the probability that Y takes on the value y.\n\nProbability distribution for \\boldsymbol Y: a formula, table, or graph that provides p(y) \\ \\forall \\ y.\nTheorem:\n\nFor any discrete probability distribution, the following must be true:\n\n0 \\le p(y) \\le 1 \\  \\forall \\ y\n\\sum_y p(y) = 1 \\ \\forall \\ p(y) &gt; 0."
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#expected-values-for-discrete-rv",
    "href": "files/lectures/03-2-probability-distributions.html#expected-values-for-discrete-rv",
    "title": "Random Variables and their Distributions",
    "section": "Expected Values for Discrete RV",
    "text": "Expected Values for Discrete RV\n\nExpected value: Let Y be a discrete random variable with probability function p(y). Then the expected value of Y, E[Y], is defined to be\n\n\nE(Y) = \\sum_{y} y p(y)\n\n\nWhen p(y) is an accurate characterization of the population frequency distribution, then the expected value is the population mean.\n\n\nE[Y] = \\mu"
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#expected-values-for-discrete-rv-1",
    "href": "files/lectures/03-2-probability-distributions.html#expected-values-for-discrete-rv-1",
    "title": "Random Variables and their Distributions",
    "section": "Expected Values for Discrete RV",
    "text": "Expected Values for Discrete RV\n\nTheorem:\n\nLet Y be a discrete random variable with probability function p(y) and g(Y) be a real-valued function of Y (i.e., a transformed variable). Then the expected value of g(Y) is given by\n\n\n\nE[g(Y)] = \\sum_{y} g(y) p(y)"
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#expected-values-for-discrete-rv-2",
    "href": "files/lectures/03-2-probability-distributions.html#expected-values-for-discrete-rv-2",
    "title": "Random Variables and their Distributions",
    "section": "Expected Values for Discrete RV",
    "text": "Expected Values for Discrete RV\n\nVariance: if Y is a random variable with mean E[Y] = \\mu, the variance of a random variable Y is defined to be the expected value of (Y-\\mu)^2.\n\n\nV[Y] = E\\left[ (Y-\\mu)^2 \\right]\n\n\nIf p(y) is an accurate characterization of the population frequency distribution, then V(Y) is the population variance,\n\n\nV[Y] = \\sigma^2\n\n\nStandard deviation: the positive square root of V[Y]."
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#expected-values-for-discrete-rv-3",
    "href": "files/lectures/03-2-probability-distributions.html#expected-values-for-discrete-rv-3",
    "title": "Random Variables and their Distributions",
    "section": "Expected Values for Discrete RV",
    "text": "Expected Values for Discrete RV\n\nThere is an alternative (and easier) way to calculate the variance manually,\nTheorem:\n\nLet Y be a discrete random variable with probability function p(y) and mean E[Y] = \\mu. Then,\n\n\nV[Y] = \\sigma^2 = E\\left[(Y-\\mu)^2\\right] = E\\left[Y^2\\right] - \\mu^2"
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#expected-values-for-discrete-rv-4",
    "href": "files/lectures/03-2-probability-distributions.html#expected-values-for-discrete-rv-4",
    "title": "Random Variables and their Distributions",
    "section": "Expected Values for Discrete RV",
    "text": "Expected Values for Discrete RV\n\nThe probability distribution for a random variable Y is given below.\n\n\n\n\ny\np(y)\n\n\n\n\n0\n1/8\n\n\n1\n1/4\n\n\n2\n3/8\n\n\n3\n1/4\n\n\n\n\nFind the mean of Y."
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#expected-values-for-discrete-rv-5",
    "href": "files/lectures/03-2-probability-distributions.html#expected-values-for-discrete-rv-5",
    "title": "Random Variables and their Distributions",
    "section": "Expected Values for Discrete RV",
    "text": "Expected Values for Discrete RV\n\nThe probability distribution for a random variable Y is given below.\n\n\n\n\ny\np(y)\n\n\n\n\n0\n1/8\n\n\n1\n1/4\n\n\n2\n3/8\n\n\n3\n1/4\n\n\n\n\nFind the variance and standard deviation of Y."
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#binomial-probability-distribution",
    "href": "files/lectures/03-2-probability-distributions.html#binomial-probability-distribution",
    "title": "Random Variables and their Distributions",
    "section": "Binomial Probability Distribution",
    "text": "Binomial Probability Distribution\n\nBinomial experiment:\n\nThe experiment consists of a fixed number, n, of identical trials.\nEach trial results in one of two outcomes: success (S) or failure (F).\nThe probability of success on a single trial is equal to some value p and remains the same from trial to trial.\n\nThe probability of failure is equal to q = (1-p).\n\nThe trials are independent.\nThe random variable of interest is Y, the number of successes observed during the n trials."
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#binomial-probability-distribution-1",
    "href": "files/lectures/03-2-probability-distributions.html#binomial-probability-distribution-1",
    "title": "Random Variables and their Distributions",
    "section": "Binomial Probability Distribution",
    "text": "Binomial Probability Distribution\n\nA random variable Y is said to have a binomial distribution based on n trials with success probability p iff\n\n\np(y) = {n \\choose y}p^y q^{n-y}, \\text{ where } y = 0, 1, 2, ..., n, \\text{ and } 0 \\le p \\le1\n\n\nLet Y be a binomial random variable based on n trials and success probability p. Then\n\n\nE[Y] = \\mu = np \\ \\ \\ \\text{and} \\ \\ \\ V[Y] = \\sigma^2 = npq\n\n\nSee Wackerly pg. 107 for derivation."
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#binomial-probability-distribution-2",
    "href": "files/lectures/03-2-probability-distributions.html#binomial-probability-distribution-2",
    "title": "Random Variables and their Distributions",
    "section": "Binomial Probability Distribution",
    "text": "Binomial Probability Distribution\n\nWe can use R to find information related to the binomial distribution.\n\nP[X = x]: dbinom(x, size, prob)\nP[X \\le x]: pbinom(q, size, prob)\nP[X &gt; x]: pbinom(q, size, prob, lower.tail = FALSE)\n\nIn the functions,\n\nx or q is the value of X we are interested in\nsize is the sample size (n)\nprob is the probability of success, \\pi\nlower.tail has two options:\n\nTRUE (default) returns P[X \\le x]\nFALSE returns P[X &gt; x]"
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#binomial-probability-distribution-3",
    "href": "files/lectures/03-2-probability-distributions.html#binomial-probability-distribution-3",
    "title": "Random Variables and their Distributions",
    "section": "Binomial Probability Distribution",
    "text": "Binomial Probability Distribution\n\nThe manufacturer of a dairy drink wishes to compare a new formula (B) with that of the standard formula (A). Each of four judges perform a blinded taste test and report which glass he or she most enjoyed. Suppose that the two formulas are equally attractive.\n\nWhat is the probability distribution? \nWhat is the mean of the distribution? \nWhat is the variance of the distribution?"
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#binomial-probability-distribution-4",
    "href": "files/lectures/03-2-probability-distributions.html#binomial-probability-distribution-4",
    "title": "Random Variables and their Distributions",
    "section": "Binomial Probability Distribution",
    "text": "Binomial Probability Distribution\n\nThe manufacturer of a dairy drink wishes to compare a new formula (B) with that of the standard formula (A). Each of four judges perform a blinded taste test and report which glass he or she most enjoyed. Suppose that the two formulas are equally attractive.\nUse R to find:\n\nP[X = 2]\nP[X &gt; 2]\nP[X &lt; 4]"
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#binomial-probability-distribution-5",
    "href": "files/lectures/03-2-probability-distributions.html#binomial-probability-distribution-5",
    "title": "Random Variables and their Distributions",
    "section": "Binomial Probability Distribution",
    "text": "Binomial Probability Distribution\n\nThe manufacturer of a dairy drink wishes to compare a new formula (B) with that of the standard formula (A). Each of four judges perform a blinded taste test and report which glass he or she most enjoyed. Suppose that the two formulas are equally attractive.\nUse R to find:\n\nP[X = 2]\n\n\n\ndbinom(x = 2, size = 4, prob = 0.5)\n\n[1] 0.375"
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#binomial-probability-distribution-6",
    "href": "files/lectures/03-2-probability-distributions.html#binomial-probability-distribution-6",
    "title": "Random Variables and their Distributions",
    "section": "Binomial Probability Distribution",
    "text": "Binomial Probability Distribution\n\nThe manufacturer of a dairy drink wishes to compare a new formula (B) with that of the standard formula (A). Each of four judges perform a blinded taste test and report which glass he or she most enjoyed. Suppose that the two formulas are equally attractive.\nUse R to find:\n\nP[X &gt; 2]\n\n\n\npbinom(q = 2, size = 4, prob = 0.5, lower.tail = FALSE)\n\n[1] 0.3125"
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#binomial-probability-distribution-7",
    "href": "files/lectures/03-2-probability-distributions.html#binomial-probability-distribution-7",
    "title": "Random Variables and their Distributions",
    "section": "Binomial Probability Distribution",
    "text": "Binomial Probability Distribution\n\nThe manufacturer of a dairy drink wishes to compare a new formula (B) with that of the standard formula (A). Each of four judges perform a blinded taste test and report which glass he or she most enjoyed. Suppose that the two formulas are equally attractive.\nUse R to find:\n\nP[X &lt; 4] = P[X \\le 3]\n\n\n\npbinom(q = 3, size = 4, prob = 0.5)\n\n[1] 0.9375"
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#poisson-probability-distribution",
    "href": "files/lectures/03-2-probability-distributions.html#poisson-probability-distribution",
    "title": "Random Variables and their Distributions",
    "section": "Poisson Probability Distribution",
    "text": "Poisson Probability Distribution\n\nWe often use the Poisson distribution to model count data.\nA random variable Y is said to have a Poisson probability distribution iff\n\n\np(y) = \\frac{\\lambda^y}{y!}e^{-\\lambda}, \\text{ where } y=0,1,2,..., \\text{ and } \\lambda &gt; 0\n\n\nIf Y is a random variable with a Poisson distribution with parameter \\lambda, then\n\n\nE[Y] = \\mu = \\lambda \\text{ and } V[Y] = \\sigma^2 = \\lambda\n\n\nSee Wackerly pg. 134 for derivation."
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#poisson-probability-distribution-1",
    "href": "files/lectures/03-2-probability-distributions.html#poisson-probability-distribution-1",
    "title": "Random Variables and their Distributions",
    "section": "Poisson Probability Distribution",
    "text": "Poisson Probability Distribution\n\nWe can use R to find information related to the Poisson distribution.\n\nP[X = x]: dpois(x, lambda)\n\nP[X \\le x]: ppois(q, lambda)\n\nP[X &gt; x]: ppois(q, lambda, lower.tail = FALSE)\n\nIn the functions:\n\nx or q is the value of X we are interested in\n\nlambda is the rate of occurrence\nlower.tail has two options:\n\nTRUE (default) returns P[X \\le x]\n\nFALSE returns P[X &gt; x]"
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#poisson-probability-distribution-2",
    "href": "files/lectures/03-2-probability-distributions.html#poisson-probability-distribution-2",
    "title": "Random Variables and their Distributions",
    "section": "Poisson Probability Distribution",
    "text": "Poisson Probability Distribution\n\nCustomers arrive at a checkout counter in a department store according to a Poisson distribution at an average of seven per hour.\n\nWhat is the probability distribution? \nWhat is the mean of the distribution? \nWhat is the variance of the distribution?"
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#poisson-probability-distribution-3",
    "href": "files/lectures/03-2-probability-distributions.html#poisson-probability-distribution-3",
    "title": "Random Variables and their Distributions",
    "section": "Poisson Probability Distribution",
    "text": "Poisson Probability Distribution\n\nCustomers arrive at a checkout counter in a department store according to a Poisson distribution at an average of seven per hour. Use R to find the following probabilities.\n\nNo more than three customers arrive.\nAt least two customers arrive.\nExactly five customers arrive."
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#poisson-probability-distribution-4",
    "href": "files/lectures/03-2-probability-distributions.html#poisson-probability-distribution-4",
    "title": "Random Variables and their Distributions",
    "section": "Poisson Probability Distribution",
    "text": "Poisson Probability Distribution\n\nCustomers arrive at a checkout counter in a department store according to a Poisson distribution at an average of seven per hour. Use R to find the following probabilities.\n\nNo more than three customers arrive.\n\n\n\nppois(q = 3, lambda = 7)\n\n[1] 0.08176542"
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#poisson-probability-distribution-5",
    "href": "files/lectures/03-2-probability-distributions.html#poisson-probability-distribution-5",
    "title": "Random Variables and their Distributions",
    "section": "Poisson Probability Distribution",
    "text": "Poisson Probability Distribution\n\nCustomers arrive at a checkout counter in a department store according to a Poisson distribution at an average of seven per hour. Use R to find the following probabilities.\n\nAt least two customers arrive.\n\n\n\nppois(q = 1, lambda = 7, lower.tail = FALSE)\n\n[1] 0.9927049"
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#poisson-probability-distribution-6",
    "href": "files/lectures/03-2-probability-distributions.html#poisson-probability-distribution-6",
    "title": "Random Variables and their Distributions",
    "section": "Poisson Probability Distribution",
    "text": "Poisson Probability Distribution\n\nCustomers arrive at a checkout counter in a department store according to a Poisson distribution at an average of seven per hour. Use R to find the following probabilities.\n\nExactly five customers arrive.\n\n\n\ndpois(x = 5, lambda = 7)\n\n[1] 0.1277167"
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#probability-distributions-for-continuous-rv",
    "href": "files/lectures/03-2-probability-distributions.html#probability-distributions-for-continuous-rv",
    "title": "Random Variables and their Distributions",
    "section": "Probability Distributions for Continuous RV",
    "text": "Probability Distributions for Continuous RV\n\nThe distribution function of Y (any random varaible), denoted by F(y), is such that\n\nF(y) = P[Y \\le y] \\text{ for } -\\infty &lt; y &lt; \\infty\n\nTheorem: Properties of a Distribution Function\n\nIf F(y) is a distribution function, then\n\nF(-\\infty)   \\equiv \\underset{y \\to -\\infty}{\\lim} F(y) = 0\nF(\\infty)    \\equiv \\underset{y \\to \\infty}{\\lim} F(y) = 1\nF(y) is a nondecreasing function of y.\n\nIf y_1 and y_2 are any values such that y_1 &lt; y_2, then F(y_1) \\le F(y_2)."
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#probability-distributions-for-continuous-rv-1",
    "href": "files/lectures/03-2-probability-distributions.html#probability-distributions-for-continuous-rv-1",
    "title": "Random Variables and their Distributions",
    "section": "Probability Distributions for Continuous RV",
    "text": "Probability Distributions for Continuous RV\n\nContinuous random variable:\n\nA random variable Y with distribution function F(y) is said to be continuous if F(y) is continuous for -\\infty &lt; y &lt; \\infty.\n\nIf Y is a continuous random variable, then for any real number y, P[Y = y] = 0.\n\ni.e., we must remember to find the probability of an interval."
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#probability-distributions-for-continuous-rv-2",
    "href": "files/lectures/03-2-probability-distributions.html#probability-distributions-for-continuous-rv-2",
    "title": "Random Variables and their Distributions",
    "section": "Probability Distributions for Continuous RV",
    "text": "Probability Distributions for Continuous RV\n\nProbability density function:\n\nLet F(y) be the cumulative density function for a continuous random variable, Y. Then\n\n\np[Y = y] = f(y) = \\frac{dF(y)}{dy} = F'(y).\n\nTheorem: Properties of a Density Function\n\nIf f(y) is a density function for a continuous random variable, then\n\nf(y) \\ge 0 \\ \\forall y, \\ -\\infty &lt; y &lt; \\infty.\n\\int_{-\\infty}^{\\infty} f(y) dy = 1."
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#probability-distributions-for-continuous-rv-3",
    "href": "files/lectures/03-2-probability-distributions.html#probability-distributions-for-continuous-rv-3",
    "title": "Random Variables and their Distributions",
    "section": "Probability Distributions for Continuous RV",
    "text": "Probability Distributions for Continuous RV\n\nCumulative density function:\n\nLet f(y) be the probability density function for a continuous random variable, Y. Then\n\n\nP[Y \\le y] = F(y) = \\int_{-\\infty}^y f(t) dt, \\text{ for } y \\in {\\rm I\\!R}."
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#probability-distributions-for-continuous-rv-4",
    "href": "files/lectures/03-2-probability-distributions.html#probability-distributions-for-continuous-rv-4",
    "title": "Random Variables and their Distributions",
    "section": "Probability Distributions for Continuous RV",
    "text": "Probability Distributions for Continuous RV\n\nTheorem\n\nIf the random variable Y has density function f(y) and a &lt; b, then the probability that Y falls in the interval [a, b] is\n\n\n\nP[a \\le Y \\le b] = \\int_a^b f(y) dy."
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#expected-values-for-continuous-rv",
    "href": "files/lectures/03-2-probability-distributions.html#expected-values-for-continuous-rv",
    "title": "Random Variables and their Distributions",
    "section": "Expected Values for Continuous RV",
    "text": "Expected Values for Continuous RV\n\nExpected value:\n\nThe expected value of a continuous variable Y is\n\n\n\nE[Y] = \\int_{-\\infty}^{\\infty} y f(y) \\ dy\n\n\nThis is the continuous version of the expected value for a discrete random variable,\n\n\nE[Y] = \\sum_y y p(y)"
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#expected-values-for-continuous-rv-1",
    "href": "files/lectures/03-2-probability-distributions.html#expected-values-for-continuous-rv-1",
    "title": "Random Variables and their Distributions",
    "section": "Expected Values for Continuous RV",
    "text": "Expected Values for Continuous RV\n\nTheorem:\n\nLet g(Y) be a function of Y; then the expected value of g(Y) is given by\n\n\n\nE\\left[ g(Y) \\right] = \\int_{-\\infty}^{\\infty} g(y) f(y) \\ dy\n\n\nTheorem:\n\nLet c be a constant and let g(Y), g_1(Y), g_2(Y), …, g_k(Y) be functions of a continuous random variable, Y. Then the following results hold:\n\nE[c] = c\nE\\left[cg(Y)] = cE[g(Y)\\right]\nE\\left[g_1(Y)+...+g_k(Y)\\right] = E\\left[ g_1(Y) \\right] + ... + E\\left[ g_k(Y) \\right]"
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#uniform-probability-distribution",
    "href": "files/lectures/03-2-probability-distributions.html#uniform-probability-distribution",
    "title": "Random Variables and their Distributions",
    "section": "Uniform Probability Distribution",
    "text": "Uniform Probability Distribution\n\nUniform Distribution"
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#uniform-probability-distribution-1",
    "href": "files/lectures/03-2-probability-distributions.html#uniform-probability-distribution-1",
    "title": "Random Variables and their Distributions",
    "section": "Uniform Probability Distribution",
    "text": "Uniform Probability Distribution\n\nA random variable Y is said to have a uniform distribution iff\n\n\nf(y) = \\frac{1}{\\theta_2 - \\theta_1}, \\ \\theta_1 \\le y \\le \\theta_2\n\n\nIf \\theta_1 &lt; \\theta_2 and Y is a uniformly distributed r.v. on the interval (\\theta_1, \\theta_2), then\n\n\nE[Y] = \\mu = \\frac{\\theta_1+\\theta_2}{2} \\ \\ \\ \\text{and} \\ \\ \\ V[Y] = \\sigma^2 = \\frac{(\\theta_2-\\theta_1)^2}{12}\n\n\nSee Wackerly pg. 176 for derivation."
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#uniform-probability-distribution-2",
    "href": "files/lectures/03-2-probability-distributions.html#uniform-probability-distribution-2",
    "title": "Random Variables and their Distributions",
    "section": "Uniform Probability Distribution",
    "text": "Uniform Probability Distribution\n\nAn industrial psychologist has determined that it takes a worker between 9 and 15 minutes to complete a task on an automobile assembly line. If the time to complete the task is uniformly distributed over the interval 9 \\le y \\le 15, then determine:\n\nThe probability distribution. \nThe mean of the distribution. \nThe variance and standard deviation of the distribution."
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#uniform-probability-distribution-3",
    "href": "files/lectures/03-2-probability-distributions.html#uniform-probability-distribution-3",
    "title": "Random Variables and their Distributions",
    "section": "Uniform Probability Distribution",
    "text": "Uniform Probability Distribution\n\nWe can use R to find information related to the uniform distribution:\n\nP[X \\le x]: punif(q, min, max)\n\nP[X \\ge x]: punif(q, min, max, lower.tail = FALSE)\n\nIn the functions:\n\nq is the value of X we are interested in\n\nmin is the lower bound of the distribution\n\nmax is the upper bound of the distribution\n\nlower.tail has two options:\n\nTRUE (default) returns P[X \\le x]\n\nFALSE returns P[X \\ge x]"
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#uniform-probability-distribution-4",
    "href": "files/lectures/03-2-probability-distributions.html#uniform-probability-distribution-4",
    "title": "Random Variables and their Distributions",
    "section": "Uniform Probability Distribution",
    "text": "Uniform Probability Distribution\n\nAn industrial psychologist has determined that it takes a worker between 9 and 15 minutes to complete a task on an automobile assembly line. If the time to complete the task is uniformly distributed over the interval 9 \\le y \\le 15, then determine the following probabilities:\n\nA worker takes fewer than 13 minutes.\nA worker takes at least 11 minutes.\nA worker takes between 14 and 15 minutes."
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#uniform-probability-distribution-5",
    "href": "files/lectures/03-2-probability-distributions.html#uniform-probability-distribution-5",
    "title": "Random Variables and their Distributions",
    "section": "Uniform Probability Distribution",
    "text": "Uniform Probability Distribution\n\nAn industrial psychologist has determined that it takes a worker between 9 and 15 minutes to complete a task on an automobile assembly line. If the time to complete the task is uniformly distributed over the interval 9 \\le y \\le 15, then determine the following probabilities:\n\nA worker takes fewer than 13 minutes.\n\n\n\npunif(13, 9, 15)\n\n[1] 0.6666667"
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#uniform-probability-distribution-6",
    "href": "files/lectures/03-2-probability-distributions.html#uniform-probability-distribution-6",
    "title": "Random Variables and their Distributions",
    "section": "Uniform Probability Distribution",
    "text": "Uniform Probability Distribution\n\nAn industrial psychologist has determined that it takes a worker between 9 and 15 minutes to complete a task on an automobile assembly line. If the time to complete the task is uniformly distributed over the interval 9 \\le y \\le 15, then determine the following probabilities:\n\nA worker takes at least 11 minutes.\n\n\n\npunif(11, 9, 15, lower.tail = TRUE)\n\n[1] 0.3333333"
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#uniform-probability-distribution-7",
    "href": "files/lectures/03-2-probability-distributions.html#uniform-probability-distribution-7",
    "title": "Random Variables and their Distributions",
    "section": "Uniform Probability Distribution",
    "text": "Uniform Probability Distribution\n\nAn industrial psychologist has determined that it takes a worker between 9 and 15 minutes to complete a task on an automobile assembly line. If the time to complete the task is uniformly distributed over the interval 9 \\le y \\le 15, then determine the following probabilities:\n\nA worker takes between 14 and 15 minutes.\n\n\n\npunif(15, 9, 15) - punif(14, 9, 15)\n\n[1] 0.1666667"
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#normal-probability-distribution",
    "href": "files/lectures/03-2-probability-distributions.html#normal-probability-distribution",
    "title": "Random Variables and their Distributions",
    "section": "Normal Probability Distribution",
    "text": "Normal Probability Distribution\n\nNormal Distribution"
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#normal-probability-distribution-1",
    "href": "files/lectures/03-2-probability-distributions.html#normal-probability-distribution-1",
    "title": "Random Variables and their Distributions",
    "section": "Normal Probability Distribution",
    "text": "Normal Probability Distribution\n\nA random variable Y is said to have a normal distribution iff, for \\sigma &gt; 0 and -\\infty &lt; \\mu &lt; \\infty,\n\n\nf(y) = \\frac{1}{\\sigma \\sqrt{2\\pi}} e^{-(y-\\mu)^2/(2\\sigma^2)}\n\n\nIf Y is a random variable normally distributed with parameters \\mu and \\sigma, then\n\n\nE[Y] = \\mu =  \\ \\ \\ \\text{and} \\ \\ \\ V[Y] = \\sigma^2"
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#normal-probability-distribution-2",
    "href": "files/lectures/03-2-probability-distributions.html#normal-probability-distribution-2",
    "title": "Random Variables and their Distributions",
    "section": "Normal Probability Distribution",
    "text": "Normal Probability Distribution\n\nWe can use R to find information related to the normal distribution.\n\nP[X \\le x]: pnorm(q, mean, sd)\n\nP[X \\ge x]: pnorm(q, mean, sd, lower.tail = FALSE)\n\nIn the functions:\n\nq is the value of X we are interested in\n\nmean is the population mean \\mu\n\nsd is the standard deviation \\sigma\n\nlower.tail has two options:\n\nTRUE (default) returns P[X \\le x]\n\nFALSE returns P[X \\ge x]"
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#normal-probability-distribution-3",
    "href": "files/lectures/03-2-probability-distributions.html#normal-probability-distribution-3",
    "title": "Random Variables and their Distributions",
    "section": "Normal Probability Distribution",
    "text": "Normal Probability Distribution\n\nA random variable Y is said to have a standard normal distribution iff\n\n\nY \\sim N(\\mu=0,\\sigma=1)\n\n\nThe normal distribution is then simplified to\n\n\nf(y) = \\frac{1}{\\sqrt{2\\pi}} e^{-y^2/2}\n\n\nNote that in all cases of the normal distribution, we assume -\\infty &lt; y &lt; \\infty."
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#normal-probability-distribution-4",
    "href": "files/lectures/03-2-probability-distributions.html#normal-probability-distribution-4",
    "title": "Random Variables and their Distributions",
    "section": "Normal Probability Distribution",
    "text": "Normal Probability Distribution\n\nWhen using pnorm(), the default values for mean and sd are 1 and 0.\nThus, if we have the standard normal our R functions simplify to:\n\nP[Z \\le z]: pnorm(z)\n\nP[Z \\ge z]: pnorm(z, lower.tail = FALSE)\n\nIn the functions:\n\nq is the z-score value of interest\n\nlower.tail = TRUE returns P[Z \\le z]\n\nlower.tail = FALSE returns P[Z \\ge z]"
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#normal-probability-distribution-5",
    "href": "files/lectures/03-2-probability-distributions.html#normal-probability-distribution-5",
    "title": "Random Variables and their Distributions",
    "section": "Normal Probability Distribution",
    "text": "Normal Probability Distribution\n\nA geneticist working for a seed company develops a new carrot for growing in heavy clay soil. After measuring 5000 of these carrots, it can be said that the carrot length, Y, is normally distributed with \\mu = 11.5 cm and \\sigma = 1.15 cm. Determine\n\nThe probability distribution. \nThe mean of the distribution. \nThe variance and standard deviation of the distribution."
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#normal-probability-distribution-6",
    "href": "files/lectures/03-2-probability-distributions.html#normal-probability-distribution-6",
    "title": "Random Variables and their Distributions",
    "section": "Normal Probability Distribution",
    "text": "Normal Probability Distribution\n\nA geneticist working for a seed company develops a new carrot for growing in heavy clay soil. After measuring 5000 of these carrots, it can be said that the carrot length, Y, is normally distributed with \\mu = 11.5 cm and \\sigma = 1.15 cm.\n\nWhat is the probability that a carrot will be between 10 and 13 cm?\nWhat is the probability that a carrot will be less than 9 cm?\nWhat is the probability that a carrot will be 12 cm or larger?"
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#normal-probability-distribution-7",
    "href": "files/lectures/03-2-probability-distributions.html#normal-probability-distribution-7",
    "title": "Random Variables and their Distributions",
    "section": "Normal Probability Distribution",
    "text": "Normal Probability Distribution\n\nA geneticist working for a seed company develops a new carrot for growing in heavy clay soil. After measuring 5000 of these carrots, it can be said that the carrot length, Y, is normally distributed with \\mu = 11.5 cm and \\sigma = 1.15 cm.\n\nWhat is the probability that a carrot will be between 10 and 13 cm?\n\n\n\npnorm(q = 13, mean = 11.5, sd = 1.15) - pnorm(q = 10, mean = 11.5, sd = 1.15)\n\n[1] 0.807885"
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#normal-probability-distribution-8",
    "href": "files/lectures/03-2-probability-distributions.html#normal-probability-distribution-8",
    "title": "Random Variables and their Distributions",
    "section": "Normal Probability Distribution",
    "text": "Normal Probability Distribution\n\nA geneticist working for a seed company develops a new carrot for growing in heavy clay soil. After measuring 5000 of these carrots, it can be said that the carrot length, Y, is normally distributed with \\mu = 11.5 cm and \\sigma = 1.15 cm.\n\nWhat is the probability that a carrot will be less than 9 cm?\n\n\n\npnorm(q = 9, mean = 11.5, sd = 1.15)\n\n[1] 0.01485583"
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#normal-probability-distribution-9",
    "href": "files/lectures/03-2-probability-distributions.html#normal-probability-distribution-9",
    "title": "Random Variables and their Distributions",
    "section": "Normal Probability Distribution",
    "text": "Normal Probability Distribution\n\nA geneticist working for a seed company develops a new carrot for growing in heavy clay soil. After measuring 5000 of these carrots, it can be said that the carrot length, Y, is normally distributed with \\mu = 11.5 cm and \\sigma = 1.15 cm.\n\nWhat is the probability that a carrot will be 12 cm or larger?\n\n\n\npnorm(q = 12, mean = 11.5, sd = 1.15, lower.tail = FALSE)\n\n[1] 0.3318601"
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#gamma-probability-distribution",
    "href": "files/lectures/03-2-probability-distributions.html#gamma-probability-distribution",
    "title": "Random Variables and their Distributions",
    "section": "Gamma Probability Distribution",
    "text": "Gamma Probability Distribution\n\nGamma Distribution"
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#gamma-probability-distribution-1",
    "href": "files/lectures/03-2-probability-distributions.html#gamma-probability-distribution-1",
    "title": "Random Variables and their Distributions",
    "section": "Gamma Probability Distribution",
    "text": "Gamma Probability Distribution\n\nA random variable Y is said to have a gamma distribution with parameters \\alpha &gt; 0 and \\beta &gt; 0 iff,\n\n\nf(y) = \\frac{y^{\\alpha-1} e^{-y/\\beta}}{\\beta^{\\alpha} \\Gamma(\\alpha)}, \\ 0 \\le y &lt; \\infty\n\n\nNote that \\Gamma(\\alpha) = \\int_{0}^{\\infty} y^{\\alpha-1} e^{-y} \\ dy.\nIf Y has a gamma distribution with parameters \\alpha and \\beta, then\n\n\nE[Y] = \\mu = \\alpha\\beta \\ \\ \\ \\text{and} \\ \\ \\ V[Y] = \\sigma^2  = \\alpha\\beta^2\n\n\nSee Wackerly pg. 187 for derivation."
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#gamma-probability-distribution-2",
    "href": "files/lectures/03-2-probability-distributions.html#gamma-probability-distribution-2",
    "title": "Random Variables and their Distributions",
    "section": "Gamma Probability Distribution",
    "text": "Gamma Probability Distribution\n\nWe can use R to find information related to the Gamma distribution.\n\nP[X \\le x]: pgamma(q, shape, rate)\n\nP[X \\ge x]: pgamma(q, shape, rate, lower.tail = FALSE)\n\nIn the functions:\n\nq is the value of X we are interested in\n\nshape is the shape parameter, \\alpha\n\nscale is the scale parameter, \\beta\n\nAlternatively, can parameterize with rate = 1/\\beta, rate = 1 / scale\n\nlower.tail has two options:\n\nTRUE (default) returns P[X \\le x]\n\nFALSE returns P[X \\ge x]"
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#gamma-probability-distribution-3",
    "href": "files/lectures/03-2-probability-distributions.html#gamma-probability-distribution-3",
    "title": "Random Variables and their Distributions",
    "section": "Gamma Probability Distribution",
    "text": "Gamma Probability Distribution\n\nAnnual incomes for heads of household in an affluent section of a city have approximately a gamma distribution with \\alpha=32 and \\beta=2500. Determine:\n\nThe probability distribution. \nThe mean of the distribution. \nThe variance and standard deviation of the distribution."
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#gamma-probability-distribution-4",
    "href": "files/lectures/03-2-probability-distributions.html#gamma-probability-distribution-4",
    "title": "Random Variables and their Distributions",
    "section": "Gamma Probability Distribution",
    "text": "Gamma Probability Distribution\n\nAnnual incomes for heads of household in an affluent section of a city have approximately a gamma distribution with \\alpha=32 and \\beta=2500.\n\nWhat proportion have incomes in excess of $100,000?\nWhat proportion have incomes between $75,000 and $150,000?"
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#gamma-probability-distribution-5",
    "href": "files/lectures/03-2-probability-distributions.html#gamma-probability-distribution-5",
    "title": "Random Variables and their Distributions",
    "section": "Gamma Probability Distribution",
    "text": "Gamma Probability Distribution\n\nAnnual incomes for heads of household in a section of a city have approximately a gamma distribution with \\alpha=32 and \\beta=2500.\n\nWhat proportion have incomes in excess of $30,000?\n\n\n\npgamma(q = 100000, shape = 32, scale = 2500, lower.tail = FALSE)\n\n[1] 0.08552057"
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#gamma-probability-distribution-6",
    "href": "files/lectures/03-2-probability-distributions.html#gamma-probability-distribution-6",
    "title": "Random Variables and their Distributions",
    "section": "Gamma Probability Distribution",
    "text": "Gamma Probability Distribution\n\nAnnual incomes for heads of household in an affluent section of a city have approximately a gamma distribution with \\alpha=32 and \\beta=2500.\n\nWhat proportion have incomes between $75,000 and $150,000?\n\n\n\npgamma(q = 150000, shape = 32, scale = 2500) - pgamma(q = 75000, shape = 32, scale = 2500)\n\n[1] 0.6186147"
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#beta-probability-distribution",
    "href": "files/lectures/03-2-probability-distributions.html#beta-probability-distribution",
    "title": "Random Variables and their Distributions",
    "section": "Beta Probability Distribution",
    "text": "Beta Probability Distribution\n\nBeta Distribution"
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#beta-probability-distribution-1",
    "href": "files/lectures/03-2-probability-distributions.html#beta-probability-distribution-1",
    "title": "Random Variables and their Distributions",
    "section": "Beta Probability Distribution",
    "text": "Beta Probability Distribution\n\nA random variable Y is said to have a beta distribution with parameters \\alpha &gt; 0 and \\beta &gt; 0 iff,\n\n\nf(y) = \\frac{y^{\\alpha-1}(1-y)^{\\beta-1}}{B(\\alpha,\\beta)}, \\ 0 \\le y \\le 1\n\n\nNote: B(\\alpha,\\beta) = \\int_0^1 y^{\\alpha-1}(1-y)^{\\beta-1} \\ dy = \\frac{\\Gamma(\\alpha) \\Gamma(\\beta)}{\\Gamma(\\alpha+\\beta)}.\nIf Y has a beta distribution with parameters \\alpha &gt; 0 and \\beta &gt; 0, then\n\n\nE[Y] = \\mu = \\frac{\\alpha}{\\alpha+\\beta} \\ \\ \\ \\text{and} \\ \\ \\ V[Y] = \\sigma^2  = \\frac{\\alpha\\beta}{(\\alpha+\\beta)^2(\\alpha+\\beta+1)}"
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#beta-probability-distribution-2",
    "href": "files/lectures/03-2-probability-distributions.html#beta-probability-distribution-2",
    "title": "Random Variables and their Distributions",
    "section": "Beta Probability Distribution",
    "text": "Beta Probability Distribution\n\nWe can use R to find information related to the Beta distribution.\n\nP[X \\le x]: pbeta(q, shape1, shape2)\n\nP[X \\ge x]: pbeta(q, shape1, shape2, lower.tail = FALSE)\n\nIn the functions:\n\nq is the value of X we are interested in – must be in [0, 1]!\nshape1 is the first shape parameter, \\alpha\n\nshape2 is the second shape parameter, \\beta\n\nlower.tail has two options:\n\nTRUE (default) returns P[X \\le x]\n\nFALSE returns P[X \\ge x]"
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#beta-probability-distribution-3",
    "href": "files/lectures/03-2-probability-distributions.html#beta-probability-distribution-3",
    "title": "Random Variables and their Distributions",
    "section": "Beta Probability Distribution",
    "text": "Beta Probability Distribution\n\nIn a survey of cupcake preferences, 8 respondents liked the new cupcake flavor and 2 did not. We will model the proportion of all respondents who would like the cupcake flavor using a Beta distribution with \\alpha = 8 and \\beta = 2. Determine:\n\nThe probability distribution. \nThe mean of the distribution. \nThe variance and standard deviation of the distribution."
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#beta-probability-distribution-4",
    "href": "files/lectures/03-2-probability-distributions.html#beta-probability-distribution-4",
    "title": "Random Variables and their Distributions",
    "section": "Beta Probability Distribution",
    "text": "Beta Probability Distribution\n\nIn a survey of cupcake preferences, 8 respondents liked the new cupcake flavor and 2 did not. We will model the proportion of all respondents who would like the cupcake flavor using a Beta distribution with \\alpha = 8 and \\beta = 2. What is the probability that:\n\nfewer than 60% of respondents like the new flavor?\nmore than 90% of respondents like the new flavor?\nsomewhere between 70% and 90% of respondents like the new flavor?"
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#beta-probability-distribution-5",
    "href": "files/lectures/03-2-probability-distributions.html#beta-probability-distribution-5",
    "title": "Random Variables and their Distributions",
    "section": "Beta Probability Distribution",
    "text": "Beta Probability Distribution\n\nIn a survey of cupcake preferences, 8 respondents liked the new cupcake flavor and 2 did not. We will model the proportion of all respondents who would like the cupcake flavor using a Beta distribution with \\alpha = 8 and \\beta = 2. What is the probability that:\n\nfewer than 60% of respondents like the new flavor?\n\n\n\npbeta(q = 0.6, shape1 = 8, shape2 = 2)\n\n[1] 0.07054387"
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#beta-probability-distribution-6",
    "href": "files/lectures/03-2-probability-distributions.html#beta-probability-distribution-6",
    "title": "Random Variables and their Distributions",
    "section": "Beta Probability Distribution",
    "text": "Beta Probability Distribution\n\nIn a survey of cupcake preferences, 8 respondents liked the new cupcake flavor and 2 did not. We will model the proportion of all respondents who would like the cupcake flavor using a Beta distribution with \\alpha = 8 and \\beta = 2. What is the probability that:\n\nmore than 90% of respondents like the new flavor?\n\n\n\npbeta(q = 0.9, shape1 = 8, shape2 = 2, lower.tail = FALSE)\n\n[1] 0.225159"
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#beta-probability-distribution-7",
    "href": "files/lectures/03-2-probability-distributions.html#beta-probability-distribution-7",
    "title": "Random Variables and their Distributions",
    "section": "Beta Probability Distribution",
    "text": "Beta Probability Distribution\n\nIn a survey of cupcake preferences, 8 respondents liked the new cupcake flavor and 2 did not. We will model the proportion of all respondents who would like the cupcake flavor using a Beta distribution with \\alpha = 8 and \\beta = 2. What is the probability that:\n\nsomewhere between 70% and 90% of respondents like the new flavor?\n\n\n\npbeta(q = 0.9, shape1 = 8, shape2 = 2) - pbeta(q = 0.7, shape1 = 8, shape2 = 2)\n\n[1] 0.5788377"
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#wrap-up",
    "href": "files/lectures/03-2-probability-distributions.html#wrap-up",
    "title": "Random Variables and their Distributions",
    "section": "Wrap Up",
    "text": "Wrap Up\n\nYou now are ready to work on Assignment 1: Probability Distributions.\n\nDue Tuesday, September 16.\n.qmd file is available to download on Canvas.\n\nGoals:\n\nIdentify the applicable distribution.\nFind probabilities using said distributions.\n\nNext week:\n\nMonday: Thinking like a Bayesian\nWednesday: Beta-Binomial"
  },
  {
    "objectID": "files/lectures/06-1-conjugate-families.html#introduction-beta-binomial-model",
    "href": "files/lectures/06-1-conjugate-families.html#introduction-beta-binomial-model",
    "title": "Conjugate Families",
    "section": "Introduction: Beta-Binomial Model",
    "text": "Introduction: Beta-Binomial Model\n\nOn Tuesday, we learned how to think like a Bayesian.\nToday, we will formalize the model we muddled through last time.\nThis is called the Beta-Binomial model.\n\nThe Beta distribution is the prior.\nThe Binomial distribution is the data distribution (or the likeihood).\nThe posterior also follows a Beta distribution.\n\nConjugate family: When the prior and posterior are the same named distribution, but different parameters."
  },
  {
    "objectID": "files/lectures/06-1-conjugate-families.html#example-set-up",
    "href": "files/lectures/06-1-conjugate-families.html#example-set-up",
    "title": "Conjugate Families",
    "section": "Example Set Up",
    "text": "Example Set Up\n\nConsider the following scenario.\n\n“Michelle” has decided to run for president and you’re her campaign manager for the state of Florida.\nAs such, you’ve conducted 30 different polls throughout the election season.\nThough Michelle’s support has hovered around 45%, she polled at around 35% in the dreariest days and around 55% in the best days on the campaign trail."
  },
  {
    "objectID": "files/lectures/06-1-conjugate-families.html#example-set-up-1",
    "href": "files/lectures/06-1-conjugate-families.html#example-set-up-1",
    "title": "Conjugate Families",
    "section": "Example Set Up",
    "text": "Example Set Up\n\nPast polls provide prior information about \\pi, the proportion of Floridians that currently support Michelle.\n\nIn fact, we can reorganize this information into a formal prior probability model of \\pi.\n\nIn a previous problem, we assumed that \\pi could only be 0.2, 0.5, or 0.8, the corresponding chances of which were defined by a discrete probability model.\n\nHowever, in the reality of Michelle’s election support, \\pi \\in [0, 1].\n\nWe can reflect this reality and conduct a Bayesian analysis by constructing a continuous prior probability model of \\pi."
  },
  {
    "objectID": "files/lectures/06-1-conjugate-families.html#example-set-up-2",
    "href": "files/lectures/06-1-conjugate-families.html#example-set-up-2",
    "title": "Conjugate Families",
    "section": "Example Set Up",
    "text": "Example Set Up\n\n\n\n\nA reasonable prior is represented by the curve on the right.\n\nNotice that this curve preserves the overall information and variability in the past polls, i.e., Michelle’s support, \\pi can be anywhere between 0 and 1, but is most likely around 0.45."
  },
  {
    "objectID": "files/lectures/06-1-conjugate-families.html#example-set-up-3",
    "href": "files/lectures/06-1-conjugate-families.html#example-set-up-3",
    "title": "Conjugate Families",
    "section": "Example Set Up",
    "text": "Example Set Up\n\nIncorporating this more nuanced, continuous view of Michelle’s support, \\pi, will require some new tools.\n\nNo matter if our parameter \\pi is continuous or discrete, the posterior model of \\pi will combine insights from the prior and data.\n\\pi isn’t the only variable of interest that lives on [0,1].\n\nMaybe we’re interested in modeling the proportion of people that use public transit, the proportion of trains that are delayed, the proportion of people that prefer cats to dogs, etc.\n\nThe Beta-Binomial model provides the tools we need to study the proportion of interest, \\pi, in each of these settings."
  },
  {
    "objectID": "files/lectures/06-1-conjugate-families.html#beta-prior",
    "href": "files/lectures/06-1-conjugate-families.html#beta-prior",
    "title": "Conjugate Families",
    "section": "Beta Prior",
    "text": "Beta Prior\n\n\n\n\nIn building the Bayesian election model of Michelle’s election support among Floridians, \\pi, we begin with the prior.\n\nOur continuous prior probability model of \\pi is specified by the probability density function (pdf).\n\nWhat values can \\pi take and which are more plausible than others?"
  },
  {
    "objectID": "files/lectures/06-1-conjugate-families.html#beta-prior-1",
    "href": "files/lectures/06-1-conjugate-families.html#beta-prior-1",
    "title": "Conjugate Families",
    "section": "Beta Prior",
    "text": "Beta Prior\n\nLet \\pi be a random variable, where \\pi \\in [0, 1].\nThe variability in \\pi may be captured by a Beta model with shape hyperparameters \\alpha &gt; 0 and \\beta &gt; 0,\n\nhyperparameter: a parameter used in a prior model.\n\n\n\\pi \\sim \\text{Beta}(\\alpha, \\beta),"
  },
  {
    "objectID": "files/lectures/06-1-conjugate-families.html#beta-prior-shapes",
    "href": "files/lectures/06-1-conjugate-families.html#beta-prior-shapes",
    "title": "Conjugate Families",
    "section": "Beta Prior: Shapes",
    "text": "Beta Prior: Shapes\n\nLet’s explore the shape of the Beta:\n\n\n\nplot_beta(1, 5) + theme_bw() + ggtitle(\"Beta(1, 5)\")"
  },
  {
    "objectID": "files/lectures/06-1-conjugate-families.html#beta-prior-shapes-1",
    "href": "files/lectures/06-1-conjugate-families.html#beta-prior-shapes-1",
    "title": "Conjugate Families",
    "section": "Beta Prior: Shapes",
    "text": "Beta Prior: Shapes\n\nLet’s explore the shape of the Beta:\n\n\n\nplot_beta(1, 2) + theme_bw() + ggtitle(\"Beta(1, 2)\")"
  },
  {
    "objectID": "files/lectures/06-1-conjugate-families.html#beta-prior-shapes-2",
    "href": "files/lectures/06-1-conjugate-families.html#beta-prior-shapes-2",
    "title": "Conjugate Families",
    "section": "Beta Prior: Shapes",
    "text": "Beta Prior: Shapes\n\nLet’s explore the shape of the Beta:\n\n\n\nplot_beta(3, 7) + theme_bw() + ggtitle(\"Beta(3, 7)\")"
  },
  {
    "objectID": "files/lectures/06-1-conjugate-families.html#beta-prior-shapes-3",
    "href": "files/lectures/06-1-conjugate-families.html#beta-prior-shapes-3",
    "title": "Conjugate Families",
    "section": "Beta Prior: Shapes",
    "text": "Beta Prior: Shapes\n\nLet’s explore the shape of the Beta:\n\n\n\nplot_beta(1, 1) + theme_bw() + ggtitle(\"Beta(1, 1)\")"
  },
  {
    "objectID": "files/lectures/06-1-conjugate-families.html#beta-prior-shapes-4",
    "href": "files/lectures/06-1-conjugate-families.html#beta-prior-shapes-4",
    "title": "Conjugate Families",
    "section": "Beta Prior: Shapes",
    "text": "Beta Prior: Shapes\n\nYour turn!\nHow would you describe the typical behavior of a:\n\nBeta(\\alpha, \\beta) variable, \\pi, when \\alpha=\\beta?\nBeta(\\alpha, \\beta) variable, \\pi, when \\alpha&gt;\\beta?\nBeta(\\alpha, \\beta) variable, \\pi, when \\alpha&lt;\\beta?\n\nFor which model is there greater variability in the plausible values of \\pi, Beta(20, 20) or Beta(5, 5)?"
  },
  {
    "objectID": "files/lectures/06-1-conjugate-families.html#beta-prior-shapes-5",
    "href": "files/lectures/06-1-conjugate-families.html#beta-prior-shapes-5",
    "title": "Conjugate Families",
    "section": "Beta Prior: Shapes",
    "text": "Beta Prior: Shapes\n\nHow would you describe the typical behavior of a Beta(\\alpha, \\beta) variable, \\pi, when \\alpha=\\beta?"
  },
  {
    "objectID": "files/lectures/06-1-conjugate-families.html#beta-prior-shapes-6",
    "href": "files/lectures/06-1-conjugate-families.html#beta-prior-shapes-6",
    "title": "Conjugate Families",
    "section": "Beta Prior: Shapes",
    "text": "Beta Prior: Shapes\n\nHow would you describe the typical behavior of a Beta(\\alpha, \\beta) variable, \\pi, when \\alpha&gt;\\beta?"
  },
  {
    "objectID": "files/lectures/06-1-conjugate-families.html#beta-prior-shapes-7",
    "href": "files/lectures/06-1-conjugate-families.html#beta-prior-shapes-7",
    "title": "Conjugate Families",
    "section": "Beta Prior: Shapes",
    "text": "Beta Prior: Shapes\n\nHow would you describe the typical behavior of a Beta(\\alpha, \\beta) variable, \\pi, when \\alpha&lt;\\beta?"
  },
  {
    "objectID": "files/lectures/06-1-conjugate-families.html#beta-prior-shapes-8",
    "href": "files/lectures/06-1-conjugate-families.html#beta-prior-shapes-8",
    "title": "Conjugate Families",
    "section": "Beta Prior: Shapes",
    "text": "Beta Prior: Shapes\n\nFor which model is there greater variability in the plausible values of \\pi, Beta(20, 20) or Beta(5, 5)?"
  },
  {
    "objectID": "files/lectures/06-1-conjugate-families.html#tuning-the-beta-prior",
    "href": "files/lectures/06-1-conjugate-families.html#tuning-the-beta-prior",
    "title": "Conjugate Families",
    "section": "Tuning the Beta Prior",
    "text": "Tuning the Beta Prior\n\nWe can tune the shape hyperparameters (\\alpha and \\beta) to reflect our prior information about Michelle’s election support, \\pi.\nIn our example, we saw that she polled between 25 and 65 percentage points, with an average of 45 percentage points.\n\nWe want our Beta(\\alpha, \\beta) to have similar patterns, so we should pick \\alpha and \\beta such that \\pi is around 0.45.\n\n\n\nE[\\pi] = \\frac{\\alpha}{\\alpha+\\beta} \\approx 0.45\n\n\nUsing algebra, we can tune, and find\n\n\\alpha \\approx \\frac{9}{11} \\beta"
  },
  {
    "objectID": "files/lectures/06-1-conjugate-families.html#tuning-the-beta-prior-1",
    "href": "files/lectures/06-1-conjugate-families.html#tuning-the-beta-prior-1",
    "title": "Conjugate Families",
    "section": "Tuning the Beta Prior",
    "text": "Tuning the Beta Prior\n\nYour turn!\n\nGraph the following and determine which is best for the example.\n\n\n\nplot_beta(9, 11) + theme_bw()\nplot_beta(27, 33) + theme_bw()\nplot_beta(45, 55) + theme_bw()\n\n\nRecall, this is what we are going for:"
  },
  {
    "objectID": "files/lectures/06-1-conjugate-families.html#tuning-the-beta-prior-2",
    "href": "files/lectures/06-1-conjugate-families.html#tuning-the-beta-prior-2",
    "title": "Conjugate Families",
    "section": "Tuning the Beta Prior",
    "text": "Tuning the Beta Prior\n\n\nplot_beta(9, 11) + theme_bw() + ggtitle(\"Beta(9, 11)\")"
  },
  {
    "objectID": "files/lectures/06-1-conjugate-families.html#tuning-the-beta-prior-3",
    "href": "files/lectures/06-1-conjugate-families.html#tuning-the-beta-prior-3",
    "title": "Conjugate Families",
    "section": "Tuning the Beta Prior",
    "text": "Tuning the Beta Prior\n\n\nplot_beta(27, 33) + theme_bw() + ggtitle(\"Beta(27, 33)\")"
  },
  {
    "objectID": "files/lectures/06-1-conjugate-families.html#tuning-the-beta-prior-4",
    "href": "files/lectures/06-1-conjugate-families.html#tuning-the-beta-prior-4",
    "title": "Conjugate Families",
    "section": "Tuning the Beta Prior",
    "text": "Tuning the Beta Prior\n\n\nplot_beta(45, 55) + theme_bw() + ggtitle(\"Beta(45, 55)\")"
  },
  {
    "objectID": "files/lectures/06-1-conjugate-families.html#tuning-the-beta-prior-5",
    "href": "files/lectures/06-1-conjugate-families.html#tuning-the-beta-prior-5",
    "title": "Conjugate Families",
    "section": "Tuning the Beta Prior",
    "text": "Tuning the Beta Prior\n\nNow that we have a prior, we “know” some things.\n\n\\pi \\sim \\text{Beta}(45, 55)\n\nFrom the properties of the beta distribution,\n\n\n\\begin{equation*}\n\\begin{aligned}\nE[\\pi] &= \\frac{\\alpha}{\\alpha + \\beta} & \\text{ and } & \\text{ } & \\text{ }  \\\\\n&=\\frac{45}{45+55} \\\\\n&= 0.45\n\\end{aligned}\n\\begin{aligned}\n\\text{var}[\\pi] &= \\frac{\\alpha\\beta}{(\\alpha+\\beta)^2(\\alpha+\\beta+1)} \\\\\n&= \\frac{(45)(55)}{(45+55)^2(45+55+1)} \\\\\n&= 0.0025\n\\end{aligned}\n\\end{equation*}"
  },
  {
    "objectID": "files/lectures/06-1-conjugate-families.html#binomial-data-model",
    "href": "files/lectures/06-1-conjugate-families.html#binomial-data-model",
    "title": "Conjugate Families",
    "section": "Binomial Data Model",
    "text": "Binomial Data Model\n\nA new poll of n = 50 Floridians recorded Y, the number that support Michelle.\n\nThe results depend upon \\pi (as \\pi increases, Y tends to increase).\n\nTo model the dependence of Y on \\pi, we assume\n\nvoters answer the poll independently of one another;\nthe probability that any polled voter supports your candidate Michelle is \\pi\n\nThis is a binomial event, Y|\\pi \\sim \\text{Bin}(50, \\pi), with conditional pmf, f(y|\\pi) defined for y \\in \\{0, 1, ..., 50\\}\n\nf(y|\\pi) = P[Y = y|\\pi] = {50 \\choose y} \\pi^y (1-\\pi)^{50-y}"
  },
  {
    "objectID": "files/lectures/06-1-conjugate-families.html#binomial-data-model-1",
    "href": "files/lectures/06-1-conjugate-families.html#binomial-data-model-1",
    "title": "Conjugate Families",
    "section": "Binomial Data Model",
    "text": "Binomial Data Model\n\nThe conditional pmf, f(y|\\pi), gives us answers to a hypothetical question:\n\nIf Michelle’s support were given some value of \\pi, then how many of the 50 polled voters (Y=y) might we expect to suppport her?\n\nLet’s look at this graphically:\n\n\nbinom_prob &lt;- tibble(n_success = 1:sample_size,\n                     prob = dbinom(n_success, size=sample_size, prob=pi_value))\n\nbinom_prob %&gt;%\n  ggplot(aes(x=n_success,y=prob))+\n  geom_col(width=0.2)+\n  labs(x= \"Number of Successes\",\n       y= \"Probability\") +\n  theme_bw()"
  },
  {
    "objectID": "files/lectures/06-1-conjugate-families.html#binomial-data-model-2",
    "href": "files/lectures/06-1-conjugate-families.html#binomial-data-model-2",
    "title": "Conjugate Families",
    "section": "Binomial Data Model",
    "text": "Binomial Data Model"
  },
  {
    "objectID": "files/lectures/06-1-conjugate-families.html#binomial-data-model-3",
    "href": "files/lectures/06-1-conjugate-families.html#binomial-data-model-3",
    "title": "Conjugate Families",
    "section": "Binomial Data Model",
    "text": "Binomial Data Model\n\nIt is observed that Y=30 of the n=50 polled voters support Michelle.\nWe now want to find the likelihood function – remember that we treat Y=30 as the observed data and \\pi as unknown,\n\n\n\\begin{align*}\nf(y|\\pi) &= {50 \\choose y} \\pi^y (1-\\pi)^{50-y} \\\\\nL(\\pi|y=30) &= {50 \\choose 30} \\pi^{30} (1-\\pi)^{20}\n\\end{align*}\n\n\nThis is valid for \\pi \\in [0, 1]."
  },
  {
    "objectID": "files/lectures/06-1-conjugate-families.html#binomial-data-model-4",
    "href": "files/lectures/06-1-conjugate-families.html#binomial-data-model-4",
    "title": "Conjugate Families",
    "section": "Binomial Data Model",
    "text": "Binomial Data Model\n\nWhat is the likelihood of 30/50 voters supporting Michelle?\n\n\ndbinom(30, 50, pi)\n\n\nYou try this for \\pi = \\{0.25, 0.50, 0.75\\}.\n\n\ndbinom(30, 50, 0.25)\ndbinom(30, 50, 0.5)\ndbinom(30, 50, 0.75)"
  },
  {
    "objectID": "files/lectures/06-1-conjugate-families.html#binomial-data-model-5",
    "href": "files/lectures/06-1-conjugate-families.html#binomial-data-model-5",
    "title": "Conjugate Families",
    "section": "Binomial Data Model",
    "text": "Binomial Data Model\n\nWhat is the likelihood of 30/50 voters supporting Michelle?\n\n\ndbinom(30, 50, 0.25)\n\n[1] 1.29633e-07\n\ndbinom(30, 50, 0.5)\n\n[1] 0.04185915\n\ndbinom(30, 50, 0.75)\n\n[1] 0.007654701"
  },
  {
    "objectID": "files/lectures/06-1-conjugate-families.html#binomial-data-model-6",
    "href": "files/lectures/06-1-conjugate-families.html#binomial-data-model-6",
    "title": "Conjugate Families",
    "section": "Binomial Data Model",
    "text": "Binomial Data Model\n\nChallenge!\nCreate a graph showing what happens to the likelihood for different values of \\pi.\n\ni.e., have \\pi on the x-axis and likelihood on the y-axis.\n\nTo get you started,\n\n\ngraph &lt;- tibble(pi = seq(0, 1, 0.001)) %&gt;%\n  mutate(likelihood = dbinom(30, 50, pi))"
  },
  {
    "objectID": "files/lectures/06-1-conjugate-families.html#binomial-data-model-7",
    "href": "files/lectures/06-1-conjugate-families.html#binomial-data-model-7",
    "title": "Conjugate Families",
    "section": "Binomial Data Model",
    "text": "Binomial Data Model\n\nCreate a graph showing what happens to the likelihood for different values of \\pi.\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhere is the maximum?"
  },
  {
    "objectID": "files/lectures/06-1-conjugate-families.html#binomial-data-model-8",
    "href": "files/lectures/06-1-conjugate-families.html#binomial-data-model-8",
    "title": "Conjugate Families",
    "section": "Binomial Data Model",
    "text": "Binomial Data Model\n\nWhere is the maximum?"
  },
  {
    "objectID": "files/lectures/06-1-conjugate-families.html#the-beta-posterior-model",
    "href": "files/lectures/06-1-conjugate-families.html#the-beta-posterior-model",
    "title": "Conjugate Families",
    "section": "The Beta Posterior Model",
    "text": "The Beta Posterior Model\n\nLooking at just the prior and the data distributions,\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe prior is a bit more pessimistic about Michelle’s election support than the data obtained from the latest poll."
  },
  {
    "objectID": "files/lectures/06-1-conjugate-families.html#the-beta-posterior-model-1",
    "href": "files/lectures/06-1-conjugate-families.html#the-beta-posterior-model-1",
    "title": "Conjugate Families",
    "section": "The Beta Posterior Model",
    "text": "The Beta Posterior Model\n\nNow including the posterior,\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe can see that the posterior model of \\pi is continuous and \\in [0, 1].\nThe shape of the posterior appears to also have a Beta(\\alpha, \\beta) model.\n\nThe shape parameters (\\alpha and \\beta) have been updated."
  },
  {
    "objectID": "files/lectures/06-1-conjugate-families.html#the-beta-posterior-model-2",
    "href": "files/lectures/06-1-conjugate-families.html#the-beta-posterior-model-2",
    "title": "Conjugate Families",
    "section": "The Beta Posterior Model",
    "text": "The Beta Posterior Model\n\nIf we were to collect more information about Michelle’s support, we would use the current posterior as the new prior, then update our posterior.\n\nHow do we know what the updated parameters are?\n\n\n\nsummarize_beta_binomial(alpha = 45, beta = 55, y = 30, n = 50)"
  },
  {
    "objectID": "files/lectures/06-1-conjugate-families.html#the-beta-posterior-model-3",
    "href": "files/lectures/06-1-conjugate-families.html#the-beta-posterior-model-3",
    "title": "Conjugate Families",
    "section": "The Beta Posterior Model",
    "text": "The Beta Posterior Model\n\nWe used Michelle’s election support to understand the Beta-Binomial model.\nLet’s now generalize it for any appropriate situation.\n\n\n\\begin{align*}\nY|\\pi &\\sim \\text{Bin}(n, \\pi) \\\\\n\\pi &\\sim \\text{Beta}(\\alpha, \\beta) \\\\\n\\pi | (Y=y) &\\sim \\text{Beta}(\\alpha+y, \\beta+n-y)\n\\end{align*}\n\n\nWe can see that the posterior distribution reveals the influence of the prior (\\alpha and \\beta) and data (y and n)."
  },
  {
    "objectID": "files/lectures/06-1-conjugate-families.html#the-beta-posterior-model-4",
    "href": "files/lectures/06-1-conjugate-families.html#the-beta-posterior-model-4",
    "title": "Conjugate Families",
    "section": "The Beta Posterior Model",
    "text": "The Beta Posterior Model\n\nUnder this updated distribution,\n\n\n\\pi | (Y=y) \\sim \\text{Beta}(\\alpha+y, \\beta+n-y)\n\n\nwe have updated moments:\n\n\n\\begin{align*}\nE[\\pi | Y = y] &= \\frac{\\alpha + y}{\\alpha + \\beta + n} \\\\\n\\text{Var}[\\pi|Y=y] &= \\frac{(\\alpha+y)(\\beta+n-y)}{(\\alpha+\\beta+n)^2(\\alpha+\\beta+1)}\n\\end{align*}"
  },
  {
    "objectID": "files/lectures/06-1-conjugate-families.html#the-beta-posterior-model-5",
    "href": "files/lectures/06-1-conjugate-families.html#the-beta-posterior-model-5",
    "title": "Conjugate Families",
    "section": "The Beta Posterior Model",
    "text": "The Beta Posterior Model\n\nLet’s pause and think about this from a theoretical standpoint.\nThe Beta distribution is a conjugate prior for the likelihood.\n\nConjugate prior: the posterior is from the same model family as the prior.\n\nRecall the Beta prior, f(\\pi),\n\n L(\\pi|y) = {n \\choose y} \\pi^y (1-\\pi)^{n-y} \n\nand the likelihood function, L(\\pi|y).\n\n f(\\pi) = \\frac{\\Gamma(\\alpha+\\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)} \\pi^{\\alpha-1}(1-\\alpha)^{\\beta-1}"
  },
  {
    "objectID": "files/lectures/06-1-conjugate-families.html#the-beta-posterior-model-6",
    "href": "files/lectures/06-1-conjugate-families.html#the-beta-posterior-model-6",
    "title": "Conjugate Families",
    "section": "The Beta Posterior Model",
    "text": "The Beta Posterior Model\n\nWe can put the prior and likelihood together to create the posterior,\n\n\n\\begin{align*}\nf(\\pi|y) &\\propto f(\\pi)L(\\pi|y) \\\\\n&= \\frac{\\Gamma(\\alpha+\\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)} \\pi^{\\alpha-1}(1-\\pi)^{\\beta-1} \\times {n \\choose y} \\pi^y (1-\\pi)^{n-1} \\\\\n&\\propto \\pi^{(\\alpha+y)-1} (1-\\pi)^{(\\beta+n-y)-1}\n\\end{align*}\n\n\nThis is the same structure as the normalized Beta(\\alpha+y, \\beta+n-y),\n\nf(\\pi|y) = \\frac{\\Gamma(\\alpha+\\beta+n)}{\\Gamma(\\alpha+y) \\Gamma(\\beta+n-y)} \\pi^{(\\alpha+y)-1} (1-\\pi)^{(\\beta+n-y)-1}"
  },
  {
    "objectID": "files/lectures/06-1-conjugate-families.html#wrap-up-beta-binomial-model",
    "href": "files/lectures/06-1-conjugate-families.html#wrap-up-beta-binomial-model",
    "title": "Conjugate Families",
    "section": "Wrap Up: Beta-Binomial Model",
    "text": "Wrap Up: Beta-Binomial Model\n\nWe have built the Beta-Binomial model for \\pi, an unknown proportion.\n\n\n\\begin{equation*}\n\\begin{aligned}\nY|\\pi &\\sim \\text{Bin}(n,\\pi) \\\\\n\\pi &\\sim \\text{Beta}(\\alpha,\\beta) &\n\\end{aligned}\n\\Rightarrow\n\\begin{aligned}\n&& \\pi | (Y=y) &\\sim \\text{Beta}(\\alpha+y, \\beta+n-y) \\\\\n\\end{aligned}\n\\end{equation*}\n\n\nThe prior model, f(\\pi), is given by Beta(\\alpha,\\beta).\nThe data model, f(Y|\\pi), is given by Bin(n,\\pi).\nThe likelihood function, L(\\pi|y), is obtained by plugging y into the Binomial pmf.\nThe posterior model is a Beta distribution with updated parameters \\alpha+y and \\beta+n-y."
  },
  {
    "objectID": "files/lectures/06-1-conjugate-families.html#introduction-gamma-poisson-model",
    "href": "files/lectures/06-1-conjugate-families.html#introduction-gamma-poisson-model",
    "title": "Conjugate Families",
    "section": "Introduction: Gamma-Poisson Model",
    "text": "Introduction: Gamma-Poisson Model\n\nRecall the Beta-Binomial model,\n\ny \\sim \\text{Bin}(n, \\pi) (data distribution)\n\\pi \\sim \\text{Beta}(\\alpha, \\beta) (prior distribution)\n\\pi|y \\sim \\text{Beta}(\\alpha+y, \\beta+n-y) (posterior distribution)\n\nThe Beta-Binomial model is from a conjugate family (i.e., the posterior is from the same model family as the prior).\nNow, we will learn about the Gamma-Poisson, another conjugate family."
  },
  {
    "objectID": "files/lectures/06-1-conjugate-families.html#example-set-up-4",
    "href": "files/lectures/06-1-conjugate-families.html#example-set-up-4",
    "title": "Conjugate Families",
    "section": "Example Set Up",
    "text": "Example Set Up\n\nSuppose we are now interested in modeling the number of spam calls we receive.\n\nThis means that we are modeling the rate, \\lambda.\n\nWe take a guess and say that the value of \\lambda that is most likely is around 5,\n\n… but reasonably ranges between 2 and 7 calls per day.\n\nWhy can’t we use the Beta distribution as our prior distribution?\n\n\\lambda is the mean of a count \\to \\lambda \\in \\mathbb{R}^+ \\to \\lambda is not limited to [0, 1] \\to broken assumption for Beta distribution.\n\nWhy can’t we use the binomial distribution as our data distribution?\n\nY_i is a count \\to Y_i \\in \\mathbb{N}^+ \\to Y_i is not limited to \\{0, 1\\} \\to broken assumption for Binomial distribution."
  },
  {
    "objectID": "files/lectures/06-1-conjugate-families.html#poisson-data-model",
    "href": "files/lectures/06-1-conjugate-families.html#poisson-data-model",
    "title": "Conjugate Families",
    "section": "Poisson Data Model",
    "text": "Poisson Data Model\n\nWe will use the Poisson distribution to model the number of spam calls\n\nY \\in \\{0, 1, 2, ...\\}\n\nY is the number of independent events that occur in a fixed amount of time or space.\n\\lambda &gt; 0 is the rate at which these events occur.\nMathematically,\n\n Y | \\lambda \\sim \\text{Pois}(\\lambda),\n\nwith pmf,\n\nf(y|\\lambda) = \\frac{\\lambda^y e^{-\\lambda}}{y!}, \\ \\ \\ y \\in \\{0,1, 2, ... \\}"
  },
  {
    "objectID": "files/lectures/06-1-conjugate-families.html#gamma-prior",
    "href": "files/lectures/06-1-conjugate-families.html#gamma-prior",
    "title": "Conjugate Families",
    "section": "Gamma Prior",
    "text": "Gamma Prior\n\nIf \\lambda is a continuous random variable that can take on any positive value (\\lambda &gt; 0), then the variability may be modeled with the Gamma distribution with\n\nshape hyperparameter s&gt;0\nrate hyperparameter r&gt;0.\n\nThus,\n\n\\lambda \\sim \\text{Gamma}(s, r)\n\nand the Gamma pdf is given by\n\nf(\\lambda) = \\frac{r^s}{\\Gamma(s)} \\lambda^{s-1} e^{-r\\lambda}"
  },
  {
    "objectID": "files/lectures/06-1-conjugate-families.html#gamma-prior-shapes",
    "href": "files/lectures/06-1-conjugate-families.html#gamma-prior-shapes",
    "title": "Conjugate Families",
    "section": "Gamma Prior: Shapes",
    "text": "Gamma Prior: Shapes\n\nLet’s explore the shape of the Gamma:\n\n\n\nplot_gamma(1, 1) + theme_bw() + ggtitle(\"Gamma(1, 1)\")"
  },
  {
    "objectID": "files/lectures/06-1-conjugate-families.html#gamma-prior-shapes-1",
    "href": "files/lectures/06-1-conjugate-families.html#gamma-prior-shapes-1",
    "title": "Conjugate Families",
    "section": "Gamma Prior: Shapes",
    "text": "Gamma Prior: Shapes\n\nLet’s explore the shape of the Gamma:\n\n\n\nplot_gamma(2, 1) + theme_bw() + ggtitle(\"Gamma(2, 1)\")"
  },
  {
    "objectID": "files/lectures/06-1-conjugate-families.html#gamma-prior-shapes-2",
    "href": "files/lectures/06-1-conjugate-families.html#gamma-prior-shapes-2",
    "title": "Conjugate Families",
    "section": "Gamma Prior: Shapes",
    "text": "Gamma Prior: Shapes\n\nLet’s explore the shape of the Gamma:\n\n\n\nplot_gamma(10, 1) + theme_bw() + ggtitle(\"Gamma(10, 1)\")"
  },
  {
    "objectID": "files/lectures/06-1-conjugate-families.html#gamma-prior-shapes-3",
    "href": "files/lectures/06-1-conjugate-families.html#gamma-prior-shapes-3",
    "title": "Conjugate Families",
    "section": "Gamma Prior: Shapes",
    "text": "Gamma Prior: Shapes\n\nLet’s explore the shape of the Gamma:\n\n\n\nplot_gamma(10, 10) + theme_bw() + ggtitle(\"Gamma(10, 10)\")"
  },
  {
    "objectID": "files/lectures/06-1-conjugate-families.html#gamma-prior-shapes-4",
    "href": "files/lectures/06-1-conjugate-families.html#gamma-prior-shapes-4",
    "title": "Conjugate Families",
    "section": "Gamma Prior: Shapes",
    "text": "Gamma Prior: Shapes\n\nWhat happens when we increase the \\alpha parameter?"
  },
  {
    "objectID": "files/lectures/06-1-conjugate-families.html#gamma-prior-shapes-5",
    "href": "files/lectures/06-1-conjugate-families.html#gamma-prior-shapes-5",
    "title": "Conjugate Families",
    "section": "Gamma Prior: Shapes",
    "text": "Gamma Prior: Shapes\n\nWhat happens when we increase the \\beta parameter?"
  },
  {
    "objectID": "files/lectures/06-1-conjugate-families.html#gamma-prior-shapes-6",
    "href": "files/lectures/06-1-conjugate-families.html#gamma-prior-shapes-6",
    "title": "Conjugate Families",
    "section": "Gamma Prior: Shapes",
    "text": "Gamma Prior: Shapes\n\nPutting these on the same scale,"
  },
  {
    "objectID": "files/lectures/06-1-conjugate-families.html#tuning-the-gamma-prior",
    "href": "files/lectures/06-1-conjugate-families.html#tuning-the-gamma-prior",
    "title": "Conjugate Families",
    "section": "Tuning the Gamma Prior",
    "text": "Tuning the Gamma Prior\n\nLet’s now tune our prior.\nWe are assuming \\lambda \\approx 5, somewhere between 2 and 7.\nWe know the mean of the gamma distribution,\n\nE(\\lambda) = \\frac{s}{r} \\approx 5 \\to 5r \\approx s\n\nYour turn! Use the plot_gamma() function to figure out what value of s and r we need."
  },
  {
    "objectID": "files/lectures/06-1-conjugate-families.html#tuning-the-gamma-prior-1",
    "href": "files/lectures/06-1-conjugate-families.html#tuning-the-gamma-prior-1",
    "title": "Conjugate Families",
    "section": "Tuning the Gamma Prior",
    "text": "Tuning the Gamma Prior\n\nLooking at different values:"
  },
  {
    "objectID": "files/lectures/06-1-conjugate-families.html#poisson-data-model-1",
    "href": "files/lectures/06-1-conjugate-families.html#poisson-data-model-1",
    "title": "Conjugate Families",
    "section": "Poisson Data Model",
    "text": "Poisson Data Model\n\nWe will be taking samples from different days.\n\nWe assume that the daily number of calls may differ from day to day. On each day i,\n\n\nY_i|\\lambda \\sim \\text{Pois}(\\lambda)\n\nThis has a unique pmf for each day (i),\n\nf(y_i|\\lambda) = \\frac{\\lambda^{y_i} e^{-\\lambda}}{y_i!}\n\nBut really, we are interested in the joint information in our sample of n observations.\n\nThe joint pmf gives us this information."
  },
  {
    "objectID": "files/lectures/06-1-conjugate-families.html#poisson-data-model-2",
    "href": "files/lectures/06-1-conjugate-families.html#poisson-data-model-2",
    "title": "Conjugate Families",
    "section": "Poisson Data Model",
    "text": "Poisson Data Model\n\nThe joint pmf for the Poisson,\n\n\n\\begin{align*}\nf\\left(\\overset{\\to}{y_i}|\\lambda\\right) &= \\prod_{i=1}^n f(y_i|\\lambda) \\\\\n&= f(y_1|\\lambda) \\times f(y_2|\\lambda) \\times ... \\times f(y_n|\\lambda) \\\\\n&= \\frac{\\lambda^{y_1}e^{-\\lambda}}{y_1!} \\times \\frac{\\lambda^{y_2}e^{-\\lambda}}{y_2!} \\times ... \\times \\frac{\\lambda^{y_n}e^{-\\lambda}}{y_n!} \\\\\n&= \\frac{\\left( \\lambda^{y_1} \\lambda^{y_2} \\cdot \\cdot \\cdot \\ \\lambda^{y_n}  \\right) \\left( e^{-\\lambda} e^{-\\lambda} \\cdot \\cdot \\cdot e^{-\\lambda}\\right)}{y_1! y_2! \\cdot \\cdot \\cdot y_n!} \\\\\n&= \\frac{\\lambda^{\\sum y_i}e^{-n\\lambda}}{\\prod_{i=1}^n y_i !}\n\\end{align*}"
  },
  {
    "objectID": "files/lectures/06-1-conjugate-families.html#poisson-data-model-3",
    "href": "files/lectures/06-1-conjugate-families.html#poisson-data-model-3",
    "title": "Conjugate Families",
    "section": "Poisson Data Model",
    "text": "Poisson Data Model\n\nIf the joint pmf for the Poisson is\n\nf\\left(\\overset{\\to}{y_i}|\\lambda\\right) = \\frac{\\lambda^{\\sum y_i}e^{-n\\lambda}}{\\prod_{i=1}^n y_i !}\n\nthen the likelihood function for \\lambda &gt; 0 is\n\n\n\\begin{align*}\nL\\left(\\lambda|\\overset{\\to}{y_i}\\right) &= \\frac{\\lambda^{\\sum y_i}e^{-n\\lambda}}{\\prod_{i=1}^n y_i !} \\\\\n& \\propto \\lambda^{\\sum y_i} e^{-n\\lambda}\n\\end{align*}\n\n\nPease see page 102 in the textbook for full derivations."
  },
  {
    "objectID": "files/lectures/06-1-conjugate-families.html#gamma-poisson-conjugacy",
    "href": "files/lectures/06-1-conjugate-families.html#gamma-poisson-conjugacy",
    "title": "Conjugate Families",
    "section": "Gamma-Poisson Conjugacy",
    "text": "Gamma-Poisson Conjugacy\n\nLet \\lambda &gt; 0 be an unknown rate parameter and (Y_1, Y_2, ... , Y_n) be an independent sample from the Poisson distribution.\nThe Gamma-Poisson Bayesian model is as follows:\n\n\n\\begin{align*}\nY_i | \\lambda &\\overset{ind}\\sim \\text{Pois}(\\lambda) \\\\\n\\lambda &\\sim \\text{Gamma}(s, r) \\\\\n\\lambda | \\overset{\\to}y &\\sim \\text{Gamma}\\left( s + \\sum y_i, r + n \\right)\n\\end{align*}\n\n\nThe proof can be seen in section 5.2.4 of the textbook."
  },
  {
    "objectID": "files/lectures/06-1-conjugate-families.html#gamma-poisson-conjugacy-1",
    "href": "files/lectures/06-1-conjugate-families.html#gamma-poisson-conjugacy-1",
    "title": "Conjugate Families",
    "section": "Gamma-Poisson Conjugacy",
    "text": "Gamma-Poisson Conjugacy\n\nSuppose we use Gamma(10, 2) as the prior for \\lambda, the daily rate of calls.\nOn four separate days in the second week of August (i.e., independent days), we received \\overset{\\to}y = (6, 2, 2, 1) calls.\nWe will use the plot_poisson_likelihood() function:\n\n\nplot_poisson_likelihood(y = c(6, 2, 2, 1), lambda_upper_bound = 10)\n\n\nNotes:\n\nlambda_upper_bound limits the x axis – recall that \\lambda \\in (0, \\infty)!\nlambda_upper_bound’s default value is 10."
  },
  {
    "objectID": "files/lectures/06-1-conjugate-families.html#gamma-poisson-conjugacy-2",
    "href": "files/lectures/06-1-conjugate-families.html#gamma-poisson-conjugacy-2",
    "title": "Conjugate Families",
    "section": "Gamma-Poisson Conjugacy",
    "text": "Gamma-Poisson Conjugacy\n\n\n\n\n\n\n\n\n\n\n\n\nWe can see that the average is around 2.75.\n\n\nmean(c(6, 2, 2, 1))\n\n[1] 2.75"
  },
  {
    "objectID": "files/lectures/06-1-conjugate-families.html#gamma-poisson-conjugacy-3",
    "href": "files/lectures/06-1-conjugate-families.html#gamma-poisson-conjugacy-3",
    "title": "Conjugate Families",
    "section": "Gamma-Poisson Conjugacy",
    "text": "Gamma-Poisson Conjugacy\n\nWe know our prior distribution is Gamma(10, 2) and the data distribution is Poi(2.75).\nThus, the posterior is as follows,\n\n\n\\begin{align*}\n\\lambda | \\overset{\\to}y &\\sim \\text{Gamma}\\left( s + \\sum y_i, r + n \\right) \\\\\n&\\sim \\text{Gamma}\\left(10 + 11, 2 + 4 \\right) \\\\\n&\\sim \\text{Gamma}\\left(21, 6 \\right)\n\\end{align*}"
  },
  {
    "objectID": "files/lectures/06-1-conjugate-families.html#the-gamma-posterior-model",
    "href": "files/lectures/06-1-conjugate-families.html#the-gamma-posterior-model",
    "title": "Conjugate Families",
    "section": "The Gamma Posterior Model",
    "text": "The Gamma Posterior Model\n\nLooking at just the prior and the data distributions,\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe prior expects more spam calls than what we observed."
  },
  {
    "objectID": "files/lectures/06-1-conjugate-families.html#the-gamma-posterior-model-1",
    "href": "files/lectures/06-1-conjugate-families.html#the-gamma-posterior-model-1",
    "title": "Conjugate Families",
    "section": "The Gamma Posterior Model",
    "text": "The Gamma Posterior Model\n\nNow including the posterior,\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe shape of the posterior has a Gamma(s, r) model.\n\nThe shape and rate parameters (s and r) have been updated."
  },
  {
    "objectID": "files/lectures/06-1-conjugate-families.html#the-gamma-posterior-model-2",
    "href": "files/lectures/06-1-conjugate-families.html#the-gamma-posterior-model-2",
    "title": "Conjugate Families",
    "section": "The Gamma Posterior Model",
    "text": "The Gamma Posterior Model\n\nThe plot_gamma_poisson() function:\n\n\nplot_gamma_poisson(shape = prior_s, rate = prior_r, \n                   sum_y = sum_of_obs, n = sample_size, \n                   posterior = TRUE or FALSE) + \n  theme_bw()"
  },
  {
    "objectID": "files/lectures/06-1-conjugate-families.html#the-gamma-posterior-model-3",
    "href": "files/lectures/06-1-conjugate-families.html#the-gamma-posterior-model-3",
    "title": "Conjugate Families",
    "section": "The Gamma Posterior Model",
    "text": "The Gamma Posterior Model\n\nYour turn! What is different if we had used one of the other priors?\nRecall, we considered\n\nGamma(5, 1)\nGamma(10, 2)\nGamma(15, 3)\nGamma(20, 4)"
  },
  {
    "objectID": "files/lectures/06-1-conjugate-families.html#the-gamma-posterior-model-4",
    "href": "files/lectures/06-1-conjugate-families.html#the-gamma-posterior-model-4",
    "title": "Conjugate Families",
    "section": "The Gamma Posterior Model",
    "text": "The Gamma Posterior Model\n\nYour turn! What is different if we had used Gamma(15, 3) as our prior?"
  },
  {
    "objectID": "files/lectures/06-1-conjugate-families.html#the-gamma-posterior-model-5",
    "href": "files/lectures/06-1-conjugate-families.html#the-gamma-posterior-model-5",
    "title": "Conjugate Families",
    "section": "The Gamma Posterior Model",
    "text": "The Gamma Posterior Model\n\nWe can use the summarize_gamma_poisson() function to summarize the distribution,\n\n\nsummarize_gamma_poisson(shape = 10, rate = 2, sum_y = 11, n = 4)"
  },
  {
    "objectID": "files/lectures/06-1-conjugate-families.html#wrap-up-gamma-poisson-model",
    "href": "files/lectures/06-1-conjugate-families.html#wrap-up-gamma-poisson-model",
    "title": "Conjugate Families",
    "section": "Wrap Up: Gamma-Poisson Model",
    "text": "Wrap Up: Gamma-Poisson Model\n\nWe have built the Gamma-Poisson model for \\lambda, an unknown rate.\n\n\n\\begin{equation*}\n\\begin{aligned}\nY|\\lambda &\\sim \\text{Poi}(\\lambda) \\\\\n\\lambda &\\sim \\text{Gamma}(s, r) &\n\\end{aligned}\n\\Rightarrow\n\\begin{aligned}\n&& \\lambda | y &\\sim \\text{Gamma}(s + \\sum y_i, r + n) \\\\\n\\end{aligned}\n\\end{equation*}\n\n\nThe prior model, f(\\lambda), is given by Gamma(s, r).\nThe data model, f(Y|\\lambda), is given by Poi(\\lambda).\nThe posterior model is a Gamma distribution with updated parameters s+\\sum y_i and r + n."
  },
  {
    "objectID": "files/lectures/06-1-conjugate-families.html#introduction-normal-normal-model",
    "href": "files/lectures/06-1-conjugate-families.html#introduction-normal-normal-model",
    "title": "Conjugate Families",
    "section": "Introduction: Normal-Normal Model",
    "text": "Introduction: Normal-Normal Model\n\nSo far, we have learned two conjugate families:\n\nBeta-Binomial (binary outcomes)\n\ny \\sim \\text{Bin}(n, \\pi) (data distribution)\n\\pi \\sim \\text{Beta}(\\alpha, \\beta) (prior distribution)\n\\pi|y \\sim \\text{Beta}(\\alpha+y, \\beta+n-y) (posterior distribution)\n\nGamma-Poisson (count outcomes)\n\nY_i | \\lambda \\overset{ind}\\sim \\text{Pois}(\\lambda) (data distribution)\n\\lambda \\sim \\text{Gamma}(s, r) (prior distribution)\n\\lambda | \\overset{\\to}y \\sim \\text{Gamma}\\left( s + \\sum y_i, r + n \\right) (posterior distribution)\n\n\nNow, we will learn about another conjugate family, the Normal-Normal, for continuous outcomes."
  },
  {
    "objectID": "files/lectures/06-1-conjugate-families.html#example-set-up-5",
    "href": "files/lectures/06-1-conjugate-families.html#example-set-up-5",
    "title": "Conjugate Families",
    "section": "Example Set Up",
    "text": "Example Set Up\n\nAs scientists learn more about brain health, the dangers of concussions are gaining greater attention.\nWe are interested in \\mu, the average volume (cm3) of a specific part of the brain: the hippocampus.\nWikipedia tells us that among the general population of human adults, each half of the hippocampus has volume between 3.0 and 3.5 cm3.\n\nTotal hippocampal volume of both sides of the brain is between 6 and 7 cm3.\nLet’s assume that the mean hippocampal volume among people with a history of concussions is also somewhere between 6 and 7 cm3.\n\nWe will take a sample of n=25 participants and update our belief."
  },
  {
    "objectID": "files/lectures/06-1-conjugate-families.html#the-normal-model",
    "href": "files/lectures/06-1-conjugate-families.html#the-normal-model",
    "title": "Conjugate Families",
    "section": "The Normal Model",
    "text": "The Normal Model\n\nLet Y \\in \\mathbb{R} be a continuous random variable.\n\nThe variability in Y may be represented with a Normal model with mean parameter \\mu \\in \\mathbb{R} and standard deviation parameter \\sigma &gt; 0.\n\nThe Normal model’s pdf is as follows,\n\nf(y) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp \\left\\{ \\frac{-(y-\\mu)^2}{2\\sigma^2} \\right\\}"
  },
  {
    "objectID": "files/lectures/06-1-conjugate-families.html#the-normal-model-1",
    "href": "files/lectures/06-1-conjugate-families.html#the-normal-model-1",
    "title": "Conjugate Families",
    "section": "The Normal Model",
    "text": "The Normal Model\n\nIf we vary \\mu,"
  },
  {
    "objectID": "files/lectures/06-1-conjugate-families.html#the-normal-model-2",
    "href": "files/lectures/06-1-conjugate-families.html#the-normal-model-2",
    "title": "Conjugate Families",
    "section": "The Normal Model",
    "text": "The Normal Model\n\nIf we vary \\sigma,"
  },
  {
    "objectID": "files/lectures/06-1-conjugate-families.html#the-normal-model-3",
    "href": "files/lectures/06-1-conjugate-families.html#the-normal-model-3",
    "title": "Conjugate Families",
    "section": "The Normal Model",
    "text": "The Normal Model\n\nOur data model is as follows,\n\nY_i | \\mu \\sim N(\\mu, \\sigma^2)\n\nThe joint pdf is as follows,\n\n\nf(\\overset{\\to}y | \\mu) = \\prod_{i=1}^n f(y_i | \\mu) = \\prod_{i=1}^n \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp \\left\\{ \\frac{-(y_i-\\mu)^2}{2\\sigma^2} \\right\\}\n\n\nMeaning the likelihood is as follows,\n\n\nL(\\mu|\\overset{\\to}y) \\propto \\prod_{i=1}^n \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp \\left\\{ \\frac{-(y_i-\\mu)^2}{2\\sigma^2} \\right\\} = \\exp \\left\\{ \\frac{- \\sum_{i=1}^n(y_i-\\mu)^2}{2\\sigma^2} \\right\\}"
  },
  {
    "objectID": "files/lectures/06-1-conjugate-families.html#the-normal-model-4",
    "href": "files/lectures/06-1-conjugate-families.html#the-normal-model-4",
    "title": "Conjugate Families",
    "section": "The Normal Model",
    "text": "The Normal Model\n\nOur data model is as follows,\n\nY_i | \\mu \\sim N(\\mu, \\sigma^2)\n\nReturning to our brain analysis, we will assume that the hippocampal volumes of our n = 25 subjects have a normal distribution with mean \\mu and standard deviation \\sigma.\n\nRight now, we are only interested in \\mu, so we assume \\sigma = 0.5 cm3\nThis choice suggests that most people have hippocampal volumes within 2 \\sigma = 1 cm3."
  },
  {
    "objectID": "files/lectures/06-1-conjugate-families.html#normal-prior",
    "href": "files/lectures/06-1-conjugate-families.html#normal-prior",
    "title": "Conjugate Families",
    "section": "Normal Prior",
    "text": "Normal Prior\n\nWe know that with Y_i | \\mu \\sim N(\\mu, \\sigma^2), \\mu \\in \\mathbb{R}.\n\nWe think a normal prior for \\mu is reasonable.\n\nThus, we assume that \\mu has a normal distribution around some mean, \\theta, with standard deviation, \\tau.\n\n\\mu \\sim N(\\theta, \\tau^2),\n\nmeaning that \\mu has prior pdf\n\nf(\\mu) = \\frac{1}{\\sqrt{2 \\pi \\tau^2}} \\exp \\left\\{ \\frac{-(\\mu - \\theta)^2}{2 \\tau^2} \\right\\}"
  },
  {
    "objectID": "files/lectures/06-1-conjugate-families.html#tuning-the-normal-prior",
    "href": "files/lectures/06-1-conjugate-families.html#tuning-the-normal-prior",
    "title": "Conjugate Families",
    "section": "Tuning the Normal Prior",
    "text": "Tuning the Normal Prior\n\nWe can tune the hyperparameters \\theta and \\tau to reflect our understanding and uncertainty about the average hippocampal volume (\\mu) among people with a history of concussions.\nWikipedia showed us that hippocampal volumes tend to be between 6 and 7 cm3 \\to \\theta=6.5.\nWhen we set the standard deviation we can check the plausible range of values of \\mu:\n\nFollow up: why 2?\n\n\n\\theta \\pm 2 \\times \\tau\n\nIf we assume \\tau=0.4,\n\n(6.5 \\pm 2 \\times 0.4) = (5.7, 7.3)"
  },
  {
    "objectID": "files/lectures/06-1-conjugate-families.html#tuning-the-normal-prior-1",
    "href": "files/lectures/06-1-conjugate-families.html#tuning-the-normal-prior-1",
    "title": "Conjugate Families",
    "section": "Tuning the Normal Prior",
    "text": "Tuning the Normal Prior\n\nThus, our tuned prior is \\mu \\sim N(6.5, 0.4^2)\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis range incorporates our uncertainty - it is wider than the Wikipedia range."
  },
  {
    "objectID": "files/lectures/06-1-conjugate-families.html#normal-normal-conjugacy",
    "href": "files/lectures/06-1-conjugate-families.html#normal-normal-conjugacy",
    "title": "Conjugate Families",
    "section": "Normal-Normal Conjugacy",
    "text": "Normal-Normal Conjugacy\n\nLet \\mu \\in \\mathbb{R} be an unknown mean parameter and (Y_1, Y_2, ..., Y_n) be an independent N(\\mu, \\sigma^2) sample where \\sigma is assumed to be known.\nThe Normal-Normal Bayesian model is as follows:\n\n\n\\begin{align*}\nY_i | \\mu &\\overset{\\text{iid}} \\sim N(\\mu, \\sigma^2) \\\\\n\\mu &\\sim N(\\theta, \\tau^2) \\\\\n\\mu | \\overset{\\to}y &\\sim N\\left( \\theta \\frac{\\sigma^2}{n\\tau^2 + \\sigma^2} + \\bar{y} \\frac{n\\tau^2}{n\\tau^2 + \\sigma^2}, \\frac{\\tau^2 \\sigma^2}{n \\tau^2 + \\sigma^2} \\right)\n\\end{align*}"
  },
  {
    "objectID": "files/lectures/06-1-conjugate-families.html#normal-normal-conjugacy-1",
    "href": "files/lectures/06-1-conjugate-families.html#normal-normal-conjugacy-1",
    "title": "Conjugate Families",
    "section": "Normal-Normal Conjugacy",
    "text": "Normal-Normal Conjugacy\n\nLet’s think about our posterior and some implications,\n\n\\mu | \\overset{\\to}y \\sim N\\left( \\theta \\frac{\\sigma^2}{n\\tau^2 + \\sigma^2} + \\bar{y} \\frac{n\\tau^2}{n\\tau^2 + \\sigma^2}, \\frac{\\tau^2 \\sigma^2}{n \\tau^2 + \\sigma^2} \\right)\n\nWhat happens as n increases?"
  },
  {
    "objectID": "files/lectures/06-1-conjugate-families.html#normal-normal-conjugacy-2",
    "href": "files/lectures/06-1-conjugate-families.html#normal-normal-conjugacy-2",
    "title": "Conjugate Families",
    "section": "Normal-Normal Conjugacy",
    "text": "Normal-Normal Conjugacy\n\nLet’s think about our posterior and some implications,\n\n\\mu | \\overset{\\to}y \\sim N\\left( \\theta \\frac{\\sigma^2}{n\\tau^2 + \\sigma^2} + \\bar{y} \\frac{n\\tau^2}{n\\tau^2 + \\sigma^2}, \\frac{\\tau^2 \\sigma^2}{n \\tau^2 + \\sigma^2} \\right)\n\nWhat happens as n increases?\n\n\n\\begin{align*}\n\\frac{\\sigma^2}{n\\tau^2 + \\sigma^2} &\\to 0 \\\\\n\\frac{n\\tau^2}{n\\tau^2 + \\sigma^2} &\\to 1 \\\\\n\\frac{\\tau^2 \\sigma^2}{n \\tau^2 + \\sigma^2} &\\to 0\n\\end{align*}"
  },
  {
    "objectID": "files/lectures/06-1-conjugate-families.html#normal-normal-conjugacy-3",
    "href": "files/lectures/06-1-conjugate-families.html#normal-normal-conjugacy-3",
    "title": "Conjugate Families",
    "section": "Normal-Normal Conjugacy",
    "text": "Normal-Normal Conjugacy\n\nLet’s think about our posterior and some implications,\n\n\\mu | \\overset{\\to}y \\sim N\\left( \\theta \\frac{\\sigma^2}{n\\tau^2 + \\sigma^2} + \\bar{y} \\frac{n\\tau^2}{n\\tau^2 + \\sigma^2}, \\frac{\\tau^2 \\sigma^2}{n \\tau^2 + \\sigma^2} \\right)\n\n\\begin{align*}\n\\frac{\\sigma^2}{n\\tau^2 + \\sigma^2} &\\to 0 \\\\\n\\frac{n\\tau^2}{n\\tau^2 + \\sigma^2} &\\to 1 \\\\\n\\frac{\\tau^2 \\sigma^2}{n \\tau^2 + \\sigma^2} &\\to 0\n\\end{align*}\n\n\nThe posterior mean places less weight on the prior mean and more weight on the sample mean \\bar{y}.\nThe posterior certainty about \\mu increases and becomes more in sync with the data."
  },
  {
    "objectID": "files/lectures/06-1-conjugate-families.html#the-normal-posterior-model",
    "href": "files/lectures/06-1-conjugate-families.html#the-normal-posterior-model",
    "title": "Conjugate Families",
    "section": "The Normal Posterior Model",
    "text": "The Normal Posterior Model\n\nLet us now apply this to our example.\nWe have our prior model, \\mu \\sim N(6.5, 0.4^2).\nLet’s look at the football dataset in the bayesrules package.\n\n\ndata(football)\nconcussion_subjects &lt;- football %&gt;% \n  filter(group == \"fb_concuss\")\n\n\nWhat is the average hippocampal volume?"
  },
  {
    "objectID": "files/lectures/06-1-conjugate-families.html#the-normal-posterior-model-1",
    "href": "files/lectures/06-1-conjugate-families.html#the-normal-posterior-model-1",
    "title": "Conjugate Families",
    "section": "The Normal Posterior Model",
    "text": "The Normal Posterior Model\n\nLet us now apply this to our example.\nWe have our prior model, \\mu \\sim N(6.5, 0.4^2).\nLet’s look at the football dataset in the bayesrules package.\n\n\ndata(football)\nconcussion_subjects &lt;- football %&gt;% \n  filter(group == \"fb_concuss\")\n\n\nWhat is the average hippocampal volume?\n\n\nmean(concussion_subjects$volume)\n\n[1] 5.7346"
  },
  {
    "objectID": "files/lectures/06-1-conjugate-families.html#the-normal-posterior-model-2",
    "href": "files/lectures/06-1-conjugate-families.html#the-normal-posterior-model-2",
    "title": "Conjugate Families",
    "section": "The Normal Posterior Model",
    "text": "The Normal Posterior Model\n\nWe can also plot the density!\n\n\n\nconcussion_subjects %&gt;% ggplot(aes(x = volume)) + geom_density() + theme_bw()"
  },
  {
    "objectID": "files/lectures/06-1-conjugate-families.html#the-normal-posterior-model-3",
    "href": "files/lectures/06-1-conjugate-families.html#the-normal-posterior-model-3",
    "title": "Conjugate Families",
    "section": "The Normal Posterior Model",
    "text": "The Normal Posterior Model\n\nNow, we can plug in the information we have (n = 25, \\bar{y} = 5.735, \\sigma = 0.5) into our likelihood,\n\n\nL(\\mu|\\overset{\\to}y) \\propto \\exp \\left\\{ \\frac{-(5.735 - \\mu)^2}{2(0.5^2/25)} \\right\\}"
  },
  {
    "objectID": "files/lectures/06-1-conjugate-families.html#the-normal-posterior-model-4",
    "href": "files/lectures/06-1-conjugate-families.html#the-normal-posterior-model-4",
    "title": "Conjugate Families",
    "section": "The Normal Posterior Model",
    "text": "The Normal Posterior Model\n\nWe are now ready to put together our posterior:\n\nData distribution, Y_i | \\mu \\overset{\\text{iid}} \\sim N(\\mu, \\sigma^2)\nPrior distribution, \\mu \\sim N(\\theta, \\tau^2)\n\nPosterior distribution, \\mu | \\overset{\\to}y \\sim N\\left( \\theta \\frac{\\sigma^2}{n\\tau^2 + \\sigma^2} + \\bar{y} \\frac{n\\tau^2}{n\\tau^2 + \\sigma^2}, \\frac{\\tau^2 \\sigma^2}{n \\tau^2 + \\sigma^2} \\right)\n\nGiven our information (\\theta=6.5, \\tau=0.4, n=25, \\bar{y}=5.735, \\sigma=0.5), our posterior is\n\n\n\\mu | \\overset{\\to}y \\sim N\\left( \\theta \\frac{\\sigma^2}{n\\tau^2 + \\sigma^2} + \\bar{y} \\frac{n\\tau^2}{n\\tau^2 + \\sigma^2}, \\frac{\\tau^2 \\sigma^2}{n \\tau^2 + \\sigma^2} \\right)"
  },
  {
    "objectID": "files/lectures/06-1-conjugate-families.html#the-normal-posterior-model-5",
    "href": "files/lectures/06-1-conjugate-families.html#the-normal-posterior-model-5",
    "title": "Conjugate Families",
    "section": "The Normal Posterior Model",
    "text": "The Normal Posterior Model\n\nGiven our information (\\theta=6.5, \\tau=0.4, n=25, \\bar{y}=5.735, \\sigma=0.5), our posterior is\n\n\n\\begin{align*}\n\\mu | \\overset{\\to}y &\\sim N\\left( \\theta \\frac{\\sigma^2}{n\\tau^2 + \\sigma^2} + \\bar{y} \\frac{n\\tau^2}{n\\tau^2 + \\sigma^2}, \\frac{\\tau^2 \\sigma^2}{n \\tau^2 + \\sigma^2} \\right) \\\\\n&\\sim N\\left( 6.5 \\frac{0.5^2}{25 \\cdot 0.4^2 + 0.5^2} + 5.735 \\frac{25 \\cdot 0.4^2}{25 \\cdot 0.4^2 + 0.5^2}, \\frac{0.4^2 \\cdot 0.5^2}{25 \\cdot 0.4^2 + 0.5^2} \\right) \\\\\n&\\sim N(6.5 \\cdot 0.0588 + 5.737 \\cdot 0.9412, 0.09^2) \\\\\n&\\sim N(5.78, 0.09^2)\n\\end{align*}\n\n\nLooking at the posterior, we can see the weights\n\n95% on the data mean, 6% on the prior mean."
  },
  {
    "objectID": "files/lectures/06-1-conjugate-families.html#the-normal-posterior-model-6",
    "href": "files/lectures/06-1-conjugate-families.html#the-normal-posterior-model-6",
    "title": "Conjugate Families",
    "section": "The Normal Posterior Model",
    "text": "The Normal Posterior Model\n\nLooking at just the prior and data distributions,"
  },
  {
    "objectID": "files/lectures/06-1-conjugate-families.html#the-normal-posterior-model-7",
    "href": "files/lectures/06-1-conjugate-families.html#the-normal-posterior-model-7",
    "title": "Conjugate Families",
    "section": "The Normal Posterior Model",
    "text": "The Normal Posterior Model\n\nNow including the posterior,"
  },
  {
    "objectID": "files/lectures/06-1-conjugate-families.html#the-normal-posterior-model-8",
    "href": "files/lectures/06-1-conjugate-families.html#the-normal-posterior-model-8",
    "title": "Conjugate Families",
    "section": "The Normal Posterior Model",
    "text": "The Normal Posterior Model\n\nWe can use the summarize_normal_normal() function to summarize the distribution,\n\n\nsummarize_normal_normal(mean = 6.5, sd = 0.4, sigma = 0.5, y_bar = 5.735, n = 25)"
  },
  {
    "objectID": "files/lectures/06-1-conjugate-families.html#wrap-up-normal-normal-model",
    "href": "files/lectures/06-1-conjugate-families.html#wrap-up-normal-normal-model",
    "title": "Conjugate Families",
    "section": "Wrap Up: Normal-Normal Model",
    "text": "Wrap Up: Normal-Normal Model\n\nWe have built the Normal-Normal model for \\mu, an unknown mean.\n\n\n\\begin{equation*}\n\\begin{aligned}\nY_i | \\mu &\\overset{\\text{iid}} \\sim N(\\mu, \\sigma^2) \\\\\n\\mu &\\sim N(\\theta, \\tau^2) &\n\\end{aligned}\n\\Rightarrow\n\\begin{aligned}\n&& \\mu | \\overset{\\to}y &\\sim N\\left( \\theta \\frac{\\sigma^2}{n\\tau^2 + \\sigma^2} + \\bar{y} \\frac{n\\tau^2}{n\\tau^2 + \\sigma^2}, \\frac{\\tau^2 \\sigma^2}{n \\tau^2 + \\sigma^2} \\right) \\\\\n\\end{aligned}\n\\end{equation*}\n\n\nThe prior model, f(\\mu), is given by N(\\theta,\\tau^2).\nThe data model, f(Y|\\mu), is given by N(\\mu, \\sigma^2).\nThe posterior model is a Normal distribution with updated parameters\n\nmean = \\theta \\frac{\\sigma^2}{n\\tau^2 + \\sigma^2} + \\bar{y} \\frac{n\\tau^2}{n\\tau^2 + \\sigma^2}\nvariance = \\frac{\\tau^2 \\sigma^2}{n \\tau^2 + \\sigma^2}"
  },
  {
    "objectID": "files/lectures/06-1-conjugate-families.html#wrap-up",
    "href": "files/lectures/06-1-conjugate-families.html#wrap-up",
    "title": "Conjugate Families",
    "section": "Wrap Up",
    "text": "Wrap Up\n\nToday we have learned about the conjugate families.\n\nBeta-Binomial: binary outcomes\nGamma-Poisson: count outcomes\nNormal-Normal: continuous outcomes\n\nWhile we are not forced to analyze our data using conjugate families, our lives are much easier when we can use the known relationships.\nNote that while we did not do this with our posteriors in this lecture, we can now move forward with drawing conclusions about the posterior distribution.\n\nProbabilities\nInference"
  },
  {
    "objectID": "files/lectures/06-1-conjugate-families.html#homework",
    "href": "files/lectures/06-1-conjugate-families.html#homework",
    "title": "Conjugate Families",
    "section": "Homework",
    "text": "Homework\n\n3.3\n3.9\n3.10\n3.18\n5.3\n5.5\n5.6\n5.9\n5.10"
  },
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "Abbreviated Syllabus",
    "section": "",
    "text": "Note: this is an abbreviated syllabus. The full syllabus is available through Classmate and/or Canvas.\n\nInstructor Information\n\nDr. Samantha Seals\nOffice: 4/344\n\n\n\nMeeting Times and Location\n\nMW 4:00 pm–5:15 pm\nPhysical classroom: 4/406\nZoom classroom: see Canvas or full syllabus for link\n\n\n\nOffice Hours\n\nMonday: 9:00 am-11:00 am\nTuesday: 2:00 pm-4:00 pm\nWednesday: 9:00 am-11:00 am\nOther times by appointment\n\n\n\nGrading and Evaluation\nThe course grade will be determined as follows:\n\nAssignments (25%): Every module will finish with an assignment to demonstrate the knowledge gained. TThe resulting .html file should be submitted to the designated Assignment on Canvas by 11:59 pm on the specified date (see Canvas).\nProject (50%): Students in this course will be collaborating with students in GEO6118 - Research Design. The final research project will be submitted along with smaller pieces throughout the semester.\nFinal Exam (25%): The final exam will be a take home final.\n\n\n\nLate Policy\nAssigments have due dates, however, the dropboxes will not close until the end of the semester. All students are automatically granted “extensions” without question.\nNote that if there is not a submission when I go to grade (after the initial deadline), I will assign a zero (0) and request that you submit the assignment when you are able to. This is only for record keeping purposes. There is no penalty for submitting late and a full grade will be given upon review of your submission.\nExtensions are not available for project pieces or the final exam."
  }
]