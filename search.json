[
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Tentative Schedule",
    "section": "",
    "text": "Week 1: August 25-29, 2025\n\n\n\n\n\n\nDr. Seals out sick :(\n\n\n\n\n\n\n\n\n\n\nWeek 2: September 1-5, 2025\n\n\n\n\n\n\nMeeting 1:\n\nLabor Day holiday - campus closed.\n\nMeeting 2:\n\nTopic(s):\n\nVery Brief Review of Probability\n\nSlides:\n\n.html: view slides here\n.qmd: see underlying code here\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 3: September 8-12, 2025\n\n\n\n\n\n\nMeeting 1:\n\nSchedule a meeting with your EES collaborator.\nWorksheet: see Canvas.\n\nMeeting 2:\n\nTopic(s):\n\nDiscrete distributions\n\nSlides:\n\n.html: view slides here\n.qmd: see underlying code here\n\n\nAssignment 1: Probability Distributions - due September 16\n\n.html: view assignment here\n.qmd: see underlying code here\n\n\n\n\n\n\n\n\n\n\n\nWeek 4: September 15-19, 2025\n\n\n\n\n\n\nMeeting 1:\n\nTopic(s):\n\nContinuous distributions\n\nSlides:\n\n.html: view slides here\n.qmd: see underlying code here\n\n\nAssignment 1: Probability Distributions - due September 21\n\n.html: view assignment here\n.qmd: see underlying code here\n\nMeeting 2:\n\nTopic(s):\n\nThinking like a Bayesian\nBayes’ Rule/Theorem\n\nSlides:\n\n.html: view slides here\n.qmd: see underlying code here\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 5: September 22-26, 2025\n\n\n\n\n\n\nMeeting 1:\n\nTopic(s):\n\nBeta-Binomial\n\nSlides:\n\n.html: view slides here\n.qmd: see underlying code here\n\n\nMeeting 2:\n\nTopic(s):\n\nBalance and Sequentiality\n\nSlides:\n\n.html: view slides here\n.qmd: see underlying code here\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 6: September 29-October 3, 2025\n\n\n\n\n\n\nMeeting 1:\n\nTopic(s):\n\nGamma-Poisson\n\nSlides:\n\n.html: view slides here\n.qmd: see underlying code here\n\n\nMeeting 2:\n\nTopic(s):\n\nNormal-Normal\n\nSlides:\n\n.html: view slides here\n.qmd: see underlying code here\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 7: October 6-10, 2025\n\n\n\n\n\n\nMeeting 1:\n\nTopic(s):\n\nApproximating the posterior\nMCMC\n\nSlides:\n\n.html: view slides here\n.qmd: see underlying code here\n\n\nMeeting 2:\n\nTopic(s):\n\nPosterior inference\nPosterior prediction\n\nSlides:\n\n.html: view slides here\n.qmd: see underlying code here\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 8: October 13-17, 2025\n\n\n\n\n\n\nMeeting 1:\n\nColumbus Day holiday - campus closed.\n\nAssignment 2: Basic Bayesian Analysis\n\n.html: view assignment here\n.qmd: see underlying code here\n\n\n\n\n\n\n\n\n\n\n\nWeek 9: October 20-24, 2025\n\n\n\n\n\n\nMeeting 1:\n\nTopic(s):\n\nRegression under the normal distribution\n\nSlides:\n\n.html: view slides here\n.qmd: see underlying code here\n\n\nMeeting 2:\n\nTopic(s):\n\nEvaluating the model\n\nSlides:\n\n.html: view slides here\n.qmd: see underlying code here\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 10: October 27-31, 2025\n\n\n\n\n\n\nMeeting 1:\n\nTopic(s):\n\nLogistic regression\n\nSlides:\n\n.html: view slides here\n.qmd: see underlying code here\n\n\nMeeting 2:\n\nTopic(s):\n\nPoisson and negative binomial regressions\nHow to “read” research papers outside of statistics\n\nSlides:\n\n.html: view slides here\n.qmd: see underlying code here\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 11: November 3-7, 2025\n\n\n\n\n\n\nMeeting 1:\n\nAssignment 3: Regression Analysis\n\n.html: view assignment here\n\n.qmd: see underlying code here\n\nCheck in with project progress\n\nMeeting 2:\n\nMeet with your EES collaborator on your own to discuss methodology.\nYour annotations will be completed, which will help guide the discussion.\nYou will complete a discussion worksheet\n\n\n\n\n\n\n\n\n\n\n\nWeek 12: November 10-14, 2025\n\n\n\n\n\n\nMeeting 1:\n\nWork on analysis\n\nMeeting 2:\n\nWork on analysis\nProject: Analysis worksheet/check in\n\nNote: Tables and graphs are due next week.\n\n\n\n\n\n\n\n\n\n\nWeek 13: November 17-21, 2025\n\n\n\n\n\n\nMeeting 1:\n\nTopic(s):\n\nGeneral guidelines for writing about statistical methodology.\n\nEthical considerations.\n\nGeneral guidelines for providing results.\n\nTables.\nVisualizations.\nWritten summaries.\n\n\nSlides:\n\n.html: view slides here\n.qmd: see underlying code here\n\nProject: Methods paragraph\nProject: Tables\nProject: Graphs\n\nMeeting 2:\n\n3 minute presentation of your methodology/results\n\n\n\n\n\n\n\n\n\n\n\nWeek 14: November 24-28, 2025\n\n\n\n\n\n\nMeeting 1:\n\nThanksgiving holiday - campus closed.\n\nMeeting 2:\n\nThanksgiving holiday - campus closed.\n\n\n\n\n\n\n\n\n\n\n\nWeek 15: December 1-5, 2025\n\n\n\n\n\n\nMeeting 1:\n\nNo meeting; meet with your EES collaborator on your own to discuss results.\nProject 5: Worksheet for discussion\nTake home final opens.\n\nMeeting 2:\n\nNo meeting.\nFinal draft of research due.\n\n\n\n\n\n\n\n\n\n\n\nWeek 16: Exam Week!\n\n\n\n\n\n\nFinal exam due this week."
  },
  {
    "objectID": "files/lectures/06-1-gamma-poisson.html#introduction-gamma-poisson-model",
    "href": "files/lectures/06-1-gamma-poisson.html#introduction-gamma-poisson-model",
    "title": "Gamma-Poisson Model",
    "section": "Introduction: Gamma-Poisson Model",
    "text": "Introduction: Gamma-Poisson Model\n\nRecall the Beta-Binomial model,\n\ny \\sim \\text{Bin}(n, \\pi) (data distribution)\n\\pi \\sim \\text{Beta}(\\alpha, \\beta) (prior distribution)\n\\pi|y \\sim \\text{Beta}(\\alpha+y, \\beta+n-y) (posterior distribution)\n\nThe Beta-Binomial model is from a conjugate family (i.e., the posterior is from the same model family as the prior).\nNow, we will learn about the Gamma-Poisson, another conjugate family."
  },
  {
    "objectID": "files/lectures/06-1-gamma-poisson.html#example-set-up",
    "href": "files/lectures/06-1-gamma-poisson.html#example-set-up",
    "title": "Gamma-Poisson Model",
    "section": "Example Set Up",
    "text": "Example Set Up\n\nSuppose we are now interested in modeling the number of spam calls we receive.\n\nThis means that we are modeling the rate, \\lambda.\n\nWe take a guess and say that the value of \\lambda that is most likely is around 5,\n\n… but reasonably ranges between 2 and 7 calls per day.\n\nWhy can’t we use the Beta distribution as our prior distribution?\n\n\\lambda is the mean of a count \\to \\lambda \\in \\mathbb{R}^+ \\to \\lambda is not limited to [0, 1] \\to broken assumption for Beta distribution.\n\nWhy can’t we use the binomial distribution as our data distribution?\n\nY_i is a count \\to Y_i \\in \\mathbb{N}^+ \\to Y_i is not limited to \\{0, 1\\} \\to broken assumption for Binomial distribution."
  },
  {
    "objectID": "files/lectures/06-1-gamma-poisson.html#poisson-data-model",
    "href": "files/lectures/06-1-gamma-poisson.html#poisson-data-model",
    "title": "Gamma-Poisson Model",
    "section": "Poisson Data Model",
    "text": "Poisson Data Model\n\nWe will use the Poisson distribution to model the number of spam calls\n\nY \\in \\{0, 1, 2, ...\\}\n\nY is the number of independent events that occur in a fixed amount of time or space.\n\\lambda &gt; 0 is the rate at which these events occur.\nMathematically,\n\n Y | \\lambda \\sim \\text{Pois}(\\lambda),\n\nwith pmf,\n\nf(y|\\lambda) = \\frac{\\lambda^y e^{-\\lambda}}{y!}, \\ \\ \\ y \\in \\{0,1, 2, ... \\}"
  },
  {
    "objectID": "files/lectures/06-1-gamma-poisson.html#gamma-prior",
    "href": "files/lectures/06-1-gamma-poisson.html#gamma-prior",
    "title": "Gamma-Poisson Model",
    "section": "Gamma Prior",
    "text": "Gamma Prior\n\nIf \\lambda is a continuous random variable that can take on any positive value (\\lambda &gt; 0), then the variability may be modeled with the Gamma distribution with\n\nshape hyperparameter s&gt;0\nrate hyperparameter r&gt;0.\n\nThus,\n\n\\lambda \\sim \\text{Gamma}(s, r)\n\nand the Gamma pdf is given by\n\nf(\\lambda) = \\frac{r^s}{\\Gamma(s)} \\lambda^{s-1} e^{-r\\lambda}"
  },
  {
    "objectID": "files/lectures/06-1-gamma-poisson.html#gamma-prior-shapes",
    "href": "files/lectures/06-1-gamma-poisson.html#gamma-prior-shapes",
    "title": "Gamma-Poisson Model",
    "section": "Gamma Prior: Shapes",
    "text": "Gamma Prior: Shapes\n\nLet’s explore the shape of the Gamma:\n\n\n\nplot_gamma(1, 1) + theme_bw() + ggtitle(\"Gamma(1, 1)\")"
  },
  {
    "objectID": "files/lectures/06-1-gamma-poisson.html#gamma-prior-shapes-1",
    "href": "files/lectures/06-1-gamma-poisson.html#gamma-prior-shapes-1",
    "title": "Gamma-Poisson Model",
    "section": "Gamma Prior: Shapes",
    "text": "Gamma Prior: Shapes\n\nLet’s explore the shape of the Gamma:\n\n\n\nplot_gamma(2, 1) + theme_bw() + ggtitle(\"Gamma(2, 1)\")"
  },
  {
    "objectID": "files/lectures/06-1-gamma-poisson.html#gamma-prior-shapes-2",
    "href": "files/lectures/06-1-gamma-poisson.html#gamma-prior-shapes-2",
    "title": "Gamma-Poisson Model",
    "section": "Gamma Prior: Shapes",
    "text": "Gamma Prior: Shapes\n\nLet’s explore the shape of the Gamma:\n\n\n\nplot_gamma(10, 1) + theme_bw() + ggtitle(\"Gamma(10, 1)\")"
  },
  {
    "objectID": "files/lectures/06-1-gamma-poisson.html#gamma-prior-shapes-3",
    "href": "files/lectures/06-1-gamma-poisson.html#gamma-prior-shapes-3",
    "title": "Gamma-Poisson Model",
    "section": "Gamma Prior: Shapes",
    "text": "Gamma Prior: Shapes\n\nLet’s explore the shape of the Gamma:\n\n\n\nplot_gamma(10, 10) + theme_bw() + ggtitle(\"Gamma(10, 10)\")"
  },
  {
    "objectID": "files/lectures/06-1-gamma-poisson.html#gamma-prior-shapes-4",
    "href": "files/lectures/06-1-gamma-poisson.html#gamma-prior-shapes-4",
    "title": "Gamma-Poisson Model",
    "section": "Gamma Prior: Shapes",
    "text": "Gamma Prior: Shapes\n\nWhat happens when we increase the \\alpha parameter?"
  },
  {
    "objectID": "files/lectures/06-1-gamma-poisson.html#gamma-prior-shapes-5",
    "href": "files/lectures/06-1-gamma-poisson.html#gamma-prior-shapes-5",
    "title": "Gamma-Poisson Model",
    "section": "Gamma Prior: Shapes",
    "text": "Gamma Prior: Shapes\n\nWhat happens when we increase the \\beta parameter?"
  },
  {
    "objectID": "files/lectures/06-1-gamma-poisson.html#gamma-prior-shapes-6",
    "href": "files/lectures/06-1-gamma-poisson.html#gamma-prior-shapes-6",
    "title": "Gamma-Poisson Model",
    "section": "Gamma Prior: Shapes",
    "text": "Gamma Prior: Shapes\n\nPutting these on the same scale,"
  },
  {
    "objectID": "files/lectures/06-1-gamma-poisson.html#tuning-the-gamma-prior",
    "href": "files/lectures/06-1-gamma-poisson.html#tuning-the-gamma-prior",
    "title": "Gamma-Poisson Model",
    "section": "Tuning the Gamma Prior",
    "text": "Tuning the Gamma Prior\n\nLet’s now tune our prior.\nWe are assuming \\lambda \\approx 5, somewhere between 2 and 7.\nWe know the mean of the gamma distribution,\n\nE(\\lambda) = \\frac{s}{r} \\approx 5 \\to 5r \\approx s\n\nYour turn! Use the plot_gamma() function to figure out what value of s and r we need."
  },
  {
    "objectID": "files/lectures/06-1-gamma-poisson.html#tuning-the-gamma-prior-1",
    "href": "files/lectures/06-1-gamma-poisson.html#tuning-the-gamma-prior-1",
    "title": "Gamma-Poisson Model",
    "section": "Tuning the Gamma Prior",
    "text": "Tuning the Gamma Prior\n\nLooking at different values:"
  },
  {
    "objectID": "files/lectures/06-1-gamma-poisson.html#poisson-data-model-1",
    "href": "files/lectures/06-1-gamma-poisson.html#poisson-data-model-1",
    "title": "Gamma-Poisson Model",
    "section": "Poisson Data Model",
    "text": "Poisson Data Model\n\nWe will be taking samples from different days.\n\nWe assume that the daily number of calls may differ from day to day. On each day i,\n\n\nY_i|\\lambda \\sim \\text{Pois}(\\lambda)\n\nThis has a unique pmf for each day (i),\n\nf(y_i|\\lambda) = \\frac{\\lambda^{y_i} e^{-\\lambda}}{y_i!}\n\nBut really, we are interested in the joint information in our sample of n observations.\n\nThe joint pmf gives us this information."
  },
  {
    "objectID": "files/lectures/06-1-gamma-poisson.html#poisson-data-model-2",
    "href": "files/lectures/06-1-gamma-poisson.html#poisson-data-model-2",
    "title": "Gamma-Poisson Model",
    "section": "Poisson Data Model",
    "text": "Poisson Data Model\n\nThe joint pmf for the Poisson,\n\n\n\\begin{align*}\nf\\left(\\overset{\\to}{y_i}|\\lambda\\right) &= \\prod_{i=1}^n f(y_i|\\lambda) \\\\\n&= f(y_1|\\lambda) \\times f(y_2|\\lambda) \\times ... \\times f(y_n|\\lambda) \\\\\n&= \\frac{\\lambda^{y_1}e^{-\\lambda}}{y_1!} \\times \\frac{\\lambda^{y_2}e^{-\\lambda}}{y_2!} \\times ... \\times \\frac{\\lambda^{y_n}e^{-\\lambda}}{y_n!} \\\\\n&= \\frac{\\left( \\lambda^{y_1} \\lambda^{y_2} \\cdot \\cdot \\cdot \\ \\lambda^{y_n}  \\right) \\left( e^{-\\lambda} e^{-\\lambda} \\cdot \\cdot \\cdot e^{-\\lambda}\\right)}{y_1! y_2! \\cdot \\cdot \\cdot y_n!} \\\\\n&= \\frac{\\lambda^{\\sum y_i}e^{-n\\lambda}}{\\prod_{i=1}^n y_i !}\n\\end{align*}"
  },
  {
    "objectID": "files/lectures/06-1-gamma-poisson.html#poisson-data-model-3",
    "href": "files/lectures/06-1-gamma-poisson.html#poisson-data-model-3",
    "title": "Gamma-Poisson Model",
    "section": "Poisson Data Model",
    "text": "Poisson Data Model\n\nIf the joint pmf for the Poisson is\n\nf\\left(\\overset{\\to}{y_i}|\\lambda\\right) = \\frac{\\lambda^{\\sum y_i}e^{-n\\lambda}}{\\prod_{i=1}^n y_i !}\n\nthen the likelihood function for \\lambda &gt; 0 is\n\n\n\\begin{align*}\nL\\left(\\lambda|\\overset{\\to}{y_i}\\right) &= \\frac{\\lambda^{\\sum y_i}e^{-n\\lambda}}{\\prod_{i=1}^n y_i !} \\\\\n& \\propto \\lambda^{\\sum y_i} e^{-n\\lambda}\n\\end{align*}\n\n\nPease see page 102 in the textbook for full derivations."
  },
  {
    "objectID": "files/lectures/06-1-gamma-poisson.html#gamma-poisson-conjugacy",
    "href": "files/lectures/06-1-gamma-poisson.html#gamma-poisson-conjugacy",
    "title": "Gamma-Poisson Model",
    "section": "Gamma-Poisson Conjugacy",
    "text": "Gamma-Poisson Conjugacy\n\nLet \\lambda &gt; 0 be an unknown rate parameter and (Y_1, Y_2, ... , Y_n) be an independent sample from the Poisson distribution.\nThe Gamma-Poisson Bayesian model is as follows:\n\n\n\\begin{align*}\nY_i | \\lambda &\\overset{ind}\\sim \\text{Pois}(\\lambda) \\\\\n\\lambda &\\sim \\text{Gamma}(s, r) \\\\\n\\lambda | \\overset{\\to}y &\\sim \\text{Gamma}\\left( s + \\sum y_i, r + n \\right)\n\\end{align*}\n\n\nThe proof can be seen in section 5.2.4 of the textbook."
  },
  {
    "objectID": "files/lectures/06-1-gamma-poisson.html#gamma-poisson-conjugacy-1",
    "href": "files/lectures/06-1-gamma-poisson.html#gamma-poisson-conjugacy-1",
    "title": "Gamma-Poisson Model",
    "section": "Gamma-Poisson Conjugacy",
    "text": "Gamma-Poisson Conjugacy\n\nSuppose we use Gamma(10, 2) as the prior for \\lambda, the daily rate of calls.\nOn four separate days in the second week of August (i.e., independent days), we received \\overset{\\to}y = (6, 2, 2, 1) calls.\nWe will use the plot_poisson_likelihood() function:\n\n\nplot_poisson_likelihood(y = c(6, 2, 2, 1), lambda_upper_bound = 10)\n\n\nNotes:\n\nlambda_upper_bound limits the x axis – recall that \\lambda \\in (0, \\infty)!\nlambda_upper_bound’s default value is 10."
  },
  {
    "objectID": "files/lectures/06-1-gamma-poisson.html#gamma-poisson-conjugacy-2",
    "href": "files/lectures/06-1-gamma-poisson.html#gamma-poisson-conjugacy-2",
    "title": "Gamma-Poisson Model",
    "section": "Gamma-Poisson Conjugacy",
    "text": "Gamma-Poisson Conjugacy\n\n\n\n\n\n\n\n\n\n\n\n\nWe can see that the average is around 2.75.\n\n\nmean(c(6, 2, 2, 1))\n\n[1] 2.75"
  },
  {
    "objectID": "files/lectures/06-1-gamma-poisson.html#gamma-poisson-conjugacy-3",
    "href": "files/lectures/06-1-gamma-poisson.html#gamma-poisson-conjugacy-3",
    "title": "Gamma-Poisson Model",
    "section": "Gamma-Poisson Conjugacy",
    "text": "Gamma-Poisson Conjugacy\n\nWe know our prior distribution is Gamma(10, 2) and the data distribution is Poi(2.75).\nThus, the posterior is as follows,\n\n\n\\begin{align*}\n\\lambda | \\overset{\\to}y &\\sim \\text{Gamma}\\left( s + \\sum y_i, r + n \\right) \\\\\n&\\sim \\text{Gamma}\\left(10 + 11, 2 + 4 \\right) \\\\\n&\\sim \\text{Gamma}\\left(21, 6 \\right)\n\\end{align*}"
  },
  {
    "objectID": "files/lectures/06-1-gamma-poisson.html#the-gamma-posterior-model",
    "href": "files/lectures/06-1-gamma-poisson.html#the-gamma-posterior-model",
    "title": "Gamma-Poisson Model",
    "section": "The Gamma Posterior Model",
    "text": "The Gamma Posterior Model\n\nLooking at just the prior and the data distributions,\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe prior expects more spam calls than what we observed."
  },
  {
    "objectID": "files/lectures/06-1-gamma-poisson.html#the-gamma-posterior-model-1",
    "href": "files/lectures/06-1-gamma-poisson.html#the-gamma-posterior-model-1",
    "title": "Gamma-Poisson Model",
    "section": "The Gamma Posterior Model",
    "text": "The Gamma Posterior Model\n\nNow including the posterior,\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe shape of the posterior has a Gamma(s, r) model.\n\nThe shape and rate parameters (s and r) have been updated."
  },
  {
    "objectID": "files/lectures/06-1-gamma-poisson.html#the-gamma-posterior-model-2",
    "href": "files/lectures/06-1-gamma-poisson.html#the-gamma-posterior-model-2",
    "title": "Gamma-Poisson Model",
    "section": "The Gamma Posterior Model",
    "text": "The Gamma Posterior Model\n\nThe plot_gamma_poisson() function:\n\n\nplot_gamma_poisson(shape = prior_s, rate = prior_r, \n                   sum_y = sum_of_obs, n = sample_size, \n                   posterior = TRUE or FALSE) + \n  theme_bw()"
  },
  {
    "objectID": "files/lectures/06-1-gamma-poisson.html#the-gamma-posterior-model-3",
    "href": "files/lectures/06-1-gamma-poisson.html#the-gamma-posterior-model-3",
    "title": "Gamma-Poisson Model",
    "section": "The Gamma Posterior Model",
    "text": "The Gamma Posterior Model\n\nYour turn! What is different if we had used one of the other priors?\nRecall, we considered\n\nGamma(5, 1)\nGamma(10, 2)\nGamma(15, 3)\nGamma(20, 4)"
  },
  {
    "objectID": "files/lectures/06-1-gamma-poisson.html#the-gamma-posterior-model-4",
    "href": "files/lectures/06-1-gamma-poisson.html#the-gamma-posterior-model-4",
    "title": "Gamma-Poisson Model",
    "section": "The Gamma Posterior Model",
    "text": "The Gamma Posterior Model\n\nYour turn! What is different if we had used Gamma(15, 3) as our prior?"
  },
  {
    "objectID": "files/lectures/06-1-gamma-poisson.html#the-gamma-posterior-model-5",
    "href": "files/lectures/06-1-gamma-poisson.html#the-gamma-posterior-model-5",
    "title": "Gamma-Poisson Model",
    "section": "The Gamma Posterior Model",
    "text": "The Gamma Posterior Model\n\nWe can use the summarize_gamma_poisson() function to summarize the distribution,\n\n\nsummarize_gamma_poisson(shape = 10, rate = 2, sum_y = 11, n = 4)"
  },
  {
    "objectID": "files/lectures/06-1-gamma-poisson.html#wrap-up-gamma-poisson-model",
    "href": "files/lectures/06-1-gamma-poisson.html#wrap-up-gamma-poisson-model",
    "title": "Gamma-Poisson Model",
    "section": "Wrap Up: Gamma-Poisson Model",
    "text": "Wrap Up: Gamma-Poisson Model\n\nWe have built the Gamma-Poisson model for \\lambda, an unknown rate.\n\n\n\\begin{equation*}\n\\begin{aligned}\nY|\\lambda &\\sim \\text{Poi}(\\lambda) \\\\\n\\lambda &\\sim \\text{Gamma}(s, r) &\n\\end{aligned}\n\\Rightarrow\n\\begin{aligned}\n&& \\lambda | y &\\sim \\text{Gamma}(s + \\sum y_i, r + n) \\\\\n\\end{aligned}\n\\end{equation*}\n\n\nThe prior model, f(\\lambda), is given by Gamma(s, r).\nThe data model, f(Y|\\lambda), is given by Poi(\\lambda).\nThe posterior model is a Gamma distribution with updated parameters s+\\sum y_i and r + n."
  },
  {
    "objectID": "files/lectures/06-1-gamma-poisson.html#homework-practice",
    "href": "files/lectures/06-1-gamma-poisson.html#homework-practice",
    "title": "Gamma-Poisson Model",
    "section": "Homework / Practice",
    "text": "Homework / Practice\n\nFrom the Bayes Rules! textbook:\n\n5.3\n5.5\n5.6"
  },
  {
    "objectID": "files/lectures/06-2-normal-normal.html#introduction-normal-normal-model",
    "href": "files/lectures/06-2-normal-normal.html#introduction-normal-normal-model",
    "title": "Normal-Normal Model",
    "section": "Introduction: Normal-Normal Model",
    "text": "Introduction: Normal-Normal Model\n\nBefore today, we have learned two conjugate families:\n\nBeta-Binomial (binary outcomes)\n\ny \\sim \\text{Bin}(n, \\pi) (data distribution)\n\\pi \\sim \\text{Beta}(\\alpha, \\beta) (prior distribution)\n\\pi|y \\sim \\text{Beta}(\\alpha+y, \\beta+n-y) (posterior distribution)\n\nGamma-Poisson (count outcomes)\n\nY_i | \\lambda \\overset{ind}\\sim \\text{Pois}(\\lambda) (data distribution)\n\\lambda \\sim \\text{Gamma}(s, r) (prior distribution)\n\\lambda | \\overset{\\to}y \\sim \\text{Gamma}\\left( s + \\sum y_i, r + n \\right) (posterior distribution)\n\n\nNow, we will learn about another conjugate family, the Normal-Normal, for continuous outcomes."
  },
  {
    "objectID": "files/lectures/06-2-normal-normal.html#example-set-up",
    "href": "files/lectures/06-2-normal-normal.html#example-set-up",
    "title": "Normal-Normal Model",
    "section": "Example Set Up",
    "text": "Example Set Up\n\nAs scientists learn more about brain health, the dangers of concussions are gaining greater attention.\nWe are interested in \\mu, the average volume (cm3) of a specific part of the brain: the hippocampus.\nWikipedia tells us that among the general population of human adults, each half of the hippocampus has volume between 3.0 and 3.5 cm3.\n\nTotal hippocampal volume of both sides of the brain is between 6 and 7 cm3.\nLet’s assume that the mean hippocampal volume among people with a history of concussions is also somewhere between 6 and 7 cm3.\n\nWe will take a sample of n=25 participants and update our belief."
  },
  {
    "objectID": "files/lectures/06-2-normal-normal.html#the-normal-model",
    "href": "files/lectures/06-2-normal-normal.html#the-normal-model",
    "title": "Normal-Normal Model",
    "section": "The Normal Model",
    "text": "The Normal Model\n\nLet Y \\in \\mathbb{R} be a continuous random variable.\n\nThe variability in Y may be represented with a Normal model with mean parameter \\mu \\in \\mathbb{R} and standard deviation parameter \\sigma &gt; 0.\n\nThe Normal model’s pdf is as follows,\n\nf(y) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp \\left\\{ \\frac{-(y-\\mu)^2}{2\\sigma^2} \\right\\}"
  },
  {
    "objectID": "files/lectures/06-2-normal-normal.html#the-normal-model-1",
    "href": "files/lectures/06-2-normal-normal.html#the-normal-model-1",
    "title": "Normal-Normal Model",
    "section": "The Normal Model",
    "text": "The Normal Model\n\nIf we vary \\mu,"
  },
  {
    "objectID": "files/lectures/06-2-normal-normal.html#the-normal-model-2",
    "href": "files/lectures/06-2-normal-normal.html#the-normal-model-2",
    "title": "Normal-Normal Model",
    "section": "The Normal Model",
    "text": "The Normal Model\n\nIf we vary \\sigma,"
  },
  {
    "objectID": "files/lectures/06-2-normal-normal.html#the-normal-model-3",
    "href": "files/lectures/06-2-normal-normal.html#the-normal-model-3",
    "title": "Normal-Normal Model",
    "section": "The Normal Model",
    "text": "The Normal Model\n\nOur data model is as follows,\n\nY_i | \\mu \\sim N(\\mu, \\sigma^2)\n\nThe joint pdf is as follows,\n\n\nf(\\overset{\\to}y | \\mu) = \\prod_{i=1}^n f(y_i | \\mu) = \\prod_{i=1}^n \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp \\left\\{ \\frac{-(y_i-\\mu)^2}{2\\sigma^2} \\right\\}\n\n\nMeaning the likelihood is as follows,\n\n\nL(\\mu|\\overset{\\to}y) \\propto \\prod_{i=1}^n \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp \\left\\{ \\frac{-(y_i-\\mu)^2}{2\\sigma^2} \\right\\} = \\exp \\left\\{ \\frac{- \\sum_{i=1}^n(y_i-\\mu)^2}{2\\sigma^2} \\right\\}"
  },
  {
    "objectID": "files/lectures/06-2-normal-normal.html#the-normal-model-4",
    "href": "files/lectures/06-2-normal-normal.html#the-normal-model-4",
    "title": "Normal-Normal Model",
    "section": "The Normal Model",
    "text": "The Normal Model\n\nOur data model is as follows,\n\nY_i | \\mu \\sim N(\\mu, \\sigma^2)\n\nReturning to our brain analysis, we will assume that the hippocampal volumes of our n = 25 subjects have a normal distribution with mean \\mu and standard deviation \\sigma.\n\nRight now, we are only interested in \\mu, so we assume \\sigma = 0.5 cm3\nThis choice suggests that most people have hippocampal volumes within 2 \\sigma = 1 cm3."
  },
  {
    "objectID": "files/lectures/06-2-normal-normal.html#normal-prior",
    "href": "files/lectures/06-2-normal-normal.html#normal-prior",
    "title": "Normal-Normal Model",
    "section": "Normal Prior",
    "text": "Normal Prior\n\nWe know that with Y_i | \\mu \\sim N(\\mu, \\sigma^2), \\mu \\in \\mathbb{R}.\n\nWe think a normal prior for \\mu is reasonable.\n\nThus, we assume that \\mu has a normal distribution around some mean, \\theta, with standard deviation, \\tau.\n\n\\mu \\sim N(\\theta, \\tau^2),\n\nmeaning that \\mu has prior pdf\n\nf(\\mu) = \\frac{1}{\\sqrt{2 \\pi \\tau^2}} \\exp \\left\\{ \\frac{-(\\mu - \\theta)^2}{2 \\tau^2} \\right\\}"
  },
  {
    "objectID": "files/lectures/06-2-normal-normal.html#tuning-the-normal-prior",
    "href": "files/lectures/06-2-normal-normal.html#tuning-the-normal-prior",
    "title": "Normal-Normal Model",
    "section": "Tuning the Normal Prior",
    "text": "Tuning the Normal Prior\n\nWe can tune the hyperparameters \\theta and \\tau to reflect our understanding and uncertainty about the average hippocampal volume (\\mu) among people with a history of concussions.\nWikipedia showed us that hippocampal volumes tend to be between 6 and 7 cm3 \\to \\theta=6.5.\nWhen we set the standard deviation we can check the plausible range of values of \\mu:\n\nFollow up: why 2?\n\n\n\\theta \\pm 2 \\times \\tau\n\nIf we assume \\tau=0.4,\n\n(6.5 \\pm 2 \\times 0.4) = (5.7, 7.3)"
  },
  {
    "objectID": "files/lectures/06-2-normal-normal.html#tuning-the-normal-prior-1",
    "href": "files/lectures/06-2-normal-normal.html#tuning-the-normal-prior-1",
    "title": "Normal-Normal Model",
    "section": "Tuning the Normal Prior",
    "text": "Tuning the Normal Prior\n\nThus, our tuned prior is \\mu \\sim N(6.5, 0.4^2)\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis range incorporates our uncertainty - it is wider than the Wikipedia range."
  },
  {
    "objectID": "files/lectures/06-2-normal-normal.html#normal-normal-conjugacy",
    "href": "files/lectures/06-2-normal-normal.html#normal-normal-conjugacy",
    "title": "Normal-Normal Model",
    "section": "Normal-Normal Conjugacy",
    "text": "Normal-Normal Conjugacy\n\nLet \\mu \\in \\mathbb{R} be an unknown mean parameter and (Y_1, Y_2, ..., Y_n) be an independent N(\\mu, \\sigma^2) sample where \\sigma is assumed to be known.\nThe Normal-Normal Bayesian model is as follows:\n\n\n\\begin{align*}\nY_i | \\mu &\\overset{\\text{iid}} \\sim N(\\mu, \\sigma^2) \\\\\n\\mu &\\sim N(\\theta, \\tau^2) \\\\\n\\mu | \\overset{\\to}y &\\sim N\\left( \\theta \\frac{\\sigma^2}{n\\tau^2 + \\sigma^2} + \\bar{y} \\frac{n\\tau^2}{n\\tau^2 + \\sigma^2}, \\frac{\\tau^2 \\sigma^2}{n \\tau^2 + \\sigma^2} \\right)\n\\end{align*}"
  },
  {
    "objectID": "files/lectures/06-2-normal-normal.html#normal-normal-conjugacy-1",
    "href": "files/lectures/06-2-normal-normal.html#normal-normal-conjugacy-1",
    "title": "Normal-Normal Model",
    "section": "Normal-Normal Conjugacy",
    "text": "Normal-Normal Conjugacy\n\nLet’s think about our posterior and some implications,\n\n\\mu | \\overset{\\to}y \\sim N\\left( \\theta \\frac{\\sigma^2}{n\\tau^2 + \\sigma^2} + \\bar{y} \\frac{n\\tau^2}{n\\tau^2 + \\sigma^2}, \\frac{\\tau^2 \\sigma^2}{n \\tau^2 + \\sigma^2} \\right)\n\nWhat happens as n increases?"
  },
  {
    "objectID": "files/lectures/06-2-normal-normal.html#normal-normal-conjugacy-2",
    "href": "files/lectures/06-2-normal-normal.html#normal-normal-conjugacy-2",
    "title": "Normal-Normal Model",
    "section": "Normal-Normal Conjugacy",
    "text": "Normal-Normal Conjugacy\n\nLet’s think about our posterior and some implications,\n\n\\mu | \\overset{\\to}y \\sim N\\left( \\theta \\frac{\\sigma^2}{n\\tau^2 + \\sigma^2} + \\bar{y} \\frac{n\\tau^2}{n\\tau^2 + \\sigma^2}, \\frac{\\tau^2 \\sigma^2}{n \\tau^2 + \\sigma^2} \\right)\n\nWhat happens as n increases?\n\n\n\\begin{align*}\n\\frac{\\sigma^2}{n\\tau^2 + \\sigma^2} &\\to 0 \\\\\n\\frac{n\\tau^2}{n\\tau^2 + \\sigma^2} &\\to 1 \\\\\n\\frac{\\tau^2 \\sigma^2}{n \\tau^2 + \\sigma^2} &\\to 0\n\\end{align*}"
  },
  {
    "objectID": "files/lectures/06-2-normal-normal.html#normal-normal-conjugacy-3",
    "href": "files/lectures/06-2-normal-normal.html#normal-normal-conjugacy-3",
    "title": "Normal-Normal Model",
    "section": "Normal-Normal Conjugacy",
    "text": "Normal-Normal Conjugacy\n\nLet’s think about our posterior and some implications,\n\n\n\\begin{align*}\n\\frac{\\sigma^2}{n\\tau^2 + \\sigma^2} &\\to 0 \\\\\n\\frac{n\\tau^2}{n\\tau^2 + \\sigma^2} &\\to 1 \\\\\n\\frac{\\tau^2 \\sigma^2}{n \\tau^2 + \\sigma^2} &\\to 0\n\\end{align*}\n\n\nThe posterior mean places less weight on the prior mean and more weight on the sample mean \\bar{y}.\nThe posterior certainty about \\mu increases and becomes more in sync with the data."
  },
  {
    "objectID": "files/lectures/06-2-normal-normal.html#the-normal-posterior-model",
    "href": "files/lectures/06-2-normal-normal.html#the-normal-posterior-model",
    "title": "Normal-Normal Model",
    "section": "The Normal Posterior Model",
    "text": "The Normal Posterior Model\n\nLet us now apply this to our example.\nWe have our prior model, \\mu \\sim N(6.5, 0.4^2).\nLet’s look at the football dataset in the bayesrules package.\n\n\ndata(football)\nconcussion_subjects &lt;- football %&gt;% \n  filter(group == \"fb_concuss\")\n\n\nWhat is the average hippocampal volume?"
  },
  {
    "objectID": "files/lectures/06-2-normal-normal.html#the-normal-posterior-model-1",
    "href": "files/lectures/06-2-normal-normal.html#the-normal-posterior-model-1",
    "title": "Normal-Normal Model",
    "section": "The Normal Posterior Model",
    "text": "The Normal Posterior Model\n\nLet us now apply this to our example.\nWe have our prior model, \\mu \\sim N(6.5, 0.4^2).\nLet’s look at the football dataset in the bayesrules package.\n\n\ndata(football)\nconcussion_subjects &lt;- football %&gt;% \n  filter(group == \"fb_concuss\")\n\n\nWhat is the average hippocampal volume?\n\n\nmean(concussion_subjects$volume)\n\n[1] 5.7346"
  },
  {
    "objectID": "files/lectures/06-2-normal-normal.html#the-normal-posterior-model-2",
    "href": "files/lectures/06-2-normal-normal.html#the-normal-posterior-model-2",
    "title": "Normal-Normal Model",
    "section": "The Normal Posterior Model",
    "text": "The Normal Posterior Model\n\nWe can also plot the density!\n\n\n\nconcussion_subjects %&gt;% ggplot(aes(x = volume)) + geom_density() + theme_bw()"
  },
  {
    "objectID": "files/lectures/06-2-normal-normal.html#the-normal-posterior-model-3",
    "href": "files/lectures/06-2-normal-normal.html#the-normal-posterior-model-3",
    "title": "Normal-Normal Model",
    "section": "The Normal Posterior Model",
    "text": "The Normal Posterior Model\n\nNow, we can plug in the information we have (n = 25, \\bar{y} = 5.735, \\sigma = 0.5) into our likelihood,\n\n\nL(\\mu|\\overset{\\to}y) \\propto \\exp \\left\\{ \\frac{-(5.735 - \\mu)^2}{2(0.5^2/25)} \\right\\}"
  },
  {
    "objectID": "files/lectures/06-2-normal-normal.html#the-normal-posterior-model-4",
    "href": "files/lectures/06-2-normal-normal.html#the-normal-posterior-model-4",
    "title": "Normal-Normal Model",
    "section": "The Normal Posterior Model",
    "text": "The Normal Posterior Model\n\nWe are now ready to put together our posterior:\n\nData distribution, Y_i | \\mu \\overset{\\text{iid}} \\sim N(\\mu, \\sigma^2)\nPrior distribution, \\mu \\sim N(\\theta, \\tau^2)\n\nPosterior distribution, \\mu | \\overset{\\to}y \\sim N\\left( \\theta \\frac{\\sigma^2}{n\\tau^2 + \\sigma^2} + \\bar{y} \\frac{n\\tau^2}{n\\tau^2 + \\sigma^2}, \\frac{\\tau^2 \\sigma^2}{n \\tau^2 + \\sigma^2} \\right)\n\nGiven our information (\\theta=6.5, \\tau=0.4, n=25, \\bar{y}=5.735, \\sigma=0.5), our posterior is\n\n\n\\mu | \\overset{\\to}y \\sim N\\left( \\theta \\frac{\\sigma^2}{n\\tau^2 + \\sigma^2} + \\bar{y} \\frac{n\\tau^2}{n\\tau^2 + \\sigma^2}, \\frac{\\tau^2 \\sigma^2}{n \\tau^2 + \\sigma^2} \\right)"
  },
  {
    "objectID": "files/lectures/06-2-normal-normal.html#the-normal-posterior-model-5",
    "href": "files/lectures/06-2-normal-normal.html#the-normal-posterior-model-5",
    "title": "Normal-Normal Model",
    "section": "The Normal Posterior Model",
    "text": "The Normal Posterior Model\n\nGiven our information (\\theta=6.5, \\tau=0.4, n=25, \\bar{y}=5.735, \\sigma=0.5), our posterior is\n\n\n\\begin{align*}\n\\mu | \\overset{\\to}y &\\sim N\\left( \\theta \\frac{\\sigma^2}{n\\tau^2 + \\sigma^2} + \\bar{y} \\frac{n\\tau^2}{n\\tau^2 + \\sigma^2}, \\frac{\\tau^2 \\sigma^2}{n \\tau^2 + \\sigma^2} \\right) \\\\\n&\\sim N\\left( 6.5 \\frac{0.5^2}{25 \\cdot 0.4^2 + 0.5^2} + 5.735 \\frac{25 \\cdot 0.4^2}{25 \\cdot 0.4^2 + 0.5^2}, \\frac{0.4^2 \\cdot 0.5^2}{25 \\cdot 0.4^2 + 0.5^2} \\right) \\\\\n&\\sim N(6.5 \\cdot 0.0588 + 5.737 \\cdot 0.9412, 0.09^2) \\\\\n&\\sim N(5.78, 0.09^2)\n\\end{align*}\n\n\nLooking at the posterior, we can see the weights\n\n95% on the data mean, 6% on the prior mean."
  },
  {
    "objectID": "files/lectures/06-2-normal-normal.html#the-normal-posterior-model-6",
    "href": "files/lectures/06-2-normal-normal.html#the-normal-posterior-model-6",
    "title": "Normal-Normal Model",
    "section": "The Normal Posterior Model",
    "text": "The Normal Posterior Model\n\nLooking at just the prior and data distributions,"
  },
  {
    "objectID": "files/lectures/06-2-normal-normal.html#the-normal-posterior-model-7",
    "href": "files/lectures/06-2-normal-normal.html#the-normal-posterior-model-7",
    "title": "Normal-Normal Model",
    "section": "The Normal Posterior Model",
    "text": "The Normal Posterior Model\n\nNow including the posterior,"
  },
  {
    "objectID": "files/lectures/06-2-normal-normal.html#the-normal-posterior-model-8",
    "href": "files/lectures/06-2-normal-normal.html#the-normal-posterior-model-8",
    "title": "Normal-Normal Model",
    "section": "The Normal Posterior Model",
    "text": "The Normal Posterior Model\n\nWe can use the summarize_normal_normal() function to summarize the distribution,\n\n\nsummarize_normal_normal(mean = 6.5, sd = 0.4, sigma = 0.5, y_bar = 5.735, n = 25)"
  },
  {
    "objectID": "files/lectures/06-2-normal-normal.html#wrap-up-normal-normal-model",
    "href": "files/lectures/06-2-normal-normal.html#wrap-up-normal-normal-model",
    "title": "Normal-Normal Model",
    "section": "Wrap Up: Normal-Normal Model",
    "text": "Wrap Up: Normal-Normal Model\n\nWe have built the Normal-Normal model for \\mu, an unknown mean.\n\n\n\\begin{equation*}\n\\begin{aligned}\nY_i | \\mu &\\overset{\\text{iid}} \\sim N(\\mu, \\sigma^2) \\\\\n\\mu &\\sim N(\\theta, \\tau^2) &\n\\end{aligned}\n\\Rightarrow\n\\begin{aligned}\n&& \\mu | \\overset{\\to}y &\\sim N\\left( \\theta \\frac{\\sigma^2}{n\\tau^2 + \\sigma^2} + \\bar{y} \\frac{n\\tau^2}{n\\tau^2 + \\sigma^2}, \\frac{\\tau^2 \\sigma^2}{n \\tau^2 + \\sigma^2} \\right) \\\\\n\\end{aligned}\n\\end{equation*}\n\n\nThe prior model, f(\\mu), is given by N(\\theta,\\tau^2).\nThe data model, f(Y|\\mu), is given by N(\\mu, \\sigma^2).\nThe posterior model is a Normal distribution with updated parameters\n\nmean = \\theta \\frac{\\sigma^2}{n\\tau^2 + \\sigma^2} + \\bar{y} \\frac{n\\tau^2}{n\\tau^2 + \\sigma^2}\nvariance = \\frac{\\tau^2 \\sigma^2}{n \\tau^2 + \\sigma^2}"
  },
  {
    "objectID": "files/lectures/06-2-normal-normal.html#wrap-up",
    "href": "files/lectures/06-2-normal-normal.html#wrap-up",
    "title": "Normal-Normal Model",
    "section": "Wrap Up",
    "text": "Wrap Up\n\nThis week we have learned the other two conjugate families.\n\nGamma-Poisson: count outcomes\nNormal-Normal: continuous outcomes\n\nWhile we are not forced to analyze our data using conjugate families, our lives are much easier when we can use the known relationships.\nNow that we know how to specify the posterior distributions, we can focus on moving forward with drawing conclusions about the posterior distribution.\n\nProbabilities\nInference"
  },
  {
    "objectID": "files/lectures/06-2-normal-normal.html#homework-practice",
    "href": "files/lectures/06-2-normal-normal.html#homework-practice",
    "title": "Normal-Normal Model",
    "section": "Homework / Practice",
    "text": "Homework / Practice\n\nFrom the Bayes Rules! textbook:\n\n5.9\n5.10"
  },
  {
    "objectID": "files/lectures/05-2-example.html",
    "href": "files/lectures/05-2-example.html",
    "title": "Mario Kart Analysis",
    "section": "",
    "text": "In Mario Kart 8 Deluxe, item boxes give different items depending on race position. To reduce the “position bias,” only item boxes opened while the racer was in mid-pack (positions 4–10) were recorded.\nYou want to estimate the probability that an item box yields a Red Shell. When playing the Special Cup, only 31 red shells were seen in 114 boxes opened by mid-pack racers.\nFind the posterior distribution under two priors:\n\nFlat/uninformative prior, Beta(1,1).\nBeta(\\alpha, \\beta) of your choosing.\n\n\nFind the posterior under the flat prior, Beta(1,1).\n\nplot_beta_binomial(alpha = 1, beta = 1,\n                   y = 31, n = 114) +\n  theme_bw()\n\n\n\n\n\n\n\n\nFind the posterior under a prior of your choosing.\n\nplot_beta_binomial(alpha = 8, beta = 4,\n                   y = 31, n = 114) +\n  theme_bw()\n\n\n\n\n\n\n\nplot_beta_binomial(alpha = 23, beta = 57,\n                   y = 31, n = 114) +\n  theme_bw()\n\n\n\n\n\n\n\n\n\nSuppose that we know the individual data points for the Special Cup:\n\nCloudtop Cruise, of 28 boxes, 6 had a red shell.\nBone-Dry Dunes, of 27 boxes, 8 had a red shell.\nBowser’s Castle, of 29 boxes, 12 had a red shell.\nRainbow Road, of 30 boxes, 5 had a red shell.\n\nProve sequentiality to yourself.\n\nAnalyze the data race-by-race in this order: Cloudtop Cruise, Bone-Dry Dunes, Bowser’s Castle, and Rainbow Road.\nAnalyze the data race-by-race in this order: Rainbow Road, Bowser’s Castle, Cloudtop Cruise, and Bone-Dry Dunes.\n\n\nAnalyze the data race-by-race in this order: Cloudtop Cruise, Bone-Dry Dunes, Bowser’s Castle, and Rainbow Road.\n\nsummarize_beta_binomial(alpha = 1, beta = 1,\n                        y = 6+8+12+5, n = 28+27+29+30) # overall\n\n\n  \n\n\nsummarize_beta_binomial(alpha = 1, beta = 1,\n                        y = 6, n = 28) # cloudtop cruise\n\n\n  \n\n\nsummarize_beta_binomial(alpha = 7, beta = 23,\n                        y = 8, n = 27) # bone-dry dunes\n\n\n  \n\n\nsummarize_beta_binomial(alpha = 15, beta = 42,\n                        y = 12, n = 29) # bowser's castle\n\n\n  \n\n\nsummarize_beta_binomial(alpha = 27, beta = 59,\n                        y = 5, n = 30) # bowser's castle\n\n\n  \n\n\n\nFinal distribution is Beta(32, 84).\n\nsummarize_beta_binomial(alpha = 27, beta = 59,\n                        y = 5, n = 30) # rainbow road\n\n\n  \n\n\n\nFinal distribution is Beta(32, 84).\nAnalyze the data race-by-race in this order: Rainbow Road, Bowser’s Castle, Cloudtop Cruise, and Bone-Dry Dunes.\nIn order:\n\nsummarize_beta_binomial(alpha = 1, beta = 1,\n                        y = 5, n = 30) # rainbow road\n\n\n  \n\n\nsummarize_beta_binomial(alpha = 6, beta = 26,\n                        y = 12, n = 29) # bowser's castle\n\n\n  \n\n\nsummarize_beta_binomial(alpha = 18, beta = 43,\n                        y = 6, n = 28) # cloudtop cruise\n\n\n  \n\n\nsummarize_beta_binomial(alpha = 24, beta = 65,\n                        y = 8, n = 27) # bone-dry dunes\n\n\n  \n\n\n\nFinal distribution is Beta(32, 84).\nAll together:\n\nsummarize_beta_binomial(alpha = 1, beta = 1,\n                        y = 5+12+6+8, n = 30+29+28+27)\n\n\n  \n\n\n\nFinal distribution is Beta(32, 84)."
  },
  {
    "objectID": "files/lectures/05-2-example.html#example-mario-kart",
    "href": "files/lectures/05-2-example.html#example-mario-kart",
    "title": "Mario Kart Analysis",
    "section": "",
    "text": "In Mario Kart 8 Deluxe, item boxes give different items depending on race position. To reduce the “position bias,” only item boxes opened while the racer was in mid-pack (positions 4–10) were recorded.\nYou want to estimate the probability that an item box yields a Red Shell. When playing the Special Cup, only 31 red shells were seen in 114 boxes opened by mid-pack racers.\nFind the posterior distribution under two priors:\n\nFlat/uninformative prior, Beta(1,1).\nBeta(\\alpha, \\beta) of your choosing.\n\n\nFind the posterior under the flat prior, Beta(1,1).\n\nplot_beta_binomial(alpha = 1, beta = 1,\n                   y = 31, n = 114) +\n  theme_bw()\n\n\n\n\n\n\n\n\nFind the posterior under a prior of your choosing.\n\nplot_beta_binomial(alpha = 8, beta = 4,\n                   y = 31, n = 114) +\n  theme_bw()\n\n\n\n\n\n\n\nplot_beta_binomial(alpha = 23, beta = 57,\n                   y = 31, n = 114) +\n  theme_bw()\n\n\n\n\n\n\n\n\n\nSuppose that we know the individual data points for the Special Cup:\n\nCloudtop Cruise, of 28 boxes, 6 had a red shell.\nBone-Dry Dunes, of 27 boxes, 8 had a red shell.\nBowser’s Castle, of 29 boxes, 12 had a red shell.\nRainbow Road, of 30 boxes, 5 had a red shell.\n\nProve sequentiality to yourself.\n\nAnalyze the data race-by-race in this order: Cloudtop Cruise, Bone-Dry Dunes, Bowser’s Castle, and Rainbow Road.\nAnalyze the data race-by-race in this order: Rainbow Road, Bowser’s Castle, Cloudtop Cruise, and Bone-Dry Dunes.\n\n\nAnalyze the data race-by-race in this order: Cloudtop Cruise, Bone-Dry Dunes, Bowser’s Castle, and Rainbow Road.\n\nsummarize_beta_binomial(alpha = 1, beta = 1,\n                        y = 6+8+12+5, n = 28+27+29+30) # overall\n\n\n  \n\n\nsummarize_beta_binomial(alpha = 1, beta = 1,\n                        y = 6, n = 28) # cloudtop cruise\n\n\n  \n\n\nsummarize_beta_binomial(alpha = 7, beta = 23,\n                        y = 8, n = 27) # bone-dry dunes\n\n\n  \n\n\nsummarize_beta_binomial(alpha = 15, beta = 42,\n                        y = 12, n = 29) # bowser's castle\n\n\n  \n\n\nsummarize_beta_binomial(alpha = 27, beta = 59,\n                        y = 5, n = 30) # bowser's castle\n\n\n  \n\n\n\nFinal distribution is Beta(32, 84).\n\nsummarize_beta_binomial(alpha = 27, beta = 59,\n                        y = 5, n = 30) # rainbow road\n\n\n  \n\n\n\nFinal distribution is Beta(32, 84).\nAnalyze the data race-by-race in this order: Rainbow Road, Bowser’s Castle, Cloudtop Cruise, and Bone-Dry Dunes.\nIn order:\n\nsummarize_beta_binomial(alpha = 1, beta = 1,\n                        y = 5, n = 30) # rainbow road\n\n\n  \n\n\nsummarize_beta_binomial(alpha = 6, beta = 26,\n                        y = 12, n = 29) # bowser's castle\n\n\n  \n\n\nsummarize_beta_binomial(alpha = 18, beta = 43,\n                        y = 6, n = 28) # cloudtop cruise\n\n\n  \n\n\nsummarize_beta_binomial(alpha = 24, beta = 65,\n                        y = 8, n = 27) # bone-dry dunes\n\n\n  \n\n\n\nFinal distribution is Beta(32, 84).\nAll together:\n\nsummarize_beta_binomial(alpha = 1, beta = 1,\n                        y = 5+12+6+8, n = 30+29+28+27)\n\n\n  \n\n\n\nFinal distribution is Beta(32, 84)."
  },
  {
    "objectID": "files/lectures/13-1-writing-stats-methods.html#the-methods-section",
    "href": "files/lectures/13-1-writing-stats-methods.html#the-methods-section",
    "title": "Writing about Statistical Methods",
    "section": "The Methods Section",
    "text": "The Methods Section\n\nOverall, the methods section of a paper includes relevant project details such that the study can be replicated.\n\nReplication for both the subject-matter expert and the person performing data analysis.\n\nThe subsections we expect to contribute the most to:\n\nData Management (optional section)\nStatistical Analysis\nSupplementary/Appendix Materials (optional section)"
  },
  {
    "objectID": "files/lectures/13-1-writing-stats-methods.html#data-wrangling",
    "href": "files/lectures/13-1-writing-stats-methods.html#data-wrangling",
    "title": "Writing about Statistical Methods",
    "section": "Data Wrangling",
    "text": "Data Wrangling\n\nThe data wrangling portion of the methods section should outline how the analysis dataset was created from the raw dataset(s).\n\nThis is why I maintain that we should do all data management with programming – we have a record of everything, from start to finish.\n\nInclusion/exclusion criteria should be outlined.\n\n“Participants with missing data on key covariates were excluded from analysis.”\n“The analysis data was restricted to those in the ‘first time in college’ cohort.”\n\nCreation of new variables should be outlined.\n\n“Hypertension was defined as systolic blood pressure &gt; 140, diastolic blood pressure &gt; 90, or use of antihypertensive medications.”"
  },
  {
    "objectID": "files/lectures/13-1-writing-stats-methods.html#order-matters",
    "href": "files/lectures/13-1-writing-stats-methods.html#order-matters",
    "title": "Writing about Statistical Methods",
    "section": "Order Matters",
    "text": "Order Matters\n\nList methodology in the order in which results are presented.\nTypically the order is:\n\nDescriptive statistics (Table 1)\nResearch Question 1 (e.g., Table 2)\nResearch Question 2 (e.g., Table 3)\n….\nSoftware utilized\n\nData management\nStatistical analysis\nGraphing\n\nA priori significance level"
  },
  {
    "objectID": "files/lectures/13-1-writing-stats-methods.html#appearance-in-paper-matters",
    "href": "files/lectures/13-1-writing-stats-methods.html#appearance-in-paper-matters",
    "title": "Writing about Statistical Methods",
    "section": "Appearance in Paper Matters",
    "text": "Appearance in Paper Matters\n\nIf a table/graph (or analysis) is not discussed in the paper, it should not be included in the paper.\nThis also extends to the methods section: we only describe methods that are used in the paper."
  },
  {
    "objectID": "files/lectures/13-1-writing-stats-methods.html#table-1",
    "href": "files/lectures/13-1-writing-stats-methods.html#table-1",
    "title": "Writing about Statistical Methods",
    "section": "Table 1",
    "text": "Table 1\n\nTable 1 (descriptives)\n\nDescriptive statistics are shown as [mean/median] ([standard deviation/IQR]) for continuous variables or n (%) for categorical variables.\n\nThese tables are sometimes split by grouping variables.\nIf comparing groups in your Table 1, indicate what test(s) were used to compare the groups.\nNote: there is some debate about using p-values in Table 1… but the idea is to show the audience what differences may or may not exist."
  },
  {
    "objectID": "files/lectures/13-1-writing-stats-methods.html#tables-2-and-beyond",
    "href": "files/lectures/13-1-writing-stats-methods.html#tables-2-and-beyond",
    "title": "Writing about Statistical Methods",
    "section": "Tables 2 and Beyond",
    "text": "Tables 2 and Beyond\n\nThe tables included after Table 1 typically correspond to research questions/hypotheses.\nFor each table, indicate what outcome was modeled, what method was used to model it, and any relevant notes about the modeling.\n\nWere any transformations used?\nWere any covariates adjusted for?\n\nOnly necessary to note when tables showing model results exclude adjustors.\n\nWere assumptions met?\n\nNormality of residuals & equal variances\nProportional odds or hazards\nMulticollinearity"
  },
  {
    "objectID": "files/lectures/13-1-writing-stats-methods.html#tables-2-and-beyond-1",
    "href": "files/lectures/13-1-writing-stats-methods.html#tables-2-and-beyond-1",
    "title": "Writing about Statistical Methods",
    "section": "Tables 2 and Beyond",
    "text": "Tables 2 and Beyond\n\nThe tables included after Table 1 typically correspond to research questions/hypotheses.\nWe also want to explicitly state how the results are displayed in the tables.\n\nMy preference: the coefficient as we will interpret it and its corresponding 95% CI.\n\nNormal: \\hat{\\beta} (95% CI)\nGamma: \\exp\\{\\hat{\\beta}\\} (95% CI)\nLogistic: Odds Ratio (95% CI)\nPoisson: Incidence Rate Ratio (95% CI)\nCox: Hazard Ratio (95% CI)"
  },
  {
    "objectID": "files/lectures/13-1-writing-stats-methods.html#the-technical-details",
    "href": "files/lectures/13-1-writing-stats-methods.html#the-technical-details",
    "title": "Writing about Statistical Methods",
    "section": "The Technical “Details”",
    "text": "The Technical “Details”\n\nFinally, we include technical details about how the data was handled during the analysis.\nWe always state (and cite) the software used to produce information for the paper.\n\nYes, sometimes I use different programs for data management, statistical analysis, and graphing….\n\nEvery software has its preferences on how use should be cited in papers.\n\nSAS software, version [version number] (SAS Institute, Cary, NC).\nStata statistical software, release [release number] (StataCorp, College Station, Texas).\nR version [version number] (R Foundation for Statistical Computing, Vienna, Austria)."
  },
  {
    "objectID": "files/lectures/13-1-writing-stats-methods.html#the-technical-details-1",
    "href": "files/lectures/13-1-writing-stats-methods.html#the-technical-details-1",
    "title": "Writing about Statistical Methods",
    "section": "The Technical “Details”",
    "text": "The Technical “Details”\n\nYES, we will cite the packages used in addition to base R!\n\nThis is the nice thing to do – we recognize those that made our lives easier with programming.\nBut also, this is how developers (academic) can quantify and demonstrate the usefulness of their contributions to the R community (for annual evaluation purposes).\n\nFor base R:\n\n\ncitation()\n\n\nFor packages:\n\n\ncitation(\"package_name\")"
  },
  {
    "objectID": "files/lectures/13-1-writing-stats-methods.html#the-technical-details-2",
    "href": "files/lectures/13-1-writing-stats-methods.html#the-technical-details-2",
    "title": "Writing about Statistical Methods",
    "section": "The Technical “Details”",
    "text": "The Technical “Details”\n\ncitation()\n\nTo cite R in publications use:\n\n  R Core Team (2025). _R: A Language and Environment for Statistical\n  Computing_. R Foundation for Statistical Computing, Vienna, Austria.\n  &lt;https://www.R-project.org/&gt;.\n\nA BibTeX entry for LaTeX users is\n\n  @Manual{,\n    title = {R: A Language and Environment for Statistical Computing},\n    author = {{R Core Team}},\n    organization = {R Foundation for Statistical Computing},\n    address = {Vienna, Austria},\n    year = {2025},\n    url = {https://www.R-project.org/},\n  }\n\nWe have invested a lot of time and effort in creating R, please cite it\nwhen using it for data analysis. See also 'citation(\"pkgname\")' for\nciting R packages."
  },
  {
    "objectID": "files/lectures/13-1-writing-stats-methods.html#the-technical-details-3",
    "href": "files/lectures/13-1-writing-stats-methods.html#the-technical-details-3",
    "title": "Writing about Statistical Methods",
    "section": "The Technical “Details”",
    "text": "The Technical “Details”\n\ncitation(\"tidyverse\")\n\nTo cite package 'tidyverse' in publications use:\n\n  Wickham H, Averick M, Bryan J, Chang W, McGowan LD, François R,\n  Grolemund G, Hayes A, Henry L, Hester J, Kuhn M, Pedersen TL, Miller\n  E, Bache SM, Müller K, Ooms J, Robinson D, Seidel DP, Spinu V,\n  Takahashi K, Vaughan D, Wilke C, Woo K, Yutani H (2019). \"Welcome to\n  the tidyverse.\" _Journal of Open Source Software_, *4*(43), 1686.\n  doi:10.21105/joss.01686 &lt;https://doi.org/10.21105/joss.01686&gt;.\n\nA BibTeX entry for LaTeX users is\n\n  @Article{,\n    title = {Welcome to the {tidyverse}},\n    author = {Hadley Wickham and Mara Averick and Jennifer Bryan and Winston Chang and Lucy D'Agostino McGowan and Romain François and Garrett Grolemund and Alex Hayes and Lionel Henry and Jim Hester and Max Kuhn and Thomas Lin Pedersen and Evan Miller and Stephan Milton Bache and Kirill Müller and Jeroen Ooms and David Robinson and Dana Paige Seidel and Vitalie Spinu and Kohske Takahashi and Davis Vaughan and Claus Wilke and Kara Woo and Hiroaki Yutani},\n    year = {2019},\n    journal = {Journal of Open Source Software},\n    volume = {4},\n    number = {43},\n    pages = {1686},\n    doi = {10.21105/joss.01686},\n  }"
  },
  {
    "objectID": "files/lectures/13-1-writing-stats-methods.html#the-technical-details-4",
    "href": "files/lectures/13-1-writing-stats-methods.html#the-technical-details-4",
    "title": "Writing about Statistical Methods",
    "section": "The Technical “Details”",
    "text": "The Technical “Details”\n\ncitation(\"bayesrules\")\n\nTo cite bayesrules package in publications use:\n\n  Mine Dogucu, Alicia Johnson, Miles Ott (2021). bayesrules: Datasets\n  and Supplemental Functions from Bayes Rules! Book Retrieved from\n  https://github.com/bayes-rules/bayesrules R package version 0.0.2.900\n\nA BibTeX entry for LaTeX users is\n\n  @Manual{,\n    title = {bayesrules: Datasets and Supplemental Functions from Bayes Rules! Book},\n    author = {Mine Dogucu and Alicia Johnson and Miles Ott},\n    year = {2021},\n    url = {https://github.com/bayes-rules/bayesrules},\n    note = {R package version 0.0.2.9000},\n  }"
  },
  {
    "objectID": "files/lectures/13-1-writing-stats-methods.html#the-technical-details-5",
    "href": "files/lectures/13-1-writing-stats-methods.html#the-technical-details-5",
    "title": "Writing about Statistical Methods",
    "section": "The Technical “Details”",
    "text": "The Technical “Details”\n\nFinally (finally!), we should state the a priori significance level if we are analyzing under the frequentist framework.\nIf we are analyzing under the Bayesian framework, that will be obvious when we describe our modeling approach."
  },
  {
    "objectID": "files/lectures/13-1-writing-stats-methods.html#example-1-table-1",
    "href": "files/lectures/13-1-writing-stats-methods.html#example-1-table-1",
    "title": "Writing about Statistical Methods",
    "section": "Example 1: Table 1",
    "text": "Example 1: Table 1\n\n\nDescriptive data are shown as median (range) or n (%), as appropriate. What numbers are shown in the cells of the table?\nContinuous variables were compared using the Kruskal-Wallis test. Categorical variables were compared using the \\chi^2 or Fisher’s exact tests, as appropriate. How were p-values generated?"
  },
  {
    "objectID": "files/lectures/13-1-writing-stats-methods.html#example-1-table-2",
    "href": "files/lectures/13-1-writing-stats-methods.html#example-1-table-2",
    "title": "Writing about Statistical Methods",
    "section": "Example 1: Table 2+",
    "text": "Example 1: Table 2+\n\n\nHospital mortality and use of extracorporeal membrane oxygenation were modeled using logistic regression. What outcomes were modeled and how?"
  },
  {
    "objectID": "files/lectures/13-1-writing-stats-methods.html#example-1-table-2-1",
    "href": "files/lectures/13-1-writing-stats-methods.html#example-1-table-2-1",
    "title": "Writing about Statistical Methods",
    "section": "Example 1: Table 2+",
    "text": "Example 1: Table 2+\n\n\nRegression results are shown as odds ratio (95% confidence interval; CI). What are in the cells of the table?"
  },
  {
    "objectID": "files/lectures/13-1-writing-stats-methods.html#example-1-table-2-2",
    "href": "files/lectures/13-1-writing-stats-methods.html#example-1-table-2-2",
    "title": "Writing about Statistical Methods",
    "section": "Example 1: Table 2+",
    "text": "Example 1: Table 2+\n\n\nSignificance of predictors was assessed using the omnibus Wald \\chi^2 test. When appropriate, pairwise comparisons were made using the Bonferroni adjustment. How were p-values generated/evaluated?"
  },
  {
    "objectID": "files/lectures/13-1-writing-stats-methods.html#example-1-table-2-3",
    "href": "files/lectures/13-1-writing-stats-methods.html#example-1-table-2-3",
    "title": "Writing about Statistical Methods",
    "section": "Example 1: Table 2+",
    "text": "Example 1: Table 2+\n\n\nNote the informative footnote!\n\nI try to include information in footnotes as a reminder for the reader."
  },
  {
    "objectID": "files/lectures/13-1-writing-stats-methods.html#example-1-pulled-together",
    "href": "files/lectures/13-1-writing-stats-methods.html#example-1-pulled-together",
    "title": "Writing about Statistical Methods",
    "section": "Example 1: Pulled together",
    "text": "Example 1: Pulled together\n\nDescriptive data are shown as median (range) or n (%), as appropriate. Continuous variables were compared using the Kruskal-Wallis test. Categorical variables were compared using the \\chi^2 or Fisher’s exact tests, as appropriate.\nHospital mortality and use of extracorporeal membrane oxygenation were modeled using logistic regression. Regression results are shown as odds ratio (95% confidence interval; CI). Significance of predictors was assessed using the omnibus Wald \\chi^2 test. When appropriate, pairwise comparisons were made using the Bonferroni adjustment.\nData management and analysis were performed using SAS software, Version 9.3 (SAS Institute, Inc., Cary, NC). A priori significance was defined as p &lt; 0.05."
  },
  {
    "objectID": "files/lectures/13-1-writing-stats-methods.html#example-2-table-2",
    "href": "files/lectures/13-1-writing-stats-methods.html#example-2-table-2",
    "title": "Writing about Statistical Methods",
    "section": "Example 2: Table 2+",
    "text": "Example 2: Table 2+\n\n\nSometimes we decide to show results for a subset of predictors."
  },
  {
    "objectID": "files/lectures/13-1-writing-stats-methods.html#example-2-table-2-1",
    "href": "files/lectures/13-1-writing-stats-methods.html#example-2-table-2-1",
    "title": "Writing about Statistical Methods",
    "section": "Example 2: Table 2+",
    "text": "Example 2: Table 2+\n\n\nWe just need to make sure that information is noted somewhere.\n\nAlthough the information is in the footnote, the models being compared/contrasted must be clearly outlined in the methods section."
  },
  {
    "objectID": "files/lectures/13-1-writing-stats-methods.html#example-3-table-2",
    "href": "files/lectures/13-1-writing-stats-methods.html#example-3-table-2",
    "title": "Writing about Statistical Methods",
    "section": "Example 3: Table 2+",
    "text": "Example 3: Table 2+\n\n\nCorrelations between covariates at initial survey were assessed using Spearman’s correlation. What method was used to assess correlations?"
  },
  {
    "objectID": "files/lectures/13-1-writing-stats-methods.html#example-3-table-2-1",
    "href": "files/lectures/13-1-writing-stats-methods.html#example-3-table-2-1",
    "title": "Writing about Statistical Methods",
    "section": "Example 3: Table 2+",
    "text": "Example 3: Table 2+\n\n\nLet’s take a look at the table description and footnotes…"
  },
  {
    "objectID": "files/lectures/13-1-writing-stats-methods.html#example-3-table-2-2",
    "href": "files/lectures/13-1-writing-stats-methods.html#example-3-table-2-2",
    "title": "Writing about Statistical Methods",
    "section": "Example 3: Table 2+",
    "text": "Example 3: Table 2+\n\n\nHierarchical linear models (HLM) were used when modeling all outcomes of interest. First, the models had only sex, type of SCD, and the baseline sleep quality score as predictors. Then, the models were further ajdusted for depression score. Finally, the models were further adjusted for pain frequency, intensity, and interference. Because all HLM incrementally increased the R2 value, only the fully adjusted models are shown."
  },
  {
    "objectID": "files/lectures/13-1-writing-stats-methods.html#example-3-table-2-3",
    "href": "files/lectures/13-1-writing-stats-methods.html#example-3-table-2-3",
    "title": "Writing about Statistical Methods",
    "section": "Example 3: Table 2+",
    "text": "Example 3: Table 2+\n\n\nSleep quality and age appropriate sleep were modeled using binary logistic regression while sleep duration was modeled using ordinary least squares regression. Regression results are shown as odds ratio (95% confidence interval; CI) for logistic regression and unstandardized beta coefficients (95% CI) for linear regression. Significance of predictors was assessed using the omnibus Wald \\chi^2 test for categorical predictors. As necessary, pairwise comparisons were made using the Bonferroni adjustment."
  },
  {
    "objectID": "files/lectures/13-1-writing-stats-methods.html#example-4-supplementary-materials",
    "href": "files/lectures/13-1-writing-stats-methods.html#example-4-supplementary-materials",
    "title": "Writing about Statistical Methods",
    "section": "Example 4: Supplementary Materials",
    "text": "Example 4: Supplementary Materials\n\nSometimes, we have additional analyses that are not central to the main results and can be cut from the main body of the paper for either space or interpretability purposes.\n\nsensitivity analysis\nfull model results, including results from adjustors\nrecoding during data management (e.g., collapsing categories)\n\nExample of recently-published supplement: click here."
  },
  {
    "objectID": "files/lectures/13-1-writing-stats-methods.html#example-4-supplementary-materials-1",
    "href": "files/lectures/13-1-writing-stats-methods.html#example-4-supplementary-materials-1",
    "title": "Writing about Statistical Methods",
    "section": "Example 4: Supplementary Materials",
    "text": "Example 4: Supplementary Materials\n\nExample of recently-published supplement: click here.\nThe original rule still applies, though: if there is no mention of it in the paper, it should not be included in the supplementary materials.\n\ne.g., Likert-scale questions were condensed into three categories: agree, neutral, and disagree. Full details on the recoding process, including summary statistics, are shown in Appendix A.\ne.g., Results shown in Table 2 focus on the primary predictors of interest. Full model results, including adjustors, are shown in Appendix B."
  },
  {
    "objectID": "files/lectures/13-1-writing-stats-methods.html#example-5-writing-more-about-methods",
    "href": "files/lectures/13-1-writing-stats-methods.html#example-5-writing-more-about-methods",
    "title": "Writing about Statistical Methods",
    "section": "Example 5: Writing More About Methods",
    "text": "Example 5: Writing More About Methods\n\nDepending on the field, more information about methods may be necessary to help the reader interpret the results. Consider this graph:"
  },
  {
    "objectID": "files/lectures/13-1-writing-stats-methods.html#example-5-writing-more-about-methods-1",
    "href": "files/lectures/13-1-writing-stats-methods.html#example-5-writing-more-about-methods-1",
    "title": "Writing about Statistical Methods",
    "section": "Example 5: Writing More About Methods",
    "text": "Example 5: Writing More About Methods\n\nWhat did the resulting statistical methods section look like?"
  },
  {
    "objectID": "files/lectures/13-1-writing-stats-methods.html#example-5-writing-more-about-methods-2",
    "href": "files/lectures/13-1-writing-stats-methods.html#example-5-writing-more-about-methods-2",
    "title": "Writing about Statistical Methods",
    "section": "Example 5: Writing More About Methods",
    "text": "Example 5: Writing More About Methods\n\nWhy are we talking about this? (Adding details for interpretations.)\nYou will be expected to provide basic examples of how to interpret coefficients in the model(s) you are using in this analysis.\nAgain, only include what is necessary to understand the results.\n\nGoal: give the reader the background necessary to understand what they’re looking at.\nActual goal: reproducibility!"
  },
  {
    "objectID": "files/lectures/13-1-writing-stats-methods.html#other-notes",
    "href": "files/lectures/13-1-writing-stats-methods.html#other-notes",
    "title": "Writing about Statistical Methods",
    "section": "Other Notes",
    "text": "Other Notes\n\nWhen working with others & disagreeing on what should be included, I will ask myself, “is this my hill to die on?”\nBattles I have won:\n\n“Multivariable” vs. “Multivariate”\nIncluding interval estimates in tables\nIncluding p-values in Table 1\n\nBattles I have not won:\n\n“Multivariable” vs. “Multivariate”\nIncluding interval estimates in tables\nIncluding p-values in Table 1\n\nActual hill to die on: posthoc power analysis (1, 2)."
  },
  {
    "objectID": "files/lectures/13-1-writing-stats-methods.html#other-notes-1",
    "href": "files/lectures/13-1-writing-stats-methods.html#other-notes-1",
    "title": "Writing about Statistical Methods",
    "section": "Other Notes",
    "text": "Other Notes\n\nI do not write this section until after the results section is finalized and/or I know exactly what tables and graphs are being included in the paper.\nNote that generally, tables take the same appearance. I make sure we are consistent with how results are displayed across tables in a paper, which also helps with writing the methods section.\n\nThink: your brand in science.\n\nHowever, sometimes a journal has requirements for tables. Fields have different ways they present model results.\n\nMy approach: check target journal guidelines; look at recent papers in target journal to see if there is a common theme for tables of results."
  },
  {
    "objectID": "files/lectures/13-1-writing-stats-methods.html#wrap-up",
    "href": "files/lectures/13-1-writing-stats-methods.html#wrap-up",
    "title": "Writing about Statistical Methods",
    "section": "Wrap Up",
    "text": "Wrap Up\n\nToday we have talked about the basics of writing a statistical methods section corresponding to the analysis you performed.\nWhile this discussion is in the academic context, you can still employ these methods outside of academic papers:\n\nThink of this as leaving yourself (or your colleagues) notes on how the analysis was performed.\nAgain, our real goal here is to make our analyses reproducible."
  },
  {
    "objectID": "files/lectures/13-1-writing-stats-methods.html#upcoming-schedule",
    "href": "files/lectures/13-1-writing-stats-methods.html#upcoming-schedule",
    "title": "Writing about Statistical Methods",
    "section": "Upcoming Schedule",
    "text": "Upcoming Schedule\n\nWednesday:\n\nEveryone will give a 3 minute presentation of their results.\n\nNext deliverables:\n\nMethods section first draft (November 23).\nTables and Graphs (November 23)\n\nReminder 1! No class next week (Thanksgiving holiday).\nReminder 2! No formal meetings the week after.\n\nMonday (December 1): meet with EES collaborator.\nWednesday (December 3): work on final draft of research; Dr. Seals available by appointment during class time."
  },
  {
    "objectID": "files/lectures/07-2-bayesian-inference.html#introduction",
    "href": "files/lectures/07-2-bayesian-inference.html#introduction",
    "title": "Posterior Analysis",
    "section": "Introduction",
    "text": "Introduction\n\nNow that we know how to find posterior distributions, we will discuss how to use them to make inference.\nHere are the packages we need today:\n\n\nlibrary(bayesrules)\nlibrary(tidyverse)\nlibrary(rstan)\nlibrary(bayesplot)\nlibrary(broom.mixed)\nlibrary(janitor)\ndata(\"moma_sample\")"
  },
  {
    "objectID": "files/lectures/07-2-bayesian-inference.html#working-example",
    "href": "files/lectures/07-2-bayesian-inference.html#working-example",
    "title": "Posterior Analysis",
    "section": "Working Example",
    "text": "Working Example\n\nImagine you find yourself standing at the Museum of Modern Art (MoMA) in New York City, captivated by the artwork in front of you.\n\nWhat are the chances that this modern artist is Gen X or even younger, i.e., born in 1965 or later?\n\nLet \\pi denote the proportion of artists represented in major U.S. modern art museums that are Gen X or younger.\n\nLet’s let the Beta(4,6) prior model for \\pi reflect our prior assumption that major modern art museums disproportionately display artists born before 1965, i.e., \\pi most likely falls below 0.5.\nRationale: “modern art” dates back to the 1880s and it can take a while to gain recognition as an artist."
  },
  {
    "objectID": "files/lectures/07-2-bayesian-inference.html#working-example-1",
    "href": "files/lectures/07-2-bayesian-inference.html#working-example-1",
    "title": "Posterior Analysis",
    "section": "Working Example",
    "text": "Working Example\n\nThe Beta(4,6) prior model for \\pi:"
  },
  {
    "objectID": "files/lectures/07-2-bayesian-inference.html#working-example-2",
    "href": "files/lectures/07-2-bayesian-inference.html#working-example-2",
    "title": "Posterior Analysis",
    "section": "Working Example",
    "text": "Working Example\n\nLet’s consider the following dataset:\n\n\nhead(moma_sample)"
  },
  {
    "objectID": "files/lectures/07-2-bayesian-inference.html#working-example-3",
    "href": "files/lectures/07-2-bayesian-inference.html#working-example-3",
    "title": "Posterior Analysis",
    "section": "Working Example",
    "text": "Working Example\n\nCounting the number of Generation X and younger,\n\n\nmoma_sample %&gt;% \n  group_by(genx) %&gt;% \n  tally()"
  },
  {
    "objectID": "files/lectures/07-2-bayesian-inference.html#working-example-4",
    "href": "files/lectures/07-2-bayesian-inference.html#working-example-4",
    "title": "Posterior Analysis",
    "section": "Working Example",
    "text": "Working Example\n\nOur modeling is as follows,\n\n\n\\begin{align*}\nY|\\pi &\\sim \\text{Bin}(100,\\pi) \\\\\n\\pi &\\sim \\text{Beta}(4,6) \\\\\n\\pi | (Y = 14) &\\sim \\text{Beta(18,92)}\n\\end{align*}"
  },
  {
    "objectID": "files/lectures/07-2-bayesian-inference.html#working-example-5",
    "href": "files/lectures/07-2-bayesian-inference.html#working-example-5",
    "title": "Posterior Analysis",
    "section": "Working Example",
    "text": "Working Example"
  },
  {
    "objectID": "files/lectures/07-2-bayesian-inference.html#working-example-6",
    "href": "files/lectures/07-2-bayesian-inference.html#working-example-6",
    "title": "Posterior Analysis",
    "section": "Working Example",
    "text": "Working Example\n\nThere are three common tasks in posterior analysis:\n\nestimation,\nhypothesis testing, and\nprediction.\n\nFor example,\n\nWhat’s our estimate of \\pi?\nDoes our model support the claim that fewer than 20% of museum artists are Gen X or younger?\nIf we sample 20 more museum artists, how many do we predict will be Gen X or younger?"
  },
  {
    "objectID": "files/lectures/07-2-bayesian-inference.html#posterior-estimation",
    "href": "files/lectures/07-2-bayesian-inference.html#posterior-estimation",
    "title": "Posterior Analysis",
    "section": "Posterior Estimation",
    "text": "Posterior Estimation\n\nWe can construct posterior credible intervals.\n\nA posterior credible interval (CI) provides a range of posterior plausible values of \\theta, and thus a summary of both posterior central tendency and variability.\nA middle 95% CI is constructed by the 2.5th and 97.5th posterior percentiles, \\left( \\theta_{0.025}, \\theta_{0.975} \\right), and there is a 95% posterior probability that \\theta is in this range,\n\n\nP\\left[ \\theta \\in (\\theta_{0.025}, \\theta_{0.975})|Y=y \\right] = \\int_{\\theta_{0.025}}^{\\theta_{0.975}} f(\\theta|y) d\\theta = 0.95"
  },
  {
    "objectID": "files/lectures/07-2-bayesian-inference.html#posterior-estimation-1",
    "href": "files/lectures/07-2-bayesian-inference.html#posterior-estimation-1",
    "title": "Posterior Analysis",
    "section": "Posterior Estimation",
    "text": "Posterior Estimation\n\nRecall the Beta(18, 92) posterior model for \\pi, the proportion of modern art museum artists that are Gen X or younger.\n\n\nqbeta(c(0.025, 0.975), 18, 92) # 95% CI\n\n[1] 0.1009084 0.2379286\n\n\n\nThere is a 95% posterior probability that somewhere between 10% and 24% of museum artists are Gen X or younger."
  },
  {
    "objectID": "files/lectures/07-2-bayesian-inference.html#posterior-estimation-2",
    "href": "files/lectures/07-2-bayesian-inference.html#posterior-estimation-2",
    "title": "Posterior Analysis",
    "section": "Posterior Estimation",
    "text": "Posterior Estimation\n\nRecall the Beta(18, 92) posterior model for \\pi, the proportion of modern art museum artists that are Gen X or younger.\n\n\nqbeta(c(0.25, 0.75), 18, 92) # 50% CI\n\n[1] 0.1388414 0.1862197\n\n\n\nThere is a 50% posterior probability that somewhere between 14% and 19% of museum artists are Gen X or younger."
  },
  {
    "objectID": "files/lectures/07-2-bayesian-inference.html#posterior-estimation-3",
    "href": "files/lectures/07-2-bayesian-inference.html#posterior-estimation-3",
    "title": "Posterior Analysis",
    "section": "Posterior Estimation",
    "text": "Posterior Estimation\n\n95% is a common choice, however, note that it is somewhat arbitrary and used because of decades of tradition.\n\n\n\n\n\nThere is no one right credible interval.\n\nIt will just depend on the context of the analysis."
  },
  {
    "objectID": "files/lectures/07-2-bayesian-inference.html#posterior-hypothesis-testing",
    "href": "files/lectures/07-2-bayesian-inference.html#posterior-hypothesis-testing",
    "title": "Posterior Analysis",
    "section": "Posterior Hypothesis Testing",
    "text": "Posterior Hypothesis Testing\n\nSuppose we read an article claiming that fewer than 20% of museum artists are Gen X or younger.\n\n\n\n\n\nHow plausible is it that \\pi &lt; 0.2?\n\n\nP\\left[ \\pi &lt; 0.2 | Y = 14 \\right] = \\int_0^{0.2} f(\\pi|y = 14) d\\pi"
  },
  {
    "objectID": "files/lectures/07-2-bayesian-inference.html#posterior-hypothesis-testing-1",
    "href": "files/lectures/07-2-bayesian-inference.html#posterior-hypothesis-testing-1",
    "title": "Posterior Analysis",
    "section": "Posterior Hypothesis Testing",
    "text": "Posterior Hypothesis Testing\n\nPosterior Probability\n\n\nP\\left[ \\pi &lt; 0.2 | Y = 14 \\right] = \\int_0^{0.2} f(\\pi|y = 14) d\\pi\n\n\nWe can find this probability by using the pbeta() function:\n\n\npbeta(0.20, 18, 92)"
  },
  {
    "objectID": "files/lectures/07-2-bayesian-inference.html#posterior-hypothesis-testing-2",
    "href": "files/lectures/07-2-bayesian-inference.html#posterior-hypothesis-testing-2",
    "title": "Posterior Analysis",
    "section": "Posterior Hypothesis Testing",
    "text": "Posterior Hypothesis Testing\n\nWe can find this probability by using the pbeta() function:\n\n\npbeta(0.20, 18, 92)\n\n[1] 0.8489856\n\n\n\nThus,\n\n\nP\\left[ \\pi &lt; 0.2 | Y = 14 \\right] = 0.849\n\n\nThere is approximately an 84.9% posterior chance that Gen Xers account for fewer than 20% of modern art museum artists."
  },
  {
    "objectID": "files/lectures/07-2-bayesian-inference.html#posterior-hypothesis-testing-3",
    "href": "files/lectures/07-2-bayesian-inference.html#posterior-hypothesis-testing-3",
    "title": "Posterior Analysis",
    "section": "Posterior Hypothesis Testing",
    "text": "Posterior Hypothesis Testing\n\nPrior Probability\n\n\nP\\left[ \\pi &lt; 0.2 \\right] = \\int_0^{0.2} f(\\pi) d\\pi\n\n\nWe can find this probability by using the pbeta() function:\n\n\npbeta(0.20, 4, 6)"
  },
  {
    "objectID": "files/lectures/07-2-bayesian-inference.html#posterior-hypothesis-testing-4",
    "href": "files/lectures/07-2-bayesian-inference.html#posterior-hypothesis-testing-4",
    "title": "Posterior Analysis",
    "section": "Posterior Hypothesis Testing",
    "text": "Posterior Hypothesis Testing\n\nWe can find this probability by using the pbeta() function:\n\n\npbeta(0.20, 4, 6)\n\n[1] 0.08564173\n\n\n\nThus,\n\n\nP\\left[ \\pi &lt; 0.2 \\right] = 0.086\n\n\nThere is approximately an 8.6% prior chance that Gen Xers account for fewer than 20% of modern art museum artists."
  },
  {
    "objectID": "files/lectures/07-2-bayesian-inference.html#posterior-hypothesis-testing-5",
    "href": "files/lectures/07-2-bayesian-inference.html#posterior-hypothesis-testing-5",
    "title": "Posterior Analysis",
    "section": "Posterior Hypothesis Testing",
    "text": "Posterior Hypothesis Testing\n\nWe can create a table of information we will use to make inferences:\n\n\n\n\n\n\n\n\nHypotheses\n\n\n\n\n\n\nPrior Probability\n\n\n\n\n\n\nPosterior Probability\n\n\n\n\n\n\nH_0: \\pi \\ge 0.2\n\n\nP[H_0] = 0.914\n\n\nP[H_0 | Y= 14] = 0.151\n\n\n\n\nH_1: \\pi &lt; 0.2\n\n\nP[H_1] = 0.086\n\n\nP[H_1 | Y = 14] = 0.849"
  },
  {
    "objectID": "files/lectures/07-2-bayesian-inference.html#posterior-hypothesis-testing-6",
    "href": "files/lectures/07-2-bayesian-inference.html#posterior-hypothesis-testing-6",
    "title": "Posterior Analysis",
    "section": "Posterior Hypothesis Testing",
    "text": "Posterior Hypothesis Testing\n\nLet’s find the posterior odds in favor of H_1:\n\n\n\n\n\n\n\n\nHypotheses\n\n\n\n\n\n\nPrior Probability\n\n\n\n\n\n\nPosterior Probability\n\n\n\n\n\n\nH_0: \\pi \\ge 0.2\n\n\nP[H_0] = 0.914\n\n\nP[H_0 | Y= 14] = 0.151\n\n\n\n\nH_1: \\pi &lt; 0.2\n\n\nP[H_1] = 0.086\n\n\nP[H_1 | Y = 14] = 0.849\n\n\n\n\n\n\n\\begin{align*}\n\\text{posterior odds} &= \\frac{P\\left[ H_1 | Y = 14 \\right]}{P\\left[ H_0 | Y = 14 \\right]} \\\\\n&= \\frac{0.849}{0.151} \\\\\n&\\approx 5.62\n\\end{align*}\n\n\nOur posterior assessment, after considering data collected, suggests that \\pi is 5.62 times more likely to be below 0.2 rather than being above 0.2."
  },
  {
    "objectID": "files/lectures/07-2-bayesian-inference.html#posterior-hypothesis-testing-7",
    "href": "files/lectures/07-2-bayesian-inference.html#posterior-hypothesis-testing-7",
    "title": "Posterior Analysis",
    "section": "Posterior Hypothesis Testing",
    "text": "Posterior Hypothesis Testing\n\nLet’s find the prior odds in favor of H_1:\n\n\n\n\n\n\n\n\nHypotheses\n\n\n\n\n\n\nPrior Probability\n\n\n\n\n\n\nPosterior Probability\n\n\n\n\n\n\nH_0: \\pi \\ge 0.2\n\n\nP[H_0] = 0.914\n\n\nP[H_0 | Y= 14] = 0.151\n\n\n\n\nH_1: \\pi &lt; 0.2\n\n\nP[H_1] = 0.086\n\n\nP[H_1 | Y = 14] = 0.849\n\n\n\n\n\n\n\\begin{align*}\n\\text{prior odds} &= \\frac{P\\left[ H_1 \\right]}{P\\left[ H_0 \\right]} \\\\\n&= \\frac{0.086}{0.914} \\\\\n&\\approx 0.093\n\\end{align*}\n\n\nOur prior assessment, before considering the data collected, suggests that \\pi is 0.093 times more likely to be below 0.2 rather than being above 0.2."
  },
  {
    "objectID": "files/lectures/07-2-bayesian-inference.html#posterior-hypothesis-testing-8",
    "href": "files/lectures/07-2-bayesian-inference.html#posterior-hypothesis-testing-8",
    "title": "Posterior Analysis",
    "section": "Posterior Hypothesis Testing",
    "text": "Posterior Hypothesis Testing\n\nBayes Factor\n\nWhen we are comparing two competing hypotheses, H_0 vs. H_1, the Bayes Factor is an odds ratio for H_1.\ni.e., the Bayes Factor as a measure of evidence in favor of H_1.\n\n\n\n\\text{Bayes Factor} = \\frac{\\text{posterior odds}}{\\text{prior odds}} = \\frac{P\\left[H_1 | Y\\right] / P\\left[H_0 | Y\\right]}{P\\left[H_1\\right] / P\\left[H_0\\right]}\n\n\nThis will be our alternative to the p-value."
  },
  {
    "objectID": "files/lectures/07-2-bayesian-inference.html#posterior-hypothesis-testing-9",
    "href": "files/lectures/07-2-bayesian-inference.html#posterior-hypothesis-testing-9",
    "title": "Posterior Analysis",
    "section": "Posterior Hypothesis Testing",
    "text": "Posterior Hypothesis Testing\n\nBayes Factor\n\n\n\\text{Bayes Factor} = \\frac{\\text{posterior odds}}{\\text{prior odds}} = \\frac{P\\left[H_1 | Y\\right] / P\\left[H_0 | Y\\right]}{P\\left[H_1\\right] / P\\left[H_0\\right]}\n\n\nWe will compare the BF to 1.\n\nBF = 1: The plausibility of H_1 did not change in light of the observed data.\nBF &gt; 1: The plausibility of H_1 increased in light of the observed data.\n\nThe greater the Bayes Factor, the more convincing the evidence for H_1.\n\nBF &lt; 1: The plausibilty of H_1 decreased in light of the observed data."
  },
  {
    "objectID": "files/lectures/07-2-bayesian-inference.html#posterior-hypothesis-testing-10",
    "href": "files/lectures/07-2-bayesian-inference.html#posterior-hypothesis-testing-10",
    "title": "Posterior Analysis",
    "section": "Posterior Hypothesis Testing",
    "text": "Posterior Hypothesis Testing\n\nLet’s now find the Bayes Factor:\n\n\n\n\n\n\n\n\nHypotheses\n\n\n\n\n\n\nPrior Probability\n\n\n\n\n\n\nPosterior Probability\n\n\n\n\n\n\nH_0: \\pi \\ge 0.2\n\n\nP[H_0] = 0.914\n\n\nP[H_0 | Y= 14] = 0.151\n\n\n\n\nH_1: \\pi &lt; 0.2\n\n\nP[H_1] = 0.086\n\n\nP[H_1 | Y = 14] = 0.849\n\n\n\n\n\n\n\\begin{align*}\n\\text{Bayes Factor} &= \\frac{\\text{posterior odds}}{\\text{prior odds}} = \\frac{P\\left[H_1 | Y\\right] / P\\left[H_0 | Y\\right]}{P\\left[H_1\\right] / P\\left[H_0\\right]} \\\\\n&= \\frac{5.62}{0.09} \\\\\n&\\approx 62.44\n\\end{align*}"
  },
  {
    "objectID": "files/lectures/07-2-bayesian-inference.html#posterior-hypothesis-testing-11",
    "href": "files/lectures/07-2-bayesian-inference.html#posterior-hypothesis-testing-11",
    "title": "Posterior Analysis",
    "section": "Posterior Hypothesis Testing",
    "text": "Posterior Hypothesis Testing\n\nLet’s now find the Bayes Factor:\n\n\n\n\n\n\n\n\nHypotheses\n\n\n\n\n\n\nPrior Probability\n\n\n\n\n\n\nPosterior Probability\n\n\n\n\n\n\nH_0: \\pi \\ge 0.2\n\n\nP[H_0] = 0.914\n\n\nP[H_0 | Y= 14] = 0.151\n\n\n\n\nH_1: \\pi &lt; 0.2\n\n\nP[H_1] = 0.086\n\n\nP[H_1 | Y = 14] = 0.849\n\n\n\n\n\n\n\nprior_odds &lt;- pbeta(0.20, 4, 6)/(1-pbeta(0.20, 4, 6))\npost_odds &lt;- pbeta(0.20, 18, 92)/(1-pbeta(0.20, 18, 92))\n(BF &lt;- post_odds/prior_odds)\n\n[1] 60.02232"
  },
  {
    "objectID": "files/lectures/07-2-bayesian-inference.html#posterior-hypothesis-testing-12",
    "href": "files/lectures/07-2-bayesian-inference.html#posterior-hypothesis-testing-12",
    "title": "Posterior Analysis",
    "section": "Posterior Hypothesis Testing",
    "text": "Posterior Hypothesis Testing\n\nIt’s time to draw a conclusion!\n\nPosterior probability: 0.85\nBayes factor: 60\n\nWe have fairly convincing evidence in factor of the claim that fewer than 20% of artists at major modern art museums are Gen X or younger.\nThis gives us more information - we have a holistic measure of the level of uncertainty about the claim.\n\nThis should help us inform our next steps."
  },
  {
    "objectID": "files/lectures/07-2-bayesian-inference.html#posterior-hypothesis-testing-13",
    "href": "files/lectures/07-2-bayesian-inference.html#posterior-hypothesis-testing-13",
    "title": "Posterior Analysis",
    "section": "Posterior Hypothesis Testing",
    "text": "Posterior Hypothesis Testing\n\nWe now want to test whether or not 30% of major museum artists are Gen X or younger.\n\n\n\\begin{align*}\nH_0&: \\ \\pi = 0.3 \\\\\nH_1&: \\ \\pi \\ne 0.3\n\\end{align*}\n\n\nWhy is this an issue?"
  },
  {
    "objectID": "files/lectures/07-2-bayesian-inference.html#posterior-hypothesis-testing-14",
    "href": "files/lectures/07-2-bayesian-inference.html#posterior-hypothesis-testing-14",
    "title": "Posterior Analysis",
    "section": "Posterior Hypothesis Testing",
    "text": "Posterior Hypothesis Testing\n\nWe now want to test whether or not 30% of major museum artists are Gen X or younger.\n\n\n\\begin{align*}\nH_0&: \\ \\pi = 0.3 \\\\\nH_1&: \\ \\pi \\ne 0.3\n\\end{align*}\n\n\nWhy is this an issue? The posterior probability of a point hypothesis is always 0!\n\n\nP\\left[ \\pi =0.3 | Y = 14 \\right] = \\int_{0.3}^{0.3} f(\\pi|y = 14) d\\pi = 0"
  },
  {
    "objectID": "files/lectures/07-2-bayesian-inference.html#posterior-hypothesis-testing-15",
    "href": "files/lectures/07-2-bayesian-inference.html#posterior-hypothesis-testing-15",
    "title": "Posterior Analysis",
    "section": "Posterior Hypothesis Testing",
    "text": "Posterior Hypothesis Testing\n\nThe posterior probability of a point hypothesis is always 0!\n\n\nP\\left[ \\pi =0.3 | Y = 14 \\right] = \\int_{0.3}^{0.3} f(\\pi|y = 14) d\\pi = 0\n\n\n…. meaning:\n\n\n\\text{posterior odds}  = \\frac{P\\left[ H_1 | Y = 14 \\right]}{P\\left[ H_0 | Y = 14 \\right]} = \\frac{1}{0}"
  },
  {
    "objectID": "files/lectures/07-2-bayesian-inference.html#posterior-hypothesis-testing-16",
    "href": "files/lectures/07-2-bayesian-inference.html#posterior-hypothesis-testing-16",
    "title": "Posterior Analysis",
    "section": "Posterior Hypothesis Testing",
    "text": "Posterior Hypothesis Testing\n\nWelp.\nLet’s think about the 95% posterior credible interval for \\pi: (0.10, 0.24).\n\nDo we think that 0.3 is a plausible value?"
  },
  {
    "objectID": "files/lectures/07-2-bayesian-inference.html#posterior-hypothesis-testing-17",
    "href": "files/lectures/07-2-bayesian-inference.html#posterior-hypothesis-testing-17",
    "title": "Posterior Analysis",
    "section": "Posterior Hypothesis Testing",
    "text": "Posterior Hypothesis Testing\n\nLet’s reframe our hypotheses:\n\n\n\\begin{align*}\nH_0&: \\ \\pi \\in (0.25, 0.35) \\\\\nH_1&: \\ \\pi \\not\\in (0.25, 0.35)\n\\end{align*}\n\n\nNow, we can more rigorously claim belief in H_1.\n\nThe entire hypothesized range is above the 95% CI.\n\nThis also allows us a way to construct our hypothesis test with posterior probability and Bayes Factor."
  },
  {
    "objectID": "files/lectures/07-2-bayesian-inference.html#posterior-prediction",
    "href": "files/lectures/07-2-bayesian-inference.html#posterior-prediction",
    "title": "Posterior Analysis",
    "section": "Posterior Prediction",
    "text": "Posterior Prediction\n\nIn addition to estimating the posterior and using the posterior distribution for hypothesis testing, we may be interested in predicting the outcome in a new dataset.\nSuppose we get our hands on data for 20 more artworks displayed at the museum.\nBased on the posterior understanding of \\pi that we’ve developed throughout this chapter, what number would you predict are done by artists that are Gen X or younger?"
  },
  {
    "objectID": "files/lectures/07-2-bayesian-inference.html#posterior-prediction-1",
    "href": "files/lectures/07-2-bayesian-inference.html#posterior-prediction-1",
    "title": "Posterior Analysis",
    "section": "Posterior Prediction",
    "text": "Posterior Prediction\n\nSuppose we get our hands on data for 20 more artworks displayed at the museum.\nBased on the posterior understanding of \\pi that we’ve developed throughout this chapter, what number would you predict are done by artists that are Gen X or younger?\n\nLogical response:\n\nposterior guess was 16%\nn = 20\nn \\hat{\\pi} = 20(0.16) = 3.2"
  },
  {
    "objectID": "files/lectures/07-2-bayesian-inference.html#posterior-prediction-2",
    "href": "files/lectures/07-2-bayesian-inference.html#posterior-prediction-2",
    "title": "Posterior Analysis",
    "section": "Posterior Prediction",
    "text": "Posterior Prediction\n\nSuppose we get our hands on data for 20 more artworks displayed at the museum. What number would you predict are done by artists that are Gen X or younger?\n\nTwo sources of variability in prediction:\n\nSampling variability: we do not expect n \\hat{\\pi} to be an integer.\nPosterior variability in \\pi: we know that 0.16 is not the only plausible \\pi.\n\n95% credible interval for \\pi: (0.10, 0.24)\nWhat do we expect to see under each possible \\pi?"
  },
  {
    "objectID": "files/lectures/07-2-bayesian-inference.html#posterior-prediction-3",
    "href": "files/lectures/07-2-bayesian-inference.html#posterior-prediction-3",
    "title": "Posterior Analysis",
    "section": "Posterior Prediction",
    "text": "Posterior Prediction\n\nLet’s look at combining the sampling variability in our new Y and posterior variability in \\pi.\n\nLet Y' = y' be the (unknown) number of the 20 new artwork that are done by Gen X or younger artists.\n\ny' \\in \\{0, 1, ..., 20\\}.\n\nConditioned on \\pi, the sampling variability in Y' can be modeled\n\n\n\nY'|\\pi \\sim \\text{Bin}(20, \\pi)\n\n\nf(y'|\\pi) = P[Y' = y'|\\pi] = {20\\choose{y'}} \\pi^{y'} (1-\\pi)^{20-y'}"
  },
  {
    "objectID": "files/lectures/07-2-bayesian-inference.html#posterior-prediction-4",
    "href": "files/lectures/07-2-bayesian-inference.html#posterior-prediction-4",
    "title": "Posterior Analysis",
    "section": "Posterior Prediction",
    "text": "Posterior Prediction\n\nWe can weight f(y'|\\pi) by the posterior pdf, f(\\pi|y=14).\n\nThis captures the chance of observing Y' = y' Gen Xers for a given \\pi\nAt the same time, this accounts for the posterior plausibility of \\pi.\n\n\nf(y'|\\pi) f(\\pi|y=14)"
  },
  {
    "objectID": "files/lectures/07-2-bayesian-inference.html#posterior-prediction-5",
    "href": "files/lectures/07-2-bayesian-inference.html#posterior-prediction-5",
    "title": "Posterior Analysis",
    "section": "Posterior Prediction",
    "text": "Posterior Prediction\n\nThis leads us to the posterior predictive model of Y'.\n\n\nf(y'|y) = \\int f(y'|\\pi) f(\\pi|y) \\ d \\pi\n\n\nThe overall chance of observing Y' = y' weights the chance of observing this outcome under any possible \\pi by the posterior plausibility of \\pi.\n\nChance of observing this outcome under any \\pi: f(y'|\\pi).\nPosterior plausibility of \\pi: f(\\pi|y)."
  },
  {
    "objectID": "files/lectures/07-2-bayesian-inference.html#posterior-prediction-6",
    "href": "files/lectures/07-2-bayesian-inference.html#posterior-prediction-6",
    "title": "Posterior Analysis",
    "section": "Posterior Prediction",
    "text": "Posterior Prediction\n\n# STEP 1: DEFINE the model\nart_model &lt;- \"\n  data {\n    int&lt;lower = 0, upper = 100&gt; Y;\n  }\n  parameters {\n    real&lt;lower = 0, upper = 1&gt; pi;\n  }\n  model {\n    Y ~ binomial(100, pi);\n    pi ~ beta(4, 6);\n  }\n\""
  },
  {
    "objectID": "files/lectures/07-2-bayesian-inference.html#posterior-prediction-7",
    "href": "files/lectures/07-2-bayesian-inference.html#posterior-prediction-7",
    "title": "Posterior Analysis",
    "section": "Posterior Prediction",
    "text": "Posterior Prediction\n\n# STEP 2: SIMULATE the posterior\nart_sim &lt;- stan(model_code = art_model, \n                data = list(Y = 14),  \n                chains = 4, \n                iter = 5000*2, \n                seed = 84735)\n\n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 3e-06 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.03 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 1: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 1: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 1: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 1: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 1: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 1: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 1: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 1: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 1: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 1: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 1: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.011 seconds (Warm-up)\nChain 1:                0.012 seconds (Sampling)\nChain 1:                0.023 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 0 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 2: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 2: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 2: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 2: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 2: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 2: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 2: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 2: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 2: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 2: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 2: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.012 seconds (Warm-up)\nChain 2:                0.013 seconds (Sampling)\nChain 2:                0.025 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 0 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 3: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 3: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 3: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 3: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 3: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 3: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 3: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 3: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 3: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 3: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 3: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 0.012 seconds (Warm-up)\nChain 3:                0.013 seconds (Sampling)\nChain 3:                0.025 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 0 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 4: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 4: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 4: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 4: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 4: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 4: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 4: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 4: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 4: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 4: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 4: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 0.012 seconds (Warm-up)\nChain 4:                0.012 seconds (Sampling)\nChain 4:                0.024 seconds (Total)\nChain 4:"
  },
  {
    "objectID": "files/lectures/07-2-bayesian-inference.html#posterior-prediction-8",
    "href": "files/lectures/07-2-bayesian-inference.html#posterior-prediction-8",
    "title": "Posterior Analysis",
    "section": "Posterior Prediction",
    "text": "Posterior Prediction\n\n\n# Parallel trace plot\nmcmc_trace(art_sim, pars = \"pi\", size = 0.5) + xlab(\"iteration\")"
  },
  {
    "objectID": "files/lectures/07-2-bayesian-inference.html#posterior-prediction-9",
    "href": "files/lectures/07-2-bayesian-inference.html#posterior-prediction-9",
    "title": "Posterior Analysis",
    "section": "Posterior Prediction",
    "text": "Posterior Prediction\n\n\n# Density plot\nmcmc_dens_overlay(art_sim, pars = \"pi\")"
  },
  {
    "objectID": "files/lectures/07-2-bayesian-inference.html#posterior-prediction-10",
    "href": "files/lectures/07-2-bayesian-inference.html#posterior-prediction-10",
    "title": "Posterior Analysis",
    "section": "Posterior Prediction",
    "text": "Posterior Prediction\n\n\n# Autocorrelation plot\nmcmc_acf(art_sim, pars = \"pi\")"
  },
  {
    "objectID": "files/lectures/07-2-bayesian-inference.html#posterior-prediction-11",
    "href": "files/lectures/07-2-bayesian-inference.html#posterior-prediction-11",
    "title": "Posterior Analysis",
    "section": "Posterior Prediction",
    "text": "Posterior Prediction\n\nAs we saw previously, the posterior was Beta(18, 92).\nWe will use the tidy() function from the broom.mixed package.\n\n\nlibrary(broom.mixed)\ntidy(art_sim, conf.int = TRUE, conf.level = 0.95)\n\n\n  \n\n\n\n\nThe approximate middle 95% CI for \\pi is (0.100, 0.239).\nOur approximation of the actual posterior median is 0.162."
  },
  {
    "objectID": "files/lectures/07-2-bayesian-inference.html#posterior-prediction-12",
    "href": "files/lectures/07-2-bayesian-inference.html#posterior-prediction-12",
    "title": "Posterior Analysis",
    "section": "Posterior Prediction",
    "text": "Posterior Prediction\n\nWe can use the mcmc_areas() function from the bayesrules package to get a corresponding graph,\n\n\n\nmcmc_areas(art_sim, pars = \"pi\", prob = 0.95)"
  },
  {
    "objectID": "files/lectures/07-2-bayesian-inference.html#posterior-simulation",
    "href": "files/lectures/07-2-bayesian-inference.html#posterior-simulation",
    "title": "Posterior Analysis",
    "section": "Posterior Simulation",
    "text": "Posterior Simulation\n\nUnfortunately, tidy() does not give everything we may be interested in.\n\nWe can save the Markov chain values to a dataset and analyze.\n\n\n\n# Store the 4 chains in 1 data frame\nart_chains_df &lt;- as.data.frame(art_sim, pars = \"lp__\", include = FALSE)\ndim(art_chains_df)\n\n[1] 20000     1\n\nhead(art_chains_df, n=3)"
  },
  {
    "objectID": "files/lectures/07-2-bayesian-inference.html#posterior-simulation-1",
    "href": "files/lectures/07-2-bayesian-inference.html#posterior-simulation-1",
    "title": "Posterior Analysis",
    "section": "Posterior Simulation",
    "text": "Posterior Simulation\n\nWe can then summarize the Markov chain values,\n\n\nart_chains_df %&gt;% \n  summarize(post_mean = mean(pi), \n            post_median = median(pi),\n            post_mode = sample_mode(pi),\n            lower_95 = quantile(pi, 0.025),\n            upper_95 = quantile(pi, 0.975))\n\n\n  \n\n\n\n\nWe have reproduced/verified the results from tidy() (and then some!)"
  },
  {
    "objectID": "files/lectures/07-2-bayesian-inference.html#posterior-simulation-2",
    "href": "files/lectures/07-2-bayesian-inference.html#posterior-simulation-2",
    "title": "Posterior Analysis",
    "section": "Posterior Simulation",
    "text": "Posterior Simulation\n\nNow that we have saved the Markov chain values, we can use them to answer questions about the data.\nRecall, we were interested in testing the claim that fewer than 20% of major museum artists are Gen X.\n\n\nart_chains_df %&gt;% \n  mutate(exceeds = pi &lt; 0.20) %&gt;% \n  tabyl(exceeds)\n\n\n  \n\n\n\n\nBy the posterior, there is an 84.6% chance that Gen X artist representation is under 20%."
  },
  {
    "objectID": "files/lectures/07-2-bayesian-inference.html#posterior-simulation-3",
    "href": "files/lectures/07-2-bayesian-inference.html#posterior-simulation-3",
    "title": "Posterior Analysis",
    "section": "Posterior Simulation",
    "text": "Posterior Simulation\n\nLet us compare the results between using conjugate family knowledge and MCMC.\n\n\n\n\n\nFrom this, we can see that MCMC gave us an accurate approximation.\nWe should use this as “proof” that the approximations are “reliable” for non-conjugate families.\n\nAlways look at diagnostics!"
  },
  {
    "objectID": "files/lectures/07-2-bayesian-inference.html#homework-practice",
    "href": "files/lectures/07-2-bayesian-inference.html#homework-practice",
    "title": "Posterior Analysis",
    "section": "Homework / Practice",
    "text": "Homework / Practice\n\nFrom the Bayes Rules! textbook:\n\n8.4\n8.8\n8.9\n8.10\n8.14\n8.15\n8.16"
  },
  {
    "objectID": "files/lectures/10-2-modeling-poisson.html#introduction",
    "href": "files/lectures/10-2-modeling-poisson.html#introduction",
    "title": "Poisson and Negative Binomial Regressions",
    "section": "Introduction",
    "text": "Introduction\n\nContinuous data: Linear\n\n\n\\hat{y_i} = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_{i1} + \\hat{\\beta}_2 x_{i2} + ... + \\hat{\\beta}_k x_{ik}\n\n\nCategorical data: Logistic\n\n\n\\ln\\left(\\frac{\\hat{\\pi_i}}{1-\\hat{\\pi_i}}\\right) = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_{i1} + \\hat{\\beta}_2 x_{i2} + ... + \\hat{\\beta}_k x_{ik}\n\n\nCount data: Poisson/negative binomial\n\n\n\\ln(\\hat{y}_i) = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_{i1} + \\hat{\\beta}_2 x_{i2} + ... + \\hat{\\beta}_k x_{ik}"
  },
  {
    "objectID": "files/lectures/10-2-modeling-poisson.html#poisson-regression",
    "href": "files/lectures/10-2-modeling-poisson.html#poisson-regression",
    "title": "Poisson and Negative Binomial Regressions",
    "section": "Poisson Regression",
    "text": "Poisson Regression\n\nGeneric code for Poisson regression:\n\n\nposterior_model &lt;- stan_glm(y ~ x1 + x2 + ... + xk, \n                            data = dataset_name, \n                            family = poisson,\n                            prior_intercept = normal(0, 1, autoscale = TRUE),\n                            prior = normal(0, 1, autoscale = TRUE), \n                            chains = 4, iter = 5000*2, seed = 84735, \n                            prior_PD = FALSE)"
  },
  {
    "objectID": "files/lectures/10-2-modeling-poisson.html#example-set-up",
    "href": "files/lectures/10-2-modeling-poisson.html#example-set-up",
    "title": "Poisson and Negative Binomial Regressions",
    "section": "Example: Set Up",
    "text": "Example: Set Up\n\nAfter a few successful rescue missions, Mickey Mouse and his friends noticed something strange: every time they set out on a new mission, the number of alert beacons that went off around the clubhouse varied. Sometimes the team moved silently, but other times the entire operation echoed with crashes, clanks, and Donald’s famous quacking. To understand what’s going on, the team started tracking three pieces of information for each mission:\n\nalerts_triggered: the number of alert beacons that went off during the mission (possible values: 0, 1, 2, 3, …).\ncharacter: which character led the mission (Mickey, Minnie, Donald, or Goofy).\nstealth_score: how carefully the team tried to move on a scale from 0 to 10, where higher values mean a quieter, more careful mission."
  },
  {
    "objectID": "files/lectures/10-2-modeling-poisson.html#example-poisson",
    "href": "files/lectures/10-2-modeling-poisson.html#example-poisson",
    "title": "Poisson and Negative Binomial Regressions",
    "section": "Example: Poisson",
    "text": "Example: Poisson\n\npoisson_posterior &lt;- stan_glm(alerts_triggered ~ character + stealth_score, \n                            data = friends, \n                            family = poisson,\n                            prior_intercept = normal(0, 1, autoscale = TRUE),\n                            prior = normal(0, 1, autoscale = TRUE), \n                            chains = 4, iter = 5000*2, seed = 84735, \n                            prior_PD = FALSE)\n\n\nSAMPLING FOR MODEL 'count' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 3.1e-05 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.31 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 1: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 1: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 1: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 1: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 1: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 1: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 1: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 1: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 1: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 1: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 1: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.151 seconds (Warm-up)\nChain 1:                0.171 seconds (Sampling)\nChain 1:                0.322 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'count' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 9e-06 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.09 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 2: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 2: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 2: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 2: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 2: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 2: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 2: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 2: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 2: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 2: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 2: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.149 seconds (Warm-up)\nChain 2:                0.181 seconds (Sampling)\nChain 2:                0.33 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'count' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 6e-06 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.06 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 3: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 3: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 3: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 3: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 3: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 3: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 3: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 3: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 3: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 3: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 3: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 0.151 seconds (Warm-up)\nChain 3:                0.172 seconds (Sampling)\nChain 3:                0.323 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'count' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 1.2e-05 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.12 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 4: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 4: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 4: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 4: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 4: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 4: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 4: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 4: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 4: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 4: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 4: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 0.155 seconds (Warm-up)\nChain 4:                0.185 seconds (Sampling)\nChain 4:                0.34 seconds (Total)\nChain 4:"
  },
  {
    "objectID": "files/lectures/10-2-modeling-poisson.html#example-poisson-1",
    "href": "files/lectures/10-2-modeling-poisson.html#example-poisson-1",
    "title": "Poisson and Negative Binomial Regressions",
    "section": "Example: Poisson",
    "text": "Example: Poisson\n\nExamining the results,\n\n\ntidy(poisson_posterior, conf.int = TRUE, exponentiate = FALSE)\n\n\n  \n\n\n\n\n\\ln(\\hat{y}) = 1.48 + 0.51 \\text{ Donald} + 0.26 \\text{ Goofy} - 0.58 \\text{ Minnie} - 0.24 \\text{ stealth}"
  },
  {
    "objectID": "files/lectures/10-2-modeling-poisson.html#example-poisson-2",
    "href": "files/lectures/10-2-modeling-poisson.html#example-poisson-2",
    "title": "Poisson and Negative Binomial Regressions",
    "section": "Example: Poisson",
    "text": "Example: Poisson\n\nWe are interested in the multiplicative effect on the expected count, so we exponentiate the coefficients and credible intervals:\n\n\ntidy(poisson_posterior, conf.int = TRUE, exponentiate = TRUE)"
  },
  {
    "objectID": "files/lectures/10-2-modeling-poisson.html#example-poisson-3",
    "href": "files/lectures/10-2-modeling-poisson.html#example-poisson-3",
    "title": "Poisson and Negative Binomial Regressions",
    "section": "Example: Poisson",
    "text": "Example: Poisson\n\nHow would I report the results in a table?\n\n\n\n\nPredictor\nIRR (95% CI)\n\n\n\n\nDonald\n1.67 (1.21, 2.30)\n\n\nGoofy\n1.30 (0.95, 1.80)\n\n\nMinnie\n0.56 (0.36, 0.85)\n\n\nStealth\n0.79 (0.75, 0.84)"
  },
  {
    "objectID": "files/lectures/10-2-modeling-poisson.html#example-poisson-4",
    "href": "files/lectures/10-2-modeling-poisson.html#example-poisson-4",
    "title": "Poisson and Negative Binomial Regressions",
    "section": "Example: Poisson",
    "text": "Example: Poisson\n\nIncident Rate Ratios (IRR): multiplicative effect on the expected count for a one unit increase in the predictor, holding other predictors constant.\nAs compared to Mickey,\n\nDonald is expected to trigger 1.67 times as many alerts (67% more).\nGoofy is expected to trigger 1.30 times as many alerts (30% more).\nMinnie is expected to trigger 0.56 times as many alerts (44% fewer).\n\nAs stealth score increases, the expected number of alerts triggered decreases.\n\nFor a one-unit increase in stealth-score, the expected number of alerts triggered decreases by 21%."
  },
  {
    "objectID": "files/lectures/10-2-modeling-poisson.html#example-poisson-5",
    "href": "files/lectures/10-2-modeling-poisson.html#example-poisson-5",
    "title": "Poisson and Negative Binomial Regressions",
    "section": "Example: Poisson",
    "text": "Example: Poisson\n\nTo visualize our model, we first find predicted values,\n\n\nfriends &lt;- friends %&gt;%\n  mutate(p_mickey = exp(1.48-0.235*stealth_score),\n         p_donald = exp(1.48+0.51-0.235*stealth_score),\n         p_goofy  = exp(1.48+0.26-0.235*stealth_score),\n         p_minnie = exp(1.48-0.58-0.235*stealth_score))\n\n\nThen, construct the ggplot,\n\n\nfriends %&gt;% ggplot(aes(x = stealth_score, y = alerts_triggered)) +\n  geom_point(size = 3) +\n  geom_line(aes(y = p_mickey, color = \"Mickey\")) +\n  geom_line(aes(y = p_donald, color = \"Donald\")) +\n  geom_line(aes(y = p_goofy,  color = \"Goofy\")) +\n  geom_line(aes(y = p_minnie, color = \"Minnie\")) +\n  labs(x = \"Stealth Score\",\n       y = \"Expected Number of Alerts Triggered\",\n       color = \"Character\") +\n  theme_bw()"
  },
  {
    "objectID": "files/lectures/10-2-modeling-poisson.html#example-poisson-6",
    "href": "files/lectures/10-2-modeling-poisson.html#example-poisson-6",
    "title": "Poisson and Negative Binomial Regressions",
    "section": "Example: Poisson",
    "text": "Example: Poisson"
  },
  {
    "objectID": "files/lectures/10-2-modeling-poisson.html#negative-binomial-regression",
    "href": "files/lectures/10-2-modeling-poisson.html#negative-binomial-regression",
    "title": "Poisson and Negative Binomial Regressions",
    "section": "Negative Binomial Regression",
    "text": "Negative Binomial Regression\n\nGeneric code for negative binomial regression:\n\n\nposterior_model &lt;- stan_glm(y ~ x1 + x2 + ... + xk, \n                            data = dataset_name, \n                            family = neg_binomial_2,\n                            prior_intercept = normal(0, 1, autoscale = TRUE),\n                            prior = normal(0, 1, autoscale = TRUE), \n                            chains = 4, iter = 5000*2, seed = 84735, \n                            prior_PD = FALSE)"
  },
  {
    "objectID": "files/lectures/10-2-modeling-poisson.html#example-set-up-1",
    "href": "files/lectures/10-2-modeling-poisson.html#example-set-up-1",
    "title": "Poisson and Negative Binomial Regressions",
    "section": "Example: Set Up",
    "text": "Example: Set Up\n\nAfter weeks of studying the clubhouse rescue missions, Mickey and friends have noticed a problem: some missions are wildly unpredictable. When Donald Duck takes charge, the number of alert beacons that go off during a mission varies dramatically. On some days, his missions are surprisingly quiet. On others, Donald’s shouting and tripping set off nearly every beacon in the clubhouse!\n\n\nfriends2 %&gt;% summarize(mean(alerts_triggered),\n                       var(alerts_triggered))\n\n\n  \n\n\n\n\nWe can see that the mean and variance do not meet the assumption for the Poisson distribution."
  },
  {
    "objectID": "files/lectures/10-2-modeling-poisson.html#example-negative-binomial",
    "href": "files/lectures/10-2-modeling-poisson.html#example-negative-binomial",
    "title": "Poisson and Negative Binomial Regressions",
    "section": "Example: Negative Binomial",
    "text": "Example: Negative Binomial\n\nnegbin_posterior &lt;- stan_glm(alerts_triggered ~ character + stealth_score, \n                             data = friends2, \n                             family = neg_binomial_2,\n                             prior_intercept = normal(0, 1, autoscale = TRUE),\n                             prior = normal(0, 1, autoscale = TRUE), \n                             chains = 4, iter = 5000*2, seed = 84735, \n                             prior_PD = FALSE)\n\n\nSAMPLING FOR MODEL 'count' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 3.3e-05 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.33 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 1: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 1: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 1: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 1: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 1: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 1: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 1: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 1: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 1: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 1: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 1: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.319 seconds (Warm-up)\nChain 1:                0.458 seconds (Sampling)\nChain 1:                0.777 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'count' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 1.4e-05 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.14 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 2: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 2: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 2: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 2: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 2: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 2: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 2: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 2: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 2: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 2: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 2: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.311 seconds (Warm-up)\nChain 2:                0.367 seconds (Sampling)\nChain 2:                0.678 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'count' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 1.1e-05 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.11 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 3: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 3: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 3: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 3: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 3: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 3: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 3: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 3: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 3: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 3: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 3: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 0.317 seconds (Warm-up)\nChain 3:                0.406 seconds (Sampling)\nChain 3:                0.723 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'count' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 1.5e-05 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.15 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 4: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 4: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 4: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 4: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 4: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 4: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 4: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 4: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 4: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 4: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 4: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 0.302 seconds (Warm-up)\nChain 4:                0.363 seconds (Sampling)\nChain 4:                0.665 seconds (Total)\nChain 4:"
  },
  {
    "objectID": "files/lectures/10-2-modeling-poisson.html#example-negative-binomial-1",
    "href": "files/lectures/10-2-modeling-poisson.html#example-negative-binomial-1",
    "title": "Poisson and Negative Binomial Regressions",
    "section": "Example: Negative Binomial",
    "text": "Example: Negative Binomial\n\ntidy(negbin_posterior, conf.int = TRUE, exponentiate = FALSE)\n\n\n  \n\n\n\n\n\\ln(\\hat{y}) = 1.39 - 0.10 \\text{ Donald} + 0.08 \\text{ Goofy} - 1.20 \\text{ Minnie} - 0.15 \\text{ stealth}"
  },
  {
    "objectID": "files/lectures/10-2-modeling-poisson.html#example-negative-binomial-2",
    "href": "files/lectures/10-2-modeling-poisson.html#example-negative-binomial-2",
    "title": "Poisson and Negative Binomial Regressions",
    "section": "Example: Negative Binomial",
    "text": "Example: Negative Binomial\n\nWe are interested in the multiplicative effect on the expected count, so we exponentiate the coefficients and credible intervals:\n\n\ntidy(negbin_posterior, conf.int = TRUE, exponentiate = TRUE)"
  },
  {
    "objectID": "files/lectures/10-2-modeling-poisson.html#example-negative-binomial-3",
    "href": "files/lectures/10-2-modeling-poisson.html#example-negative-binomial-3",
    "title": "Poisson and Negative Binomial Regressions",
    "section": "Example: Negative Binomial",
    "text": "Example: Negative Binomial\n\nHow would I report the results in a table?\n\n\n\n\nPredictor\nIRR (95% CI)\n\n\n\n\nDonald\n0.91 (0.60, 1.37)\n\n\nGoofy\n1.08 (0.71, 1.63)\n\n\nMinnie\n0.30 (0.18, 0.48)\n\n\nStealth\n0.86 (0.80, 0.93)"
  },
  {
    "objectID": "files/lectures/10-2-modeling-poisson.html#example-negative-binomial-4",
    "href": "files/lectures/10-2-modeling-poisson.html#example-negative-binomial-4",
    "title": "Poisson and Negative Binomial Regressions",
    "section": "Example: Negative Binomial",
    "text": "Example: Negative Binomial\n\nIncident Rate Ratios (IRR): multiplicative effect on the expected count for a one unit increase in the predictor, holding other predictors constant.\nAs compared to Mickey,\n\nDonald is expected to trigger 0.91 times as many alerts (9% fewer).\nGoofy is expected to trigger 1.08 times as many alerts (8% increase).\nMinnie is expected to trigger 0.30 times as many alerts (70% fewer).\n\nAs stealth score increases, the expected number of alerts triggered decreases.\n\nFor a one-unit increase in stealth-score, the expected number of alerts triggered decreases by 14%."
  },
  {
    "objectID": "files/lectures/10-2-modeling-poisson.html#example-negative-binomial-5",
    "href": "files/lectures/10-2-modeling-poisson.html#example-negative-binomial-5",
    "title": "Poisson and Negative Binomial Regressions",
    "section": "Example: Negative Binomial",
    "text": "Example: Negative Binomial\n\nTo visualize our model, we first find predicted values,\n\n() = 1.39 - 0.10 + 0.08 - 1.20 - 0.15 \n\nfriends2 &lt;- friends2 %&gt;%\n  mutate(p_mickey = exp(1.39 - 0.15*stealth_score),\n         p_donald = exp(1.39 - 0.10 -0.15*stealth_score),\n         p_goofy  = exp(1.39 + 0.08 -0.15*stealth_score),\n         p_minnie = exp(1.39 - 1.20 -0.15*stealth_score))\n\n\nThen, construct the ggplot,\n\n\nfriends2 %&gt;% ggplot(aes(x = stealth_score, y = alerts_triggered)) +\n  geom_point(size = 3) +\n  geom_line(aes(y = p_mickey, color = \"Mickey\")) +\n  geom_line(aes(y = p_donald, color = \"Donald\")) +\n  geom_line(aes(y = p_goofy,  color = \"Goofy\")) +\n  geom_line(aes(y = p_minnie, color = \"Minnie\")) +\n  labs(x = \"Stealth Score\",\n       y = \"Expected Number of Alerts Triggered\",\n       color = \"Character\") +\n  theme_bw()"
  },
  {
    "objectID": "files/lectures/10-2-modeling-poisson.html#example-negative-binomial-6",
    "href": "files/lectures/10-2-modeling-poisson.html#example-negative-binomial-6",
    "title": "Poisson and Negative Binomial Regressions",
    "section": "Example: Negative Binomial",
    "text": "Example: Negative Binomial"
  },
  {
    "objectID": "files/lectures/10-2-modeling-poisson.html#wrap-up",
    "href": "files/lectures/10-2-modeling-poisson.html#wrap-up",
    "title": "Poisson and Negative Binomial Regressions",
    "section": "Wrap Up",
    "text": "Wrap Up\n\nPoisson and negative binomial regressions are used to model count data.\n\nPoisson: mean = variance.\nNegative binomial: variance &gt; mean (overdispersion).\n\nCoefficients are interpreted as log-incident rate ratios (log-IRR).\n\nExponentiating coefficients gives incident rate ratios (IRR).\n\nRest of lecture:\n\nHow to “read” subject-matter articles to extract information for analysis.\n\nWorksheet due Tuesday next week.\n\n\nNext week:\n\nMonday: Assignment 3 (Regression Analysis)\nWednesday: Meet with EES collaborator (no formal class). Discuss methodology and their research objective(s)."
  },
  {
    "objectID": "files/lectures/10-2-modeling-poisson.html#practice-homework",
    "href": "files/lectures/10-2-modeling-poisson.html#practice-homework",
    "title": "Poisson and Negative Binomial Regressions",
    "section": "Practice / Homework",
    "text": "Practice / Homework\n\nFrom the Bayes Rules! textbook:\n\n12.5, 12.6, 12.7, 12.8, 12.9"
  },
  {
    "objectID": "files/lectures/05-2-balance-squentiality.html#introduction",
    "href": "files/lectures/05-2-balance-squentiality.html#introduction",
    "title": "Balance and Sequentiality in Bayesian Analysis",
    "section": "Introduction",
    "text": "Introduction\n\nOn Monday, we talked about the Beta-Binomial model for binary outcomes with an unknown probability of success, \\pi.\nWe will now discuss sequentality in Bayesian analyses.\nWorking example:\n\nIn Alison Bechdel’s 1985 comic strip The Rule, a character states that they only see a movie if it satisfies the following three rules (Bechdel 1986):\n\nthe movie has to have at least two women in it;\nthese two women talk to each other; and\nthey talk about something besides a man.\n\nThinking of movies you’ve watched, what percentage of all recent movies do you think pass the Bechdel test? Is it closer to 10%, 50%, 80%, or 100%?"
  },
  {
    "objectID": "files/lectures/05-2-balance-squentiality.html#introduction-example",
    "href": "files/lectures/05-2-balance-squentiality.html#introduction-example",
    "title": "Balance and Sequentiality in Bayesian Analysis",
    "section": "Introduction: Example",
    "text": "Introduction: Example\n\nLet \\pi, a random value between 0 and 1, denote the unknown proportion of recent movies that pass the Bechdel test.\nThree friends (feminist, clueless, and optimist) have some prior ideas about \\pi.\n\nReflecting upon movies that he has seen in the past, the feminist understands that the majority lack strong women characters.\nThe clueless doesn’t really recall the movies they’ve seen, and so are unsure whether passing the Bechdel test is common or uncommon.\nLastly, the optimist thinks that the Bechdel test is a really low bar for the representation of women in film, and thus assumes almost all movies pass the test."
  },
  {
    "objectID": "files/lectures/05-2-balance-squentiality.html#introduction-example-1",
    "href": "files/lectures/05-2-balance-squentiality.html#introduction-example-1",
    "title": "Balance and Sequentiality in Bayesian Analysis",
    "section": "Introduction: Example",
    "text": "Introduction: Example\n\nGraph the following priors:\n\n\nplot_beta(alpha = 1, beta = 1)\nplot_beta(alpha = 5, beta = 11)\nplot_beta(alpha = 14, beta = 1)\n\n\nWhich prior belongs to each friend?\n\nReflecting upon movies that he has seen in the past, the feminist understands that the majority lack strong women characters.\nThe clueless doesn’t really recall the movies they’ve seen, and so are unsure whether passing the Bechdel test is common or uncommon.\nLastly, the optimist thinks that the Bechdel test is a really low bar for the representation of women in film, and thus assumes almost all movies pass the test."
  },
  {
    "objectID": "files/lectures/05-2-balance-squentiality.html#introduction-example-2",
    "href": "files/lectures/05-2-balance-squentiality.html#introduction-example-2",
    "title": "Balance and Sequentiality in Bayesian Analysis",
    "section": "Introduction: Example",
    "text": "Introduction: Example\n\nThe clueless doesn’t really recall the movies they’ve seen, and so are unsure whether passing the Bechdel test is common or uncommon."
  },
  {
    "objectID": "files/lectures/05-2-balance-squentiality.html#introduction-example-3",
    "href": "files/lectures/05-2-balance-squentiality.html#introduction-example-3",
    "title": "Balance and Sequentiality in Bayesian Analysis",
    "section": "Introduction: Example",
    "text": "Introduction: Example\n\nReflecting upon movies that he has seen in the past, the feminist understands that the majority lack strong women characters."
  },
  {
    "objectID": "files/lectures/05-2-balance-squentiality.html#introduction-example-4",
    "href": "files/lectures/05-2-balance-squentiality.html#introduction-example-4",
    "title": "Balance and Sequentiality in Bayesian Analysis",
    "section": "Introduction: Example",
    "text": "Introduction: Example\n\nLastly, the optimist thinks that the Bechdel test is a really low bar for the representation of women in film, and thus assumes almost all movies pass the test."
  },
  {
    "objectID": "files/lectures/05-2-balance-squentiality.html#introduction-example-5",
    "href": "files/lectures/05-2-balance-squentiality.html#introduction-example-5",
    "title": "Balance and Sequentiality in Bayesian Analysis",
    "section": "Introduction: Example",
    "text": "Introduction: Example\n\nThe analysts agree to review a sample of n recent movies and record Y, the number that pass the Bechdel test.\n\nBecause the outcome is yes/no, the binomial distribution is appropriate for the data distribution.\nWe aren’t sure what the population proportion, \\pi, is, so we will not restrict it to a fixed value.\n\nBecause we know \\pi \\in [0, 1], the beta distribution is appropriate for the prior distribution.\n\n\n\n\n\\begin{align*}\nY|\\pi &\\sim \\text{Bin}(n, \\pi) \\\\\n\\pi &\\sim \\text{Beta}(\\alpha, \\beta)\n\\end{align*}"
  },
  {
    "objectID": "files/lectures/05-2-balance-squentiality.html#introduction-example-6",
    "href": "files/lectures/05-2-balance-squentiality.html#introduction-example-6",
    "title": "Balance and Sequentiality in Bayesian Analysis",
    "section": "Introduction: Example",
    "text": "Introduction: Example\n\nBecause we know \\pi \\in [0, 1], the beta distribution is appropriate for the prior distribution.\n\n\n\\begin{align*}\nY|\\pi &\\sim \\text{Bin}(n, \\pi) \\\\\n\\pi &\\sim \\text{Beta}(\\alpha, \\beta)\n\\end{align*}\n\n\nFrom the previous chapter, we know that this results in the following posterior distribution\n\n\n\\pi | (Y=y) \\sim \\text{Beta}(\\alpha+y, \\beta+n-y)"
  },
  {
    "objectID": "files/lectures/05-2-balance-squentiality.html#introduction-example-7",
    "href": "files/lectures/05-2-balance-squentiality.html#introduction-example-7",
    "title": "Balance and Sequentiality in Bayesian Analysis",
    "section": "Introduction: Example",
    "text": "Introduction: Example\n\nWait!!\n\nEveryone gets their own prior?\n… is there a “correct” prior?\n…… is the Bayesian world always this subjective?"
  },
  {
    "objectID": "files/lectures/05-2-balance-squentiality.html#introduction-example-8",
    "href": "files/lectures/05-2-balance-squentiality.html#introduction-example-8",
    "title": "Balance and Sequentiality in Bayesian Analysis",
    "section": "Introduction: Example",
    "text": "Introduction: Example\n\nMore clearly defined questions that we can actually answer:\n\nTo what extent might different priors lead the analysts to three different posterior conclusions about the Bechdel test?\n\nHow might this depend upon the sample size and outcomes of the movie data they collect?\n\nTo what extent will the analysts’ posterior understandings evolve as they collect more and more data?\nWill they ever come to agreement about the representation of women in film?!"
  },
  {
    "objectID": "files/lectures/05-2-balance-squentiality.html#different-priors-to-different-posteriors",
    "href": "files/lectures/05-2-balance-squentiality.html#different-priors-to-different-posteriors",
    "title": "Balance and Sequentiality in Bayesian Analysis",
    "section": "Different Priors \\to Different Posteriors",
    "text": "Different Priors \\to Different Posteriors\n\n\nThe differing prior means show disagreement about whether \\pi is closer to 0 or 1.\nThe differing levels of prior variability show that the analysts have different degrees of certainty in their prior information."
  },
  {
    "objectID": "files/lectures/05-2-balance-squentiality.html#different-priors-to-different-posteriors-1",
    "href": "files/lectures/05-2-balance-squentiality.html#different-priors-to-different-posteriors-1",
    "title": "Balance and Sequentiality in Bayesian Analysis",
    "section": "Different Priors \\to Different Posteriors",
    "text": "Different Priors \\to Different Posteriors\n\n\nInformative prior: reflects specific information about the unknown variable with high certainty, i.e., low variability."
  },
  {
    "objectID": "files/lectures/05-2-balance-squentiality.html#different-priors-to-different-posteriors-2",
    "href": "files/lectures/05-2-balance-squentiality.html#different-priors-to-different-posteriors-2",
    "title": "Balance and Sequentiality in Bayesian Analysis",
    "section": "Different Priors \\to Different Posteriors",
    "text": "Different Priors \\to Different Posteriors\n\n\nVague or diffuse prior: reflects little specific information about the unknown variable.\n\nA flat prior, which assigns equal prior plausibility to all possible values of the variable, is a special case.\nThis is effectively saying “🤷.”"
  },
  {
    "objectID": "files/lectures/05-2-balance-squentiality.html#different-priors-to-different-posteriors-3",
    "href": "files/lectures/05-2-balance-squentiality.html#different-priors-to-different-posteriors-3",
    "title": "Balance and Sequentiality in Bayesian Analysis",
    "section": "Different Priors \\to Different Posteriors",
    "text": "Different Priors \\to Different Posteriors\n\nOkay, great - we have different priors.\n\nHow do the different priors affect the posterior?\n\nWe have data from FiveThirtyEight, reporting results of the Bechdel test."
  },
  {
    "objectID": "files/lectures/05-2-balance-squentiality.html#different-priors-to-different-posteriors-4",
    "href": "files/lectures/05-2-balance-squentiality.html#different-priors-to-different-posteriors-4",
    "title": "Balance and Sequentiality in Bayesian Analysis",
    "section": "Different Priors \\to Different Posteriors",
    "text": "Different Priors \\to Different Posteriors\n\nSo how many pass the test in this sample?\n\n\nbechdel20 %&gt;% tabyl(binary) %&gt;% adorn_totals(\"row\")"
  },
  {
    "objectID": "files/lectures/05-2-balance-squentiality.html#different-priors-to-different-posteriors-5",
    "href": "files/lectures/05-2-balance-squentiality.html#different-priors-to-different-posteriors-5",
    "title": "Balance and Sequentiality in Bayesian Analysis",
    "section": "Different Priors \\to Different Posteriors",
    "text": "Different Priors \\to Different Posteriors\n\nLet’s look at the graphs of just the prior and likelihood.\n\n\nplot_beta_binomial(alpha = 5, beta = 11, y = 9, n = 20, posterior = FALSE) + theme_bw()\nplot_beta_binomial(alpha = 1, beta = 1, y = 9, n = 20, posterior = FALSE) + theme_bw()\nplot_beta_binomial(alpha = 14, beta = 1, y = 9, n = 20, posterior = FALSE) + theme_bw()\n\n\nQuestions to think about:\n\nWhose posterior do you anticipate will look the most like the scaled likelihood?\nWhose do you anticipate will look the least like the scaled likelihood?"
  },
  {
    "objectID": "files/lectures/05-2-balance-squentiality.html#different-priors-to-different-posteriors-6",
    "href": "files/lectures/05-2-balance-squentiality.html#different-priors-to-different-posteriors-6",
    "title": "Balance and Sequentiality in Bayesian Analysis",
    "section": "Different Priors \\to Different Posteriors",
    "text": "Different Priors \\to Different Posteriors\n\nLet’s look at the graphs of just the prior and likelihood."
  },
  {
    "objectID": "files/lectures/05-2-balance-squentiality.html#different-priors-to-different-posteriors-7",
    "href": "files/lectures/05-2-balance-squentiality.html#different-priors-to-different-posteriors-7",
    "title": "Balance and Sequentiality in Bayesian Analysis",
    "section": "Different Priors \\to Different Posteriors",
    "text": "Different Priors \\to Different Posteriors\n\nLet’s look at the graphs of just the prior and likelihood."
  },
  {
    "objectID": "files/lectures/05-2-balance-squentiality.html#different-priors-to-different-posteriors-8",
    "href": "files/lectures/05-2-balance-squentiality.html#different-priors-to-different-posteriors-8",
    "title": "Balance and Sequentiality in Bayesian Analysis",
    "section": "Different Priors \\to Different Posteriors",
    "text": "Different Priors \\to Different Posteriors\n\nLet’s look at the graphs of just the prior and likelihood."
  },
  {
    "objectID": "files/lectures/05-2-balance-squentiality.html#different-priors-to-different-posteriors-9",
    "href": "files/lectures/05-2-balance-squentiality.html#different-priors-to-different-posteriors-9",
    "title": "Balance and Sequentiality in Bayesian Analysis",
    "section": "Different Priors \\to Different Posteriors",
    "text": "Different Priors \\to Different Posteriors\n\nFind the posterior distributions. (i.e., What are the updated parameters?)\n\n\n\n\n\n\nAnalyst\n\n\nPrior\n\n\nPosterior\n\n\n\n\n\n\nthe feminist\n\n\nBeta(5, 11)\n\n\nBeta(14, 22)\n\n\n\n\nthe clueless\n\n\nBeta(1, 1)\n\n\nBeta(10, 12)\n\n\n\n\nthe optimist\n\n\nBeta(14, 1)\n\n\nBeta(23, 12)\n\n\n\n\n\n\nLet’s now explore what the posteriors look like.\n\n\nplot_beta_binomial(alpha = 5, beta = 11, y = 9, n = 20) + theme_bw()\nplot_beta_binomial(alpha = 1, beta = 1, y = 9, n = 20) + theme_bw()\nplot_beta_binomial(alpha = 14, beta = 1, y = 9, n = 20) + theme_bw()"
  },
  {
    "objectID": "files/lectures/05-2-balance-squentiality.html#different-priors-to-different-posteriors-10",
    "href": "files/lectures/05-2-balance-squentiality.html#different-priors-to-different-posteriors-10",
    "title": "Balance and Sequentiality in Bayesian Analysis",
    "section": "Different Priors \\to Different Posteriors",
    "text": "Different Priors \\to Different Posteriors\n\nLet’s now explore what the posteriors look like."
  },
  {
    "objectID": "files/lectures/05-2-balance-squentiality.html#different-priors-to-different-posteriors-11",
    "href": "files/lectures/05-2-balance-squentiality.html#different-priors-to-different-posteriors-11",
    "title": "Balance and Sequentiality in Bayesian Analysis",
    "section": "Different Priors \\to Different Posteriors",
    "text": "Different Priors \\to Different Posteriors\n\nLet’s now explore what the posteriors look like."
  },
  {
    "objectID": "files/lectures/05-2-balance-squentiality.html#different-priors-to-different-posteriors-12",
    "href": "files/lectures/05-2-balance-squentiality.html#different-priors-to-different-posteriors-12",
    "title": "Balance and Sequentiality in Bayesian Analysis",
    "section": "Different Priors \\to Different Posteriors",
    "text": "Different Priors \\to Different Posteriors\n\nLet’s now explore what the posteriors look like."
  },
  {
    "objectID": "files/lectures/05-2-balance-squentiality.html#different-priors-to-different-posteriors-13",
    "href": "files/lectures/05-2-balance-squentiality.html#different-priors-to-different-posteriors-13",
    "title": "Balance and Sequentiality in Bayesian Analysis",
    "section": "Different Priors \\to Different Posteriors",
    "text": "Different Priors \\to Different Posteriors\n\nIn addition to priors affecting our posterior distributions… the data also affects it.\nLet’s now consider three new analysts: they all share the optimistic Beta(14, 1) for \\pi, however, they have access to different data.\n\nMorteza reviews n = 13 movies from the year 1991, among which Y=6 (about 46%) pass the Bechdel.\nNadide reviews n = 63 movies from the year 2001, among which Y=29 (about 46%) pass the Bechdel.\nUrsula reviews n = 99 movies from the year 2013, among which Y=46 (about 46%) pass the Bechdel.\n\nHow will the different data affect the posterior distributions?"
  },
  {
    "objectID": "files/lectures/05-2-balance-squentiality.html#different-priors-to-different-posteriors-14",
    "href": "files/lectures/05-2-balance-squentiality.html#different-priors-to-different-posteriors-14",
    "title": "Balance and Sequentiality in Bayesian Analysis",
    "section": "Different Priors \\to Different Posteriors",
    "text": "Different Priors \\to Different Posteriors\n\nHow will the different data affect the posterior distributions?\n\n\nplot_beta_binomial(alpha = 14, beta = 1, y = 6, n = 13) + theme_bw()\nplot_beta_binomial(alpha = 14, beta = 1, y = 29, n = 63) + theme_bw()\nplot_beta_binomial(alpha = 14, beta = 1, y = 46, n = 99) + theme_bw()\n\n\nWhich posterior is the most in sync with their data?\nWhich posterior is the least in sync with their data?"
  },
  {
    "objectID": "files/lectures/05-2-balance-squentiality.html#different-priors-to-different-posteriors-15",
    "href": "files/lectures/05-2-balance-squentiality.html#different-priors-to-different-posteriors-15",
    "title": "Balance and Sequentiality in Bayesian Analysis",
    "section": "Different Priors \\to Different Posteriors",
    "text": "Different Priors \\to Different Posteriors\n\nHow will the different data affect the posterior distributions?"
  },
  {
    "objectID": "files/lectures/05-2-balance-squentiality.html#different-priors-to-different-posteriors-16",
    "href": "files/lectures/05-2-balance-squentiality.html#different-priors-to-different-posteriors-16",
    "title": "Balance and Sequentiality in Bayesian Analysis",
    "section": "Different Priors \\to Different Posteriors",
    "text": "Different Priors \\to Different Posteriors\n\nHow will the different data affect the posterior distributions?"
  },
  {
    "objectID": "files/lectures/05-2-balance-squentiality.html#different-priors-to-different-posteriors-17",
    "href": "files/lectures/05-2-balance-squentiality.html#different-priors-to-different-posteriors-17",
    "title": "Balance and Sequentiality in Bayesian Analysis",
    "section": "Different Priors \\to Different Posteriors",
    "text": "Different Priors \\to Different Posteriors\n\nHow will the different data affect the posterior distributions?"
  },
  {
    "objectID": "files/lectures/05-2-balance-squentiality.html#different-priors-to-different-posteriors-18",
    "href": "files/lectures/05-2-balance-squentiality.html#different-priors-to-different-posteriors-18",
    "title": "Balance and Sequentiality in Bayesian Analysis",
    "section": "Different Priors \\to Different Posteriors",
    "text": "Different Priors \\to Different Posteriors\n\nFind the posterior distributions. (i.e., What are the updated parameters?)\n\nRecall that all use the Beta(14, 1) prior.\n\n\n\n\n\n\n\n\nAnalyst\n\n\n\n\nData\n\n\n\n\nPosterior\n\n\n\n\n\n\n\nMorteza\n\n\nY=6 of n=13\n\n\nBeta(20, 8)\n\n\n\n\nNadide\n\n\nY=29 of n=63\n\n\nBeta(45, 35)\n\n\n\n\nUrsula\n\n\nY=46 of n=99\n\n\nBeta(60, 54)\n\n\n\n\n\n\nLet’s also explore what the posteriors look like.\n\n\nplot_beta_binomial(alpha = 14, beta = 1, y = 6, n = 13) + theme_bw() \nplot_beta_binomial(alpha = 14, beta = 1, y = 29, n = 63) + theme_bw()\nplot_beta_binomial(alpha = 14, beta = 1, y = 46, n = 99) + theme_bw()"
  },
  {
    "objectID": "files/lectures/05-2-balance-squentiality.html#different-priors-to-different-posteriors-19",
    "href": "files/lectures/05-2-balance-squentiality.html#different-priors-to-different-posteriors-19",
    "title": "Balance and Sequentiality in Bayesian Analysis",
    "section": "Different Priors \\to Different Posteriors",
    "text": "Different Priors \\to Different Posteriors\n\nLet’s explore what the posteriors look like."
  },
  {
    "objectID": "files/lectures/05-2-balance-squentiality.html#different-priors-to-different-posteriors-20",
    "href": "files/lectures/05-2-balance-squentiality.html#different-priors-to-different-posteriors-20",
    "title": "Balance and Sequentiality in Bayesian Analysis",
    "section": "Different Priors \\to Different Posteriors",
    "text": "Different Priors \\to Different Posteriors\n\nLet’s explore what the posteriors look like."
  },
  {
    "objectID": "files/lectures/05-2-balance-squentiality.html#different-priors-to-different-posteriors-21",
    "href": "files/lectures/05-2-balance-squentiality.html#different-priors-to-different-posteriors-21",
    "title": "Balance and Sequentiality in Bayesian Analysis",
    "section": "Different Priors \\to Different Posteriors",
    "text": "Different Priors \\to Different Posteriors\n\nLet’s explore what the posteriors look like."
  },
  {
    "objectID": "files/lectures/05-2-balance-squentiality.html#different-priors-to-different-posteriors-22",
    "href": "files/lectures/05-2-balance-squentiality.html#different-priors-to-different-posteriors-22",
    "title": "Balance and Sequentiality in Bayesian Analysis",
    "section": "Different Priors \\to Different Posteriors",
    "text": "Different Priors \\to Different Posteriors\n\nWhat did we observe?\n\nAs n \\to \\infty, variance in the likelihood \\to 0.\n\nIn Morteza’s small sample of 13 movies, the likelihood function is wide.\nIn Ursula’s larger sample size of 99 movies, the likelihood function is narrower.\n\nWe see that the narrower the likelihood, the more influence the data holds over the posterior."
  },
  {
    "objectID": "files/lectures/05-2-balance-squentiality.html#striking-a-balance",
    "href": "files/lectures/05-2-balance-squentiality.html#striking-a-balance",
    "title": "Balance and Sequentiality in Bayesian Analysis",
    "section": "Striking a Balance",
    "text": "Striking a Balance\n\n\nOverall message: no matter the strength of and discrepancies among their prior understanding of \\pi, analysts will come to a common posterior understanding in light of strong data."
  },
  {
    "objectID": "files/lectures/05-2-balance-squentiality.html#striking-a-balance-1",
    "href": "files/lectures/05-2-balance-squentiality.html#striking-a-balance-1",
    "title": "Balance and Sequentiality in Bayesian Analysis",
    "section": "Striking a Balance",
    "text": "Striking a Balance\n\nThe posterior can either favor the data or the prior.\n\nThe rate at which the posterior balance tips in favor of the data depends upon the prior.\n\nLeft to right on the graph, the sample size increases from n=13 to n=99 movies, while preserving the proportion that pass (\\approx 0.46).\n\nThe likelihood’s insistence and the data’s influence over the posterior increase with sample size.\nThis also means that the influence of our prior understanding diminishes as we gather new data.\n\nTop to bottom on the graph, priors move from informative (Beta(14,1)) to vague (Beta(1,1)).\n\nNaturally, the more informative the prior, the greater its influence on the posterior."
  },
  {
    "objectID": "files/lectures/05-2-balance-squentiality.html#introduction-sequentiality",
    "href": "files/lectures/05-2-balance-squentiality.html#introduction-sequentiality",
    "title": "Balance and Sequentiality in Bayesian Analysis",
    "section": "Introduction: Sequentiality",
    "text": "Introduction: Sequentiality\n\nLet’s now turn our thinking to - okay, we’ve updated our beliefs… but now we have new data!\nThe evolution in our posterior understanding happens incrementally, as we accumulate new data.\n\nScientists’ understanding of climate change has evolved over the span of decades as they gain new information.\nPresidential candidates’ understanding of their chances of winning an election evolve over months as new poll results become available."
  },
  {
    "objectID": "files/lectures/05-2-balance-squentiality.html#introduction-sequentiality-1",
    "href": "files/lectures/05-2-balance-squentiality.html#introduction-sequentiality-1",
    "title": "Balance and Sequentiality in Bayesian Analysis",
    "section": "Introduction: Sequentiality",
    "text": "Introduction: Sequentiality\n\nLet’s revisit Milgram’s behavioral study of obedience from Chapter 3. Recall, \\pi represents the proportion of people that will obey authority, even if it means bringing harm to others.\nPrior to Milgram’s experiments, our fictional psychologist expected that few people would obey authority in the face of harming another: \\pi \\sim \\text{Beta}(1,10).\nNow, suppose that the psychologist collected the data incrementally, day by day, over a three-day period.\nFind the following posterior distributions, each building off the last:\n\nDay 0: \\text{Beta}(1,10).\nDay 1: Y=1 out of n=10.\nDay 2: Y=17 out of n=20.\nDay 3: Y=8 out of n=10."
  },
  {
    "objectID": "files/lectures/05-2-balance-squentiality.html#introduction-sequentiality-2",
    "href": "files/lectures/05-2-balance-squentiality.html#introduction-sequentiality-2",
    "title": "Balance and Sequentiality in Bayesian Analysis",
    "section": "Introduction: Sequentiality",
    "text": "Introduction: Sequentiality\n\nFind the following posterior distributions, each building off the last:\n\nDay 0: \\text{Beta}(1,10).\nDay 1: Y=1 out of n=10: \\text{Beta}(1,10) \\to \\text{Beta}(2, 19).\nDay 2: Y=17 out of n=20: \\text{Beta}(2, 19) \\to \\text{Beta}(19, 22).\nDay 3: Y=8 out of n=10: \\text{Beta}(19, 22) \\to \\text{Beta}(27, 24).\n\nRecall from Chapter 3, our posterior was \\text{Beta}(27,24)!"
  },
  {
    "objectID": "files/lectures/05-2-balance-squentiality.html#sequential-bayesian-analysis-or-bayesian-learning",
    "href": "files/lectures/05-2-balance-squentiality.html#sequential-bayesian-analysis-or-bayesian-learning",
    "title": "Balance and Sequentiality in Bayesian Analysis",
    "section": "Sequential Bayesian Analysis or Bayesian Learning",
    "text": "Sequential Bayesian Analysis or Bayesian Learning\n\nIn a sequential Bayesian analysis, a posterior model is updated incrementally as more data come in.\n\nWith each new piece of data, the previous posterior model reflecting our understanding prior to observing this data becomes the new prior model.\n\nThis is why we love Bayesian!\n\nWe evolve our thinking as new data come in.\n\nThese types of sequential analyses also uphold two fundamental properties:\n\nThe final posterior model is data order invariant,\n\nThe final posterior only depends upon the cumulative data."
  },
  {
    "objectID": "files/lectures/05-2-balance-squentiality.html#sequential-bayesian-analysis-or-bayesian-learning-1",
    "href": "files/lectures/05-2-balance-squentiality.html#sequential-bayesian-analysis-or-bayesian-learning-1",
    "title": "Balance and Sequentiality in Bayesian Analysis",
    "section": "Sequential Bayesian Analysis or Bayesian Learning",
    "text": "Sequential Bayesian Analysis or Bayesian Learning\n\nIn order:\n\nDay 0: \\text{Beta}(1,10).\nDay 1: Y=1 out of n=10: \\text{Beta}(1,10) \\to \\text{Beta}(2, 19).\nDay 2: Y=17 out of n=20: \\text{Beta}(2, 19) \\to \\text{Beta}(19, 22).\nDay 3: Y=8 out of n=10: \\text{Beta}(19, 22) \\to \\text{Beta}(27, 24).\n\nOut of order:\n\nDay 0: \\text{Beta}(1,10).\nDay 3: Y=8 out of n=10: \\text{Beta}(1,10) \\to \\text{Beta}(9, 12).\nDay 2: Y=17 out of n=20: \\text{Beta}(9, 12) \\to \\text{Beta}(26, 15).\nDay 1: Y=1 out of n=10: \\text{Beta}(26, 15) \\to \\text{Beta}(27, 24)."
  },
  {
    "objectID": "files/lectures/05-2-balance-squentiality.html#sequential-bayesian-analysis-or-bayesian-learning-2",
    "href": "files/lectures/05-2-balance-squentiality.html#sequential-bayesian-analysis-or-bayesian-learning-2",
    "title": "Balance and Sequentiality in Bayesian Analysis",
    "section": "Sequential Bayesian Analysis or Bayesian Learning",
    "text": "Sequential Bayesian Analysis or Bayesian Learning"
  },
  {
    "objectID": "files/lectures/05-2-balance-squentiality.html#example-mario-kart",
    "href": "files/lectures/05-2-balance-squentiality.html#example-mario-kart",
    "title": "Balance and Sequentiality in Bayesian Analysis",
    "section": "Example: Mario Kart",
    "text": "Example: Mario Kart\n\nIn Mario Kart 8 Deluxe, item boxes give different items depending on race position. To reduce the “position bias,” only item boxes opened while the racer was in mid-pack (positions 4–10) were recorded.\nYou want to estimate the probability that an item box yields a Red Shell. When playing the Special Cup, only 31 red shells were seen in 114 boxes opened by mid-pack racers.\nFind the posterior distribution under two priors:\n\nFlat/uninformative prior, Beta(1,1).\nBeta(\\alpha, \\beta) of your choosing.\n\nHow different are the posterior distributions?"
  },
  {
    "objectID": "files/lectures/05-2-balance-squentiality.html#example-mario-kart-1",
    "href": "files/lectures/05-2-balance-squentiality.html#example-mario-kart-1",
    "title": "Balance and Sequentiality in Bayesian Analysis",
    "section": "Example: Mario Kart",
    "text": "Example: Mario Kart\n\nSuppose that we know the individual data points for the Special Cup:\n\nCloudtop Cruise, of 28 boxes, 6 had a red shell.\nBone-Dry Dunes, of 27 boxes, 8 had a red shell.\nBowser’s Castle, of 29 boxes, 12 had a red shell.\nRainbow Road, of 30 boxes, 5 had a red shell.\n\nProve sequentiality to yourself.\n\nAnalyze the data race-by-race in this order: Cloudtop Cruise, Bone-Dry Dunes, Bowser’s Castle, and Rainbow Road.\nAnalyze the data race-by-race in this order: Rainbow Road, Bowser’s Castle, Cloudtop Cruise, and Bone-Dry Dunes."
  },
  {
    "objectID": "files/lectures/05-2-balance-squentiality.html#wrap-up",
    "href": "files/lectures/05-2-balance-squentiality.html#wrap-up",
    "title": "Balance and Sequentiality in Bayesian Analysis",
    "section": "Wrap Up",
    "text": "Wrap Up\n\nToday we have discussed balance and sequentiality.\nRemember that order of data inclusion does not matter – we will end up with the same posterior.\nWe have seen that prior specification “matters” but there will not be a large difference in the posterior distribution when priors are more similar to one another.\nNext week:\n\nGamma-Poisson\nNormal-Normal\nWhat to do with the posterior distribution."
  },
  {
    "objectID": "files/lectures/05-2-balance-squentiality.html#homework-practice",
    "href": "files/lectures/05-2-balance-squentiality.html#homework-practice",
    "title": "Balance and Sequentiality in Bayesian Analysis",
    "section": "Homework / Practice",
    "text": "Homework / Practice\n\nFrom the Bayes Rules! textbook:\n\n4.3\n4.4\n4.6\n4.9\n4.15\n4.16\n4.17\n4.18\n4.19"
  },
  {
    "objectID": "files/lectures/05-1-beta-binomial.html#introduction-beta-binomial-model",
    "href": "files/lectures/05-1-beta-binomial.html#introduction-beta-binomial-model",
    "title": "Beta-Binomial Model",
    "section": "Introduction: Beta-Binomial Model",
    "text": "Introduction: Beta-Binomial Model\n\nLast week, we learned how to think like a Bayesian.\n\nToday, we will formalize the model we muddled through last time.\n\nThis is called the Beta-Binomial model.\n\nThe Beta distribution is the prior.\nThe Binomial distribution is the data distribution (or the likeihood).\nThe posterior also follows a Beta distribution.\n\nConjugate family: When the prior and posterior are the same named distribution, but different parameters."
  },
  {
    "objectID": "files/lectures/05-1-beta-binomial.html#example-set-up",
    "href": "files/lectures/05-1-beta-binomial.html#example-set-up",
    "title": "Beta-Binomial Model",
    "section": "Example Set Up",
    "text": "Example Set Up\n\nConsider the following scenario.\n\n“Michelle” has decided to run for president and you’re her campaign manager for the state of Florida.\nAs such, you’ve conducted 30 different polls throughout the election season.\nThough Michelle’s support has hovered around 45%, she polled at around 35% in the dreariest days and around 55% in the best days on the campaign trail."
  },
  {
    "objectID": "files/lectures/05-1-beta-binomial.html#example-set-up-1",
    "href": "files/lectures/05-1-beta-binomial.html#example-set-up-1",
    "title": "Beta-Binomial Model",
    "section": "Example Set Up",
    "text": "Example Set Up\n\nPast polls provide prior information about \\pi, the proportion of Floridians that currently support Michelle.\n\nIn fact, we can reorganize this information into a formal prior probability model of \\pi.\n\nIn a previous problem, we assumed that \\pi could only be 0.2, 0.5, or 0.8, the corresponding chances of which were defined by a discrete probability model.\n\nHowever, in the reality of Michelle’s election support, \\pi \\in [0, 1].\n\nWe can reflect this reality and conduct a Bayesian analysis by constructing a continuous prior probability model of \\pi."
  },
  {
    "objectID": "files/lectures/05-1-beta-binomial.html#example-set-up-2",
    "href": "files/lectures/05-1-beta-binomial.html#example-set-up-2",
    "title": "Beta-Binomial Model",
    "section": "Example Set Up",
    "text": "Example Set Up\n\n\n\n\nA reasonable prior is represented by the curve on the right.\n\nNotice that this curve preserves the overall information and variability in the past polls, i.e., Michelle’s support, \\pi can be anywhere between 0 and 1, but is most likely around 0.45."
  },
  {
    "objectID": "files/lectures/05-1-beta-binomial.html#example-set-up-3",
    "href": "files/lectures/05-1-beta-binomial.html#example-set-up-3",
    "title": "Beta-Binomial Model",
    "section": "Example Set Up",
    "text": "Example Set Up\n\nIncorporating this more nuanced, continuous view of Michelle’s support, \\pi, will require some new tools.\n\nNo matter if our parameter \\pi is continuous or discrete, the posterior model of \\pi will combine insights from the prior and data.\n\\pi isn’t the only variable of interest that lives on [0,1].\n\nMaybe we’re interested in modeling the proportion of people that use public transit, the proportion of trains that are delayed, the proportion of people that prefer cats to dogs, etc.\n\nThe Beta-Binomial model provides the tools we need to study the proportion of interest, \\pi, in each of these settings."
  },
  {
    "objectID": "files/lectures/05-1-beta-binomial.html#beta-prior",
    "href": "files/lectures/05-1-beta-binomial.html#beta-prior",
    "title": "Beta-Binomial Model",
    "section": "Beta Prior",
    "text": "Beta Prior\n\n\n\n\nIn building the Bayesian election model of Michelle’s election support among Floridians, \\pi, we begin with the prior.\n\nOur continuous prior probability model of \\pi is specified by the probability density function (pdf).\n\nWhat values can \\pi take and which are more plausible than others?"
  },
  {
    "objectID": "files/lectures/05-1-beta-binomial.html#beta-prior-1",
    "href": "files/lectures/05-1-beta-binomial.html#beta-prior-1",
    "title": "Beta-Binomial Model",
    "section": "Beta Prior",
    "text": "Beta Prior\n\nLet \\pi be a random variable, where \\pi \\in [0, 1].\nThe variability in \\pi may be captured by a Beta model with shape hyperparameters \\alpha &gt; 0 and \\beta &gt; 0,\n\nhyperparameter: a parameter used in a prior model.\n\n\n\\pi \\sim \\text{Beta}(\\alpha, \\beta),"
  },
  {
    "objectID": "files/lectures/05-1-beta-binomial.html#beta-prior-shapes",
    "href": "files/lectures/05-1-beta-binomial.html#beta-prior-shapes",
    "title": "Beta-Binomial Model",
    "section": "Beta Prior: Shapes",
    "text": "Beta Prior: Shapes\n\nLet’s explore the shape of the Beta:\n\n\n\nplot_beta(1, 5) + theme_bw() + ggtitle(\"Beta(1, 5)\")"
  },
  {
    "objectID": "files/lectures/05-1-beta-binomial.html#beta-prior-shapes-1",
    "href": "files/lectures/05-1-beta-binomial.html#beta-prior-shapes-1",
    "title": "Beta-Binomial Model",
    "section": "Beta Prior: Shapes",
    "text": "Beta Prior: Shapes\n\nLet’s explore the shape of the Beta:\n\n\n\nplot_beta(1, 2) + theme_bw() + ggtitle(\"Beta(1, 2)\")"
  },
  {
    "objectID": "files/lectures/05-1-beta-binomial.html#beta-prior-shapes-2",
    "href": "files/lectures/05-1-beta-binomial.html#beta-prior-shapes-2",
    "title": "Beta-Binomial Model",
    "section": "Beta Prior: Shapes",
    "text": "Beta Prior: Shapes\n\nLet’s explore the shape of the Beta:\n\n\n\nplot_beta(3, 7) + theme_bw() + ggtitle(\"Beta(3, 7)\")"
  },
  {
    "objectID": "files/lectures/05-1-beta-binomial.html#beta-prior-shapes-3",
    "href": "files/lectures/05-1-beta-binomial.html#beta-prior-shapes-3",
    "title": "Beta-Binomial Model",
    "section": "Beta Prior: Shapes",
    "text": "Beta Prior: Shapes\n\nLet’s explore the shape of the Beta:\n\n\n\nplot_beta(1, 1) + theme_bw() + ggtitle(\"Beta(1, 1)\")"
  },
  {
    "objectID": "files/lectures/05-1-beta-binomial.html#beta-prior-shapes-4",
    "href": "files/lectures/05-1-beta-binomial.html#beta-prior-shapes-4",
    "title": "Beta-Binomial Model",
    "section": "Beta Prior: Shapes",
    "text": "Beta Prior: Shapes\n\nYour turn!\nHow would you describe the typical behavior of a:\n\nBeta(\\alpha, \\beta) variable, \\pi, when \\alpha=\\beta?\nBeta(\\alpha, \\beta) variable, \\pi, when \\alpha&gt;\\beta?\nBeta(\\alpha, \\beta) variable, \\pi, when \\alpha&lt;\\beta?\n\nFor which model is there greater variability in the plausible values of \\pi, Beta(20, 20) or Beta(5, 5)?"
  },
  {
    "objectID": "files/lectures/05-1-beta-binomial.html#beta-prior-shapes-5",
    "href": "files/lectures/05-1-beta-binomial.html#beta-prior-shapes-5",
    "title": "Beta-Binomial Model",
    "section": "Beta Prior: Shapes",
    "text": "Beta Prior: Shapes\n\nHow would you describe the typical behavior of a Beta(\\alpha, \\beta) variable, \\pi, when \\alpha=\\beta?"
  },
  {
    "objectID": "files/lectures/05-1-beta-binomial.html#beta-prior-shapes-6",
    "href": "files/lectures/05-1-beta-binomial.html#beta-prior-shapes-6",
    "title": "Beta-Binomial Model",
    "section": "Beta Prior: Shapes",
    "text": "Beta Prior: Shapes\n\nHow would you describe the typical behavior of a Beta(\\alpha, \\beta) variable, \\pi, when \\alpha&gt;\\beta?"
  },
  {
    "objectID": "files/lectures/05-1-beta-binomial.html#beta-prior-shapes-7",
    "href": "files/lectures/05-1-beta-binomial.html#beta-prior-shapes-7",
    "title": "Beta-Binomial Model",
    "section": "Beta Prior: Shapes",
    "text": "Beta Prior: Shapes\n\nHow would you describe the typical behavior of a Beta(\\alpha, \\beta) variable, \\pi, when \\alpha&lt;\\beta?"
  },
  {
    "objectID": "files/lectures/05-1-beta-binomial.html#beta-prior-shapes-8",
    "href": "files/lectures/05-1-beta-binomial.html#beta-prior-shapes-8",
    "title": "Beta-Binomial Model",
    "section": "Beta Prior: Shapes",
    "text": "Beta Prior: Shapes\n\nFor which model is there greater variability in the plausible values of \\pi, Beta(20, 20) or Beta(5, 5)?"
  },
  {
    "objectID": "files/lectures/05-1-beta-binomial.html#tuning-the-beta-prior",
    "href": "files/lectures/05-1-beta-binomial.html#tuning-the-beta-prior",
    "title": "Beta-Binomial Model",
    "section": "Tuning the Beta Prior",
    "text": "Tuning the Beta Prior\n\nWe can tune the shape hyperparameters (\\alpha and \\beta) to reflect our prior information about Michelle’s election support, \\pi.\nIn our example, we saw that she polled between 25 and 65 percentage points, with an average of 45 percentage points.\n\nWe want our Beta(\\alpha, \\beta) to have similar patterns, so we should pick \\alpha and \\beta such that \\pi is around 0.45.\n\n\n\nE[\\pi] = \\frac{\\alpha}{\\alpha+\\beta} \\approx 0.45\n\n\nUsing algebra, we can tune, and find\n\n\\alpha \\approx \\frac{9}{11} \\beta"
  },
  {
    "objectID": "files/lectures/05-1-beta-binomial.html#tuning-the-beta-prior-1",
    "href": "files/lectures/05-1-beta-binomial.html#tuning-the-beta-prior-1",
    "title": "Beta-Binomial Model",
    "section": "Tuning the Beta Prior",
    "text": "Tuning the Beta Prior\n\nYour turn!\n\nGraph the following and determine which is best for the example.\n\n\n\nplot_beta(9, 11) + theme_bw()\nplot_beta(27, 33) + theme_bw()\nplot_beta(45, 55) + theme_bw()\n\n\nRecall, this is what we are going for:"
  },
  {
    "objectID": "files/lectures/05-1-beta-binomial.html#tuning-the-beta-prior-2",
    "href": "files/lectures/05-1-beta-binomial.html#tuning-the-beta-prior-2",
    "title": "Beta-Binomial Model",
    "section": "Tuning the Beta Prior",
    "text": "Tuning the Beta Prior\n\n\nplot_beta(9, 11) + theme_bw() + ggtitle(\"Beta(9, 11)\")"
  },
  {
    "objectID": "files/lectures/05-1-beta-binomial.html#tuning-the-beta-prior-3",
    "href": "files/lectures/05-1-beta-binomial.html#tuning-the-beta-prior-3",
    "title": "Beta-Binomial Model",
    "section": "Tuning the Beta Prior",
    "text": "Tuning the Beta Prior\n\n\nplot_beta(27, 33) + theme_bw() + ggtitle(\"Beta(27, 33)\")"
  },
  {
    "objectID": "files/lectures/05-1-beta-binomial.html#tuning-the-beta-prior-4",
    "href": "files/lectures/05-1-beta-binomial.html#tuning-the-beta-prior-4",
    "title": "Beta-Binomial Model",
    "section": "Tuning the Beta Prior",
    "text": "Tuning the Beta Prior\n\n\nplot_beta(45, 55) + theme_bw() + ggtitle(\"Beta(45, 55)\")"
  },
  {
    "objectID": "files/lectures/05-1-beta-binomial.html#tuning-the-beta-prior-5",
    "href": "files/lectures/05-1-beta-binomial.html#tuning-the-beta-prior-5",
    "title": "Beta-Binomial Model",
    "section": "Tuning the Beta Prior",
    "text": "Tuning the Beta Prior\n\nNow that we have a prior, we “know” some things.\n\n\\pi \\sim \\text{Beta}(45, 55)\n\nFrom the properties of the beta distribution,\n\n\n\\begin{equation*}\n\\begin{aligned}\nE[\\pi] &= \\frac{\\alpha}{\\alpha + \\beta} & \\text{ and } & \\text{ } & \\text{ }  \\\\\n&=\\frac{45}{45+55} \\\\\n&= 0.45\n\\end{aligned}\n\\begin{aligned}\n\\text{var}[\\pi] &= \\frac{\\alpha\\beta}{(\\alpha+\\beta)^2(\\alpha+\\beta+1)} \\\\\n&= \\frac{(45)(55)}{(45+55)^2(45+55+1)} \\\\\n&= 0.0025\n\\end{aligned}\n\\end{equation*}"
  },
  {
    "objectID": "files/lectures/05-1-beta-binomial.html#binomial-data-model",
    "href": "files/lectures/05-1-beta-binomial.html#binomial-data-model",
    "title": "Beta-Binomial Model",
    "section": "Binomial Data Model",
    "text": "Binomial Data Model\n\nA new poll of n = 50 Floridians recorded Y, the number that support Michelle.\n\nThe results depend upon \\pi (as \\pi increases, Y tends to increase).\n\nTo model the dependence of Y on \\pi, we assume\n\nvoters answer the poll independently of one another;\nthe probability that any polled voter supports your candidate Michelle is \\pi\n\nThis is a binomial event, Y|\\pi \\sim \\text{Bin}(50, \\pi), with conditional pmf, f(y|\\pi) defined for y \\in \\{0, 1, ..., 50\\}\n\nf(y|\\pi) = P[Y = y|\\pi] = {50 \\choose y} \\pi^y (1-\\pi)^{50-y}"
  },
  {
    "objectID": "files/lectures/05-1-beta-binomial.html#binomial-data-model-1",
    "href": "files/lectures/05-1-beta-binomial.html#binomial-data-model-1",
    "title": "Beta-Binomial Model",
    "section": "Binomial Data Model",
    "text": "Binomial Data Model\n\nThe conditional pmf, f(y|\\pi), gives us answers to a hypothetical question:\n\nIf Michelle’s support were given some value of \\pi, then how many of the 50 polled voters (Y=y) might we expect to suppport her?\n\nLet’s look at this graphically:\n\n\nbinom_prob &lt;- tibble(n_success = 1:sample_size,\n                     prob = dbinom(n_success, size=sample_size, prob=pi_value))\n\nbinom_prob %&gt;%\n  ggplot(aes(x=n_success,y=prob))+\n  geom_col(width=0.2)+\n  labs(x= \"Number of Successes\",\n       y= \"Probability\") +\n  theme_bw()"
  },
  {
    "objectID": "files/lectures/05-1-beta-binomial.html#binomial-data-model-2",
    "href": "files/lectures/05-1-beta-binomial.html#binomial-data-model-2",
    "title": "Beta-Binomial Model",
    "section": "Binomial Data Model",
    "text": "Binomial Data Model"
  },
  {
    "objectID": "files/lectures/05-1-beta-binomial.html#binomial-data-model-3",
    "href": "files/lectures/05-1-beta-binomial.html#binomial-data-model-3",
    "title": "Beta-Binomial Model",
    "section": "Binomial Data Model",
    "text": "Binomial Data Model\n\nIt is observed that Y=30 of the n=50 polled voters support Michelle.\nWe now want to find the likelihood function – remember that we treat Y=30 as the observed data and \\pi as unknown,\n\n\n\\begin{align*}\nf(y|\\pi) &= {50 \\choose y} \\pi^y (1-\\pi)^{50-y} \\\\\nL(\\pi|y=30) &= {50 \\choose 30} \\pi^{30} (1-\\pi)^{20}\n\\end{align*}\n\n\nThis is valid for \\pi \\in [0, 1]."
  },
  {
    "objectID": "files/lectures/05-1-beta-binomial.html#binomial-data-model-4",
    "href": "files/lectures/05-1-beta-binomial.html#binomial-data-model-4",
    "title": "Beta-Binomial Model",
    "section": "Binomial Data Model",
    "text": "Binomial Data Model\n\nWhat is the likelihood of 30/50 voters supporting Michelle?\n\n\ndbinom(30, 50, pi)\n\n\nYou try this for \\pi = \\{0.25, 0.50, 0.75\\}.\n\n\ndbinom(30, 50, 0.25)\ndbinom(30, 50, 0.5)\ndbinom(30, 50, 0.75)"
  },
  {
    "objectID": "files/lectures/05-1-beta-binomial.html#binomial-data-model-5",
    "href": "files/lectures/05-1-beta-binomial.html#binomial-data-model-5",
    "title": "Beta-Binomial Model",
    "section": "Binomial Data Model",
    "text": "Binomial Data Model\n\nWhat is the likelihood of 30/50 voters supporting Michelle?\n\n\ndbinom(30, 50, 0.25)\n\n[1] 1.29633e-07\n\ndbinom(30, 50, 0.5)\n\n[1] 0.04185915\n\ndbinom(30, 50, 0.75)\n\n[1] 0.007654701"
  },
  {
    "objectID": "files/lectures/05-1-beta-binomial.html#binomial-data-model-6",
    "href": "files/lectures/05-1-beta-binomial.html#binomial-data-model-6",
    "title": "Beta-Binomial Model",
    "section": "Binomial Data Model",
    "text": "Binomial Data Model\n\nChallenge!\nCreate a graph showing what happens to the likelihood for different values of \\pi.\n\ni.e., have \\pi on the x-axis and likelihood on the y-axis.\n\nTo get you started,\n\n\ngraph &lt;- tibble(pi = seq(0, 1, 0.001)) %&gt;%\n  mutate(likelihood = dbinom(30, 50, pi))"
  },
  {
    "objectID": "files/lectures/05-1-beta-binomial.html#binomial-data-model-7",
    "href": "files/lectures/05-1-beta-binomial.html#binomial-data-model-7",
    "title": "Beta-Binomial Model",
    "section": "Binomial Data Model",
    "text": "Binomial Data Model\n\nCreate a graph showing what happens to the likelihood for different values of \\pi.\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhere is the maximum?"
  },
  {
    "objectID": "files/lectures/05-1-beta-binomial.html#binomial-data-model-8",
    "href": "files/lectures/05-1-beta-binomial.html#binomial-data-model-8",
    "title": "Beta-Binomial Model",
    "section": "Binomial Data Model",
    "text": "Binomial Data Model\n\nWhere is the maximum?\n\n\n\n\nError in `geom_text()`:\n! Problem while setting up geom aesthetics.\nℹ Error occurred in the 3rd layer.\nCaused by error in `list_sizes()`:\n! `x$label` must be a vector, not a &lt;latexexpression/expression&gt; object."
  },
  {
    "objectID": "files/lectures/05-1-beta-binomial.html#the-beta-posterior-model",
    "href": "files/lectures/05-1-beta-binomial.html#the-beta-posterior-model",
    "title": "Beta-Binomial Model",
    "section": "The Beta Posterior Model",
    "text": "The Beta Posterior Model\n\nLooking at just the prior and the data distributions,\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe prior is a bit more pessimistic about Michelle’s election support than the data obtained from the latest poll."
  },
  {
    "objectID": "files/lectures/05-1-beta-binomial.html#the-beta-posterior-model-1",
    "href": "files/lectures/05-1-beta-binomial.html#the-beta-posterior-model-1",
    "title": "Beta-Binomial Model",
    "section": "The Beta Posterior Model",
    "text": "The Beta Posterior Model\n\nNow including the posterior,\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe can see that the posterior model of \\pi is continuous and \\in [0, 1].\nThe shape of the posterior appears to also have a Beta(\\alpha, \\beta) model.\n\nThe shape parameters (\\alpha and \\beta) have been updated."
  },
  {
    "objectID": "files/lectures/05-1-beta-binomial.html#the-beta-posterior-model-2",
    "href": "files/lectures/05-1-beta-binomial.html#the-beta-posterior-model-2",
    "title": "Beta-Binomial Model",
    "section": "The Beta Posterior Model",
    "text": "The Beta Posterior Model\n\nIf we were to collect more information about Michelle’s support, we would use the current posterior as the new prior, then update our posterior.\n\nHow do we know what the updated parameters are?\n\n\n\nsummarize_beta_binomial(alpha = 45, beta = 55, y = 30, n = 50)"
  },
  {
    "objectID": "files/lectures/05-1-beta-binomial.html#the-beta-posterior-model-3",
    "href": "files/lectures/05-1-beta-binomial.html#the-beta-posterior-model-3",
    "title": "Beta-Binomial Model",
    "section": "The Beta Posterior Model",
    "text": "The Beta Posterior Model\n\nWe used Michelle’s election support to understand the Beta-Binomial model.\nLet’s now generalize it for any appropriate situation.\n\n\n\\begin{align*}\nY|\\pi &\\sim \\text{Bin}(n, \\pi) \\\\\n\\pi &\\sim \\text{Beta}(\\alpha, \\beta) \\\\\n\\pi | (Y=y) &\\sim \\text{Beta}(\\alpha+y, \\beta+n-y)\n\\end{align*}\n\n\nWe can see that the posterior distribution reveals the influence of the prior (\\alpha and \\beta) and data (y and n)."
  },
  {
    "objectID": "files/lectures/05-1-beta-binomial.html#the-beta-posterior-model-4",
    "href": "files/lectures/05-1-beta-binomial.html#the-beta-posterior-model-4",
    "title": "Beta-Binomial Model",
    "section": "The Beta Posterior Model",
    "text": "The Beta Posterior Model\n\nUnder this updated distribution,\n\n\n\\pi | (Y=y) \\sim \\text{Beta}(\\alpha+y, \\beta+n-y)\n\n\nwe have updated moments:\n\n\n\\begin{align*}\nE[\\pi | Y = y] &= \\frac{\\alpha + y}{\\alpha + \\beta + n} \\\\\n\\text{Var}[\\pi|Y=y] &= \\frac{(\\alpha+y)(\\beta+n-y)}{(\\alpha+\\beta+n)^2(\\alpha+\\beta+1)}\n\\end{align*}"
  },
  {
    "objectID": "files/lectures/05-1-beta-binomial.html#the-beta-posterior-model-5",
    "href": "files/lectures/05-1-beta-binomial.html#the-beta-posterior-model-5",
    "title": "Beta-Binomial Model",
    "section": "The Beta Posterior Model",
    "text": "The Beta Posterior Model\n\nLet’s pause and think about this from a theoretical standpoint.\nThe Beta distribution is a conjugate prior for the likelihood.\n\nConjugate prior: the posterior is from the same model family as the prior.\n\nRecall the Beta prior, f(\\pi),\n\n L(\\pi|y) = {n \\choose y} \\pi^y (1-\\pi)^{n-y} \n\nand the likelihood function, L(\\pi|y).\n\n f(\\pi) = \\frac{\\Gamma(\\alpha+\\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)} \\pi^{\\alpha-1}(1-\\alpha)^{\\beta-1}"
  },
  {
    "objectID": "files/lectures/05-1-beta-binomial.html#the-beta-posterior-model-6",
    "href": "files/lectures/05-1-beta-binomial.html#the-beta-posterior-model-6",
    "title": "Beta-Binomial Model",
    "section": "The Beta Posterior Model",
    "text": "The Beta Posterior Model\n\nWe can put the prior and likelihood together to create the posterior,\n\n\n\\begin{align*}\nf(\\pi|y) &\\propto f(\\pi)L(\\pi|y) \\\\\n&= \\frac{\\Gamma(\\alpha+\\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)} \\pi^{\\alpha-1}(1-\\pi)^{\\beta-1} \\times {n \\choose y} \\pi^y (1-\\pi)^{n-1} \\\\\n&\\propto \\pi^{(\\alpha+y)-1} (1-\\pi)^{(\\beta+n-y)-1}\n\\end{align*}\n\n\nThis is the same structure as the normalized Beta(\\alpha+y, \\beta+n-y),\n\nf(\\pi|y) = \\frac{\\Gamma(\\alpha+\\beta+n)}{\\Gamma(\\alpha+y) \\Gamma(\\beta+n-y)} \\pi^{(\\alpha+y)-1} (1-\\pi)^{(\\beta+n-y)-1}"
  },
  {
    "objectID": "files/lectures/05-1-beta-binomial.html#beta-binomial-example",
    "href": "files/lectures/05-1-beta-binomial.html#beta-binomial-example",
    "title": "Beta-Binomial Model",
    "section": "Beta-Binomial: Example",
    "text": "Beta-Binomial: Example\n\nIn Mario Kart 8 Deluxe, item boxes give different items depending on race position. To reduce the “position bias,” only item boxes opened while the racer was in mid-pack (positions 4–10) were recorded.\nYou want to estimate the probability that an item box yields a Red Shell. When playing the Special Cup, only 31 red shells were seen in 114 boxes opened by mid-pack racers.\nFind the posterior distribution under two priors:\n\nFlat/uninformative prior, Beta(1,1).\nBeta(\\alpha, \\beta) of your choosing."
  },
  {
    "objectID": "files/lectures/05-1-beta-binomial.html#wrap-up-beta-binomial-model",
    "href": "files/lectures/05-1-beta-binomial.html#wrap-up-beta-binomial-model",
    "title": "Beta-Binomial Model",
    "section": "Wrap Up: Beta-Binomial Model",
    "text": "Wrap Up: Beta-Binomial Model\n\nWe have built the Beta-Binomial model for \\pi, an unknown proportion.\n\n\n\\begin{equation*}\n\\begin{aligned}\nY|\\pi &\\sim \\text{Bin}(n,\\pi) \\\\\n\\pi &\\sim \\text{Beta}(\\alpha,\\beta) &\n\\end{aligned}\n\\Rightarrow\n\\begin{aligned}\n&& \\pi | (Y=y) &\\sim \\text{Beta}(\\alpha+y, \\beta+n-y) \\\\\n\\end{aligned}\n\\end{equation*}\n\n\nThe prior model, f(\\pi), is given by Beta(\\alpha,\\beta).\nThe data model, f(Y|\\pi), is given by Bin(n,\\pi).\nThe likelihood function, L(\\pi|y), is obtained by plugging y into the Binomial pmf.\nThe posterior model is a Beta distribution with updated parameters \\alpha+y and \\beta+n-y."
  },
  {
    "objectID": "files/lectures/05-1-beta-binomial.html#homework-practice",
    "href": "files/lectures/05-1-beta-binomial.html#homework-practice",
    "title": "Beta-Binomial Model",
    "section": "Homework / Practice",
    "text": "Homework / Practice\n\nFrom the Bayes Rules! textbook:\n\n3.3\n3.9\n3.10\n3.18"
  },
  {
    "objectID": "files/assignments/assignment2.html",
    "href": "files/assignments/assignment2.html",
    "title": "Assignment 2: Bayesian Inference",
    "section": "",
    "text": "All data for this project can be found here: Google Sheets\n1. Belle has taken over the castle’s grand library and is determined to restore its warm, welcoming glow. The enchanted lanterns are meant to light automatically each evening, but ever since the enchantress’s curse, not all of them behave reliably.\nFrom past experience, Lumière estimates that somewhere between 50% and 70% of the lanterns light properly on any given night. When he is pressed for a single (point) estimate, he says that about 60% of the lanterns light properly. However, Belle suspects that things are improving as the castle’s magic grows stronger… perhaps the true proportion of working lanterns is now higher?\nTo test this, she records data for a week evenings. Each night, she notes how many of the 10 enchanted lanterns successfully light when the sun sets.\n1a. What modeling approach is appropriate for this scenario? Explain why.\nInsert your answer here.\n1b. Find an appropriate prior given the information provided. Justify your choice.\nInsert your answer here.\n1c. Find the posterior distribution in light of the observed data.\nInsert your answer here.\n1d. Construct the appropriate hypothesis test (in the Bayesian framework) to determine if the proportion of lanterns that successfully light has increased.\n\nHypotheses:\n\nH_0:\nH_1:\n\nDecision information:\n\nPrior probabilities:\n\nPrior odds:\n\nPosterior probabilities:\n\nPosterior odds:\n\nBayes Factor:\n95% credible interval:\n\nConclusion and interpretation:\n\n2. As the castle’s magic returns, Cogsworth has been busy ensuring that every enchanted clock in the west wing ticks properly. Each evening, he listens carefully for the number of clocks that chime on time during the first hour after sunset.\nBefore Belle’s arrival, the castle staff believed that the average number of properly chiming clocks per hour was around 5, but they are not too confident about that estimate. However, as the curse weakens, Cogsworth suspects the clocks are performing better than before. To check, he records data over seven evenings.\n2a. What modeling approach is appropriate for this scenario? Explain why.\nInsert your answer here.\n2b. Find an appropriate prior given the information provided. Justify your choice.\nInsert your answer here.\n2c. Find the posterior distribution in light of the observed data.\nInsert your answer here.\n2d. Construct the appropriate hypothesis test (in the Bayesian framework) to determine if the number of clocks that properly chime has increased.\n\nHypotheses:\n\nH_0:\nH_1:\n\nDecision information:\n\nPrior probabilities:\n\nPrior odds:\n\nPosterior probabilities:\n\nPosterior odds:\n\nBayes Factor:\n95% credible interval:\n\nConclusion and interpretation:\n\n3. Mrs. Potts prides herself on brewing tea at just the right temperature: not too hot to startle a guest, not too cool to lose its charm. She believes that when the castle’s magic was weaker, the average serving temperature of her enchanted teapot’s pours was about 85°C, with a standard deviation of about 3°C.\nNow that the curse has nearly lifted, Mrs. Potts suspects her tea is coming out hotter on average. To investigate, she measures the temperature (in °C) of her pours over seven evenings.\n3a. What modeling approach is appropriate for this scenario? Explain why.\nInsert your answer here.\n3b. Find an appropriate prior given the information provided. Justify your choice.\nInsert your answer here.\n3c. Find the posterior distribution in light of the observed data.\nInsert your answer here.\n3d. Construct the appropriate hypothesis test (in the Bayesian framework) to determine if the tea is hotter when poured.\n\nHypotheses:\n\nH_0:\nH_1:\n\nDecision information:\n\nPrior probabilities:\n\nPrior odds:\n\nPosterior probabilities:\n\nPosterior odds:\n\nBayes Factor:\n95% credible interval:\n\nConclusion and interpretation:"
  },
  {
    "objectID": "files/assignments/assignment1.html",
    "href": "files/assignments/assignment1.html",
    "title": "Assignment 1: Probability Distributions",
    "section": "",
    "text": "Note: do not read too much into the following situations. The purpose is to get comfortable identifying distributions and finding corresponding probabilities.\nSituation 1: Walter is known for his intense personality during bowling league nights. Whether it’s a foot foul, a scoring dispute, or just general frustration with life, he frequently loses his temper and yells loudly at his teammates or opponents. Based on observations over several league nights, it has been determined that Walter loses his temper at an average rate of 4 outbursts per hour. His outbursts are unpredictable and can happen at any point during the game, but they seem to occur independently of each other.\na. What is the random variable of interest? (What is being measured?)\nReplace with your answer\nb. What distribution is appropriate and why?\nReplace with your answer\nc. What are the parameters of the distribution?\nReplace with your answer\nd. What is the probability that Walter will not have an outburst at all?\nReplace with your answer (i.e., don’t just show me R output here :))\ne. What is the probability that Walter has more than 10 outbursts during league night?\nReplace with your answer (i.e., don’t just show me R output here :))\nScenario 2: Donny often struggles to find parking near the bowling alley, especially on busy league nights. Sometimes he gets lucky and finds a spot fairly quickly, but other times he ends up circling the block several times before he can finally park. His parking times are generally right-skewed as it is much more common for it to take a shorter amount of time… but every now and then it can take quite a while. Over time, his friends have noticed a pattern: it usually takes him a few minutes, but there is a decent chance it could take significantly longer.\na. What is the random variable of interest? (What is being measured?)\nReplace with your answer\nb. What distribution is appropriate and why?\nReplace with your answer\nc. What are the parameters of the distribution? (Hint: for this situation, they are 3 and 4 (in that order)).\nReplace with your answer\nd. What is the probability that Donny will take more than 15 minutes to find a spot?\nReplace with your answer (i.e., don’t just show me R output here :))\ne. What is the probability that Donny will find a parking space quickly – i.e., that he parks within 5 minutes of arriving?\nReplace with your answer (i.e., don’t just show me R output here :))\nSituation 3: Maude is famous for her avant-garde dance performances that leave audiences both puzzled and mesmerized. Her performances at the local art venue are known for their spontaneity and lack of strict timing. Observers have noted that the duration of her performances can last anywhere between 15 and 30 minutes, with any length within this interval being equally likely. There’s no predicting exactly how long Maude will choose to perform on any given night… sometimes it’s a brief expression and sometimes it stretches to the edge of the audience’s patience.\na. What is the random variable of interest? (What is being measured?)\nReplace with your answer\nb. What distribution is appropriate and why?\nReplace with your answer\nc. What are the parameters of the distribution?\nReplace with your answer\nd. What is the probability that Maude’s performance will be on the shorter end – i.e., it lasts somewhere between 15 and 16 minutes?\nReplace with your answer (i.e., don’t just show me R output here :))\ne. What is the probability that Maude’s performance will be on the longer end – i.e., it lasts somewhere between 25 and 30 minutes?\nReplace with your answer (i.e., don’t just show me R output here :))\nScenario 4. The Dude always orders a White Russian when he’s at the bowling alley. However, the bowling alley bar is not always well-stocked. Each time The Dude places an order, there is only a 60% chance that the bartender has all the necessary ingredients on hand to make the drink (they are frequently out of cream). Throughout one particularly long evening of bowling and philosophical debates, The Dude orders 8 White Russians. He hopes that most of them will be successfully made.\na. What is the random variable of interest? (What is being measured?)\nReplace with your answer\nb. What distribution is appropriate and why?\nReplace with your answer\nc. What are the parameters of the distribution?\nReplace with your answer\nd. What is the probability that at least 5 drinks will be successfully made?\nReplace with your answer (i.e., don’t just show me R output here :))\ne. What is the probability that all 8 drinks are successfully made?\nReplace with your answer (i.e., don’t just show me R output here :))\nScenario 5: The Dude’s bowling games tend to last around 45 minutes, give or take. While the exact length can vary from game to game, it’s rare for his games to be extremely short or unusually long. Most of the time, his game lengths cluster fairly tightly around that 45-minute mark, though occasionally he’ll have a quicker match or one that drags on a little longer if the vibe calls for it, to the tune of \\pm 5 minutes.\na. What is the random variable of interest? (What is being measured?)\nReplace with your answer\nb. What distribution is appropriate and why?\nReplace with your answer\nc. What are the parameters of the distribution?\nReplace with your answer\nd. What is the probability that the game will last exactly 45 minutes?\nReplace with your answer (i.e., don’t just show me R output here :))\ne. What is the probability that the game will really drag on – that it lasts somewhere between 50 and 60 minutes?\nReplace with your answer (i.e., don’t just show me R output here :))\nScenario 6: After many laid-back bowling sessions (and several White Russians), The Dude has started keeping track of how often he lands a strike. He knows he’s not perfect, but he’s developed a pretty good feel for his overall strike success rate. It seems like his strike percentage tends to hang out somewhere between 60% and 80% most nights, though there’s still some uncertainty. Sometimes he’s on fire, sometimes he’s a little off, but overall he’s confident his long-term strike rate leans more toward success than failure.\na. What is the random variable of interest? (What is being measured?)\nReplace with your answer\nb. What distribution is appropriate and why?\nReplace with your answer\nc. What are the parameters of the distribution? (Hint: for this example, they are 7 and 3 (in that order)).\nReplace with your answer\nd. What is the probability that The Dude’s strike success rate is less than 50%?\nReplace with your answer (i.e., don’t just show me R output here :))\ne. What is the probability that The Dude’s strike success rate is above 80%?\nReplace with your answer (i.e., don’t just show me R output here :))"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "STA6349 - Applied Bayesian Analysis - Fall 2025",
    "section": "",
    "text": "Welcome to Applied Bayesian Analysis for Fall 2025!\nThe initial development of this course was supported by the 2023 BATS program, funded by NSF IUSE: EHR program with award numbers 2215879, 2215920, and 2215709."
  },
  {
    "objectID": "files/assignments/assignment3.html",
    "href": "files/assignments/assignment3.html",
    "title": "Assignment 3: Bayesian Regression Analysis",
    "section": "",
    "text": "1. You are a Data Analyst on the Card Operations Analytics Team at the Equestrian Credit Union (ECU). Your team supports internal business partners by identifying performance drivers and operational bottlenecks across member service workflows. Recently, senior management requested an analysis of case processing time for member disputes handled by two internal departments: Operations and Fraud. The goal is to better understand how analyst experience and workload influence resolution time and to identify potential areas for process improvement.\nConsider the data here: click for Google Sheet\n\nProcessingTime – time to resolution (minutes)\nTeam – team of employment; either Fraud or Operations\nExperience – length of employment (years)\nWorkload – number of open cases\n\n1a. Import the data.\n\n1b. Construct the appropriate model to answer the following question: What factors explain variation in average case processing time, and does the impact of workload depend on analyst experience?\n\nRemember to state the resulting model. \n1c. Construct the appropriate table to display results. Recall that you should display \\hat{\\beta}_i \\ (95\\% \\text{ CI for } \\beta_i) or \\exp\\{\\hat{\\beta}_i\\} \\ (95\\% \\text{ CI for } \\exp\\{\\beta_i\\}).\nInsert your table here. \n1d. Let’s investigate the relationships observed. Create predicted values for those on the Fraud team with {0, 5, 10, 15} years of experience.\n\n1e. Let’s investigate the relationships observed. Create predicted values for those on the Fraud team with {1, 5, 10, 20} open cases.\n\n1f. Let’s investigate the relationships observed. Construct the following graph, showing the relationship between processing time and number of cases:\n\nScatterplot: x = Workload, y = ProcessingTime\nRegression lines for the fraud team: multiple geom_line()s calling aes(y = predicted_column).\n\n1g. Let’s investigate the relationships observed. Construct the following graph, showing the relationship between processing time and experience:\n\nScatterplot: x = Experience, y = ProcessingTime\nRegression lines for the fraud team: multiple geom_line()s calling aes(y = predicted_column).\n\n1h. Is there evidence of an interaction effect? Why or why not? Use your model output as well as graphs above to answer this question.\nInsert your answer here.\n1i. Write a brief summary of your model results. How would you explain the results to your boss?\nInsert your answer here.\n\n2. You are now a Data Analyst on the Member Insights & Retention Team at ECU. Your department collaborates with the Marketing and Contact Center Operations teams to identify members at risk of closing their accounts or discontinuing services. Member retention has become a top strategic priority following recent internal reports showing increased attrition among certain demographics.\nConsider the data here: click for Google Sheet\n\nClosedAccount – whether the member closed their account within the last 12 months (1 = yes, 0 = no)\nAge – member age (years)\nDigitalEngagement – average number of monthly logins to ECU digital platforms (mobile app, website)\nAccountType – checking, savings, or credit card\n\n2a. Import the data.\n\n2b. Construct the appropriate model to answer the following question: What member characteristics are associated with a higher likelihood of account closure, and does the risk of closing an account based on digital engagement vary by account type?\n\nRemember to state the resulting model. \n2c. Construct the appropriate table to display results. Recall that you should display \\hat{\\beta}_i \\ (95\\% \\text{ CI for } \\beta_i) or \\exp\\{\\hat{\\beta}_i\\} \\ (95\\% \\text{ CI for } \\exp\\{\\beta_i\\}).\nInsert your table here. \n2d. Let’s investigate the relationships observed. Create predicted values for members aged {18, 30, 45, 60} with checkings accounts.\n\n2e. Let’s investigate the relationships observed. Create predicted values for members aged {18, 30, 45, 60} with savings accounts.\n\n2f. Let’s investigate the relationships observed. Create predicted values for members aged {18, 30, 45, 60} with credit card accounts.\n\n2g. Let’s investigate the relationships observed. Construct the following graph, showing the relationship between account closure and digital engagement:\n\nScatterplot: x = DigitalEngagement, y = ClosedAccount\nRegression lines for the those with checkings accounts: multiple geom_line()s calling aes(y = predicted_column). \n\n2h. Let’s investigate the relationships observed. Construct the following graph, showing the relationship between account closure and digital engagement:\n\nScatterplot: x = DigitalEngagement, y = ClosedAccount\nRegression lines for the those with savings accounts: multiple geom_line()s calling aes(y = predicted_column). \n\n2i. Let’s investigate the relationships observed. Construct the following graph, showing the relationship between account closure and digital engagement:\n\nScatterplot: x = DigitalEngagement, y = ClosedAccount\nRegression lines for the those with credit card accounts: multiple geom_line()s calling aes(y = predicted_column). \n\n2j. Let’s investigate the relationships observed. Construct the following graph, showing the relationship between account closure and digital engagement:\n\nScatterplot: x = DigitalEngagement, y = ClosedAccount\nRegression lines for 18 year olds: multiple geom_line()s calling aes(y = predicted_column). \n\n2k. Let’s investigate the relationships observed. Construct the following graph, showing the relationship between account closure and digital engagement:\n\nScatterplot: x = DigitalEngagement, y = ClosedAccount\nRegression lines for 45 year olds: multiple geom_line()s calling aes(y = predicted_column). \n\n2l. Is there evidence of an interaction effect? Why or why not? Use your model output as well as graphs above to answer this question.\nInsert your answer here.\n2m. Write a brief summary of your model results. How would you explain the results to your boss?\nInsert your answer here.\n\n3. You are now a Data Analyst on the Fraud Monitoring & Risk Analytics team at ECU. The team tracks operational risk and partner performance across card portfolios. Senior leadership wants to understand what drives the weekly number of fraud alerts escalated to manual review so they can allocate staff and guide fraud-control investments.\nConsider the data here: click for Google Sheet\n\nAlerts – number of fraud alerts needing manual review that week\nPortfolio – type of portfolio (checking only, savings only, mixed)\nAvgTicket – average transaction amount that week (bits)\nTxnVolume – total number of transactions processed that week\n\n3a. Import the data.\n\n3b. Construct the appropriate model to answer the following questions: What factors explain variation in the count of weekly fraud alerts, after accounting for weekly transaction volume? In particular, what are the differences in portfolio type?\n\nRemember to state the resulting model. \n3c. Construct the appropriate table to display results. Recall that you should display \\hat{\\beta}_i \\ (95\\% \\text{ CI for } \\beta_i) or \\exp\\{\\hat{\\beta}_i\\} \\ (95\\% \\text{ CI for } \\exp\\{\\beta_i\\}).\nInsert your table here. \n3d. Let’s investigate the relationships observed. Create predicted values for weeks with 40,000 transactions.\n\n3e. Let’s investigate the relationships observed. Construct the following graph, showing the relationship between account closure and digital engagement:\n\nScatterplot: x = AvgTicket, y = Alerts\nRegression lines for the those with savings accounts: multiple geom_line()s calling aes(y = predicted_column). \n\n3f. Write a brief summary of your model results. How would you explain the results to your boss?\nInsert your answer here."
  },
  {
    "objectID": "files/assignments/keys/assignment2_KEY.html",
    "href": "files/assignments/keys/assignment2_KEY.html",
    "title": "Assignment 2: Bayesian Inference",
    "section": "",
    "text": "All data for this project can be found here: Google Sheets\n\nlibrary(tidyverse)\nlibrary(bayesrules)\n\n1. Belle has taken over the castle’s grand library and is determined to restore its warm, welcoming glow. The enchanted lanterns are meant to light automatically each evening, but ever since the enchantress’s curse, not all of them behave reliably.\nFrom past experience, Lumière estimates that somewhere between 50% and 70% of the lanterns light properly on any given night. When he is pressed for a single (point) estimate, he says that about 60% of the lanterns light properly. However, Belle suspects that things are improving as the castle’s magic grows stronger… perhaps the true proportion of working lanterns is now higher?\nTo test this, she records data for a week evenings. Each night, she notes how many of the 10 enchanted lanterns successfully light when the sun sets.\n1a. What modeling approach is appropriate for this scenario? Explain why.\nInsert your answer here.\n1b. Find an appropriate prior given the information provided. Justify your choice.\nInsert your answer here.\n1c. Find the posterior distribution in light of the observed data.\nInsert your answer here.\n1d. Construct the appropriate hypothesis test (in the Bayesian framework) to determine if the proportion of lanterns that successfully light has increased.\n\nHypotheses:\n\nH_0:\nH_1:\n\nDecision information:\n\nPrior probabilities:\n\nPrior odds:\n\nPosterior probabilities:\n\nPosterior odds:\n\nBayes Factor:\n95% credible interval:\n\nConclusion and interpretation:\n\n2. As the castle’s magic returns, Cogsworth has been busy ensuring that every enchanted clock in the west wing ticks properly. Each evening, he listens carefully for the number of clocks that chime on time during the first hour after sunset.\nBefore Belle’s arrival, the castle staff believed that the average number of properly chiming clocks per hour was around 5, but they are not too confident about that estimate. However, as the curse weakens, Cogsworth suspects the clocks are performing better than before. To check, he records data over seven evenings.\n2a. What modeling approach is appropriate for this scenario? Explain why.\nInsert your answer here.\n2b. Find an appropriate prior given the information provided. Justify your choice.\nInsert your answer here.\n2c. Find the posterior distribution in light of the observed data.\nInsert your answer here.\n2d. Construct the appropriate hypothesis test (in the Bayesian framework) to determine if the number of clocks that properly chime has increased.\n\nHypotheses:\n\nH_0:\nH_1:\n\nDecision information:\n\nPrior probabilities:\n\nPrior odds:\n\nPosterior probabilities:\n\nPosterior odds:\n\nBayes Factor:\n95% credible interval:\n\nConclusion and interpretation:\n\n3. Mrs. Potts prides herself on brewing tea at just the right temperature: not too hot to startle a guest, not too cool to lose its charm. She believes that when the castle’s magic was weaker, the average serving temperature of her enchanted teapot’s pours was about 85°C, with a standard deviation of about 3°C.\nNow that the curse has nearly lifted, Mrs. Potts suspects her tea is coming out hotter on average. To investigate, she measures the temperature (in °C) of her pours over seven evenings.\n3a. What modeling approach is appropriate for this scenario? Explain why.\nInsert your answer here.\n3b. Find an appropriate prior given the information provided. Justify your choice.\nInsert your answer here.\n3c. Find the posterior distribution in light of the observed data.\nInsert your answer here.\n3d. Construct the appropriate hypothesis test (in the Bayesian framework) to determine if the tea is hotter when poured.\n\nHypotheses:\n\nH_0:\nH_1:\n\nDecision information:\n\nPrior probabilities:\n\nPrior odds:\n\nPosterior probabilities:\n\nPosterior odds:\n\nBayes Factor:\n95% credible interval:\n\nConclusion and interpretation:"
  },
  {
    "objectID": "files/lectures/09-2-evaluating-model.html#introduction",
    "href": "files/lectures/09-2-evaluating-model.html#introduction",
    "title": "Evaluating Regression Models",
    "section": "Introduction",
    "text": "Introduction\n\nWe want to now learn how to evaluate how good our regression model is.\nHow fair is the model?\n\nHow was the data collected?\nBy whom and for what purpose?\nHow might the results of the data collection or analysis impact individuals and society?\nWhat biases might be baked into the analysis?"
  },
  {
    "objectID": "files/lectures/09-2-evaluating-model.html#introduction-1",
    "href": "files/lectures/09-2-evaluating-model.html#introduction-1",
    "title": "Evaluating Regression Models",
    "section": "Introduction",
    "text": "Introduction\n\nWe want to now learn how to evaluate how good our regression model is.\nHow wrong is the model?\n\n“All models are wrong, but some are useful.” - George E.P. Box\nAre our assumptions reasonable?\n\nHow accurate are the posterior predictive models?\n\nHow far are the posterior predictive models from reality?"
  },
  {
    "objectID": "files/lectures/09-2-evaluating-model.html#is-the-model-fair",
    "href": "files/lectures/09-2-evaluating-model.html#is-the-model-fair",
    "title": "Evaluating Regression Models",
    "section": "Is the Model Fair?",
    "text": "Is the Model Fair?\n\nConsider our example from last lecture, the Capital Bikeshare data.\nWe simulated under the following model:\n\n\n\\begin{align*}\nY_i | \\beta_0, \\beta_1, \\sigma &\\overset{\\text{ind}}{\\sim} N(\\beta_0 + \\beta_1X_i,\\sigma^2) \\\\\n\\beta_{0\\text{c}} &\\sim N(5000, 1000^2) \\\\\n\\beta_{1\\text{c}} &\\sim N(100, 40^2) \\\\\n\\sigma &\\sim \\text{Exp}(0.0008)\n\\end{align*}"
  },
  {
    "objectID": "files/lectures/09-2-evaluating-model.html#is-the-model-fair-1",
    "href": "files/lectures/09-2-evaluating-model.html#is-the-model-fair-1",
    "title": "Evaluating Regression Models",
    "section": "Is the Model Fair?",
    "text": "Is the Model Fair?\n\nConsider our example from last lecture, the Capital Bikeshare data.\nHow was the data collected?\n\nIt is reasonable to assume that this was collected electronically.\nWe do not have individual-level information, only aggregated ride data.\n\nBy whom and for what purpose was the data collected?\n\nIt is reasonable for a bike sharing company to track data such as number of rides in a day.\nThe company is likely doing this to make evidence-based business decisions."
  },
  {
    "objectID": "files/lectures/09-2-evaluating-model.html#is-the-model-fair-2",
    "href": "files/lectures/09-2-evaluating-model.html#is-the-model-fair-2",
    "title": "Evaluating Regression Models",
    "section": "Is the Model Fair?",
    "text": "Is the Model Fair?\n\nConsider our example from last lecture, the Capital Bikeshare data.\nHow might the results of the analysis, or the data collection itself, impact individuals and society?\n\nBecause there is no individual-level information, this will not negatively impact customers.\nImproving the service might help reduce car traffic in D.C. - this is good!\n\nNote!\n\nJust because we cannot imagine societal impacts does not mean that none exist.\nIt’s critical to recognize our perspective based on our experiences.\nHow can we truly determine the impacts?\n\nWe would need to engage with the company and its community."
  },
  {
    "objectID": "files/lectures/09-2-evaluating-model.html#is-the-model-fair-3",
    "href": "files/lectures/09-2-evaluating-model.html#is-the-model-fair-3",
    "title": "Evaluating Regression Models",
    "section": "Is the Model Fair?",
    "text": "Is the Model Fair?\n\nWhat biases might be baked into this analysis?\n\nThe data is only for one company.\n\nOther bikeshare companies may have different usage patterns.\n\nThe data is only for one city.\n\nOther cities may have different usage patterns for bikeshare.\nOther cities will have different weather patterns."
  },
  {
    "objectID": "files/lectures/09-2-evaluating-model.html#is-the-model-fair-4",
    "href": "files/lectures/09-2-evaluating-model.html#is-the-model-fair-4",
    "title": "Evaluating Regression Models",
    "section": "Is the Model Fair?",
    "text": "Is the Model Fair?\n\nHow do I approach thinking about bias in analysis?\n\nWho is and is not represented in the data?\nWhat assumptions were made about the data?\nWhat assumptions are made about the truth/the population?\nIs the analysis method appropriate for the question posed?\n\ne.g., did they use logistic regression for a binary outcome?\n\nWere there any “reaching” assumptions made in order to complete the analysis?\n\ne.g., assuming data is MCAR when it is likely MAR or MNAR.\ne.g., assuming independent observations when there is likely correlation.\n\nAre the results stated appropriately? (vs. overstated)\n\nAre broad, sweeping generalizations made from a narrow dataset?"
  },
  {
    "objectID": "files/lectures/09-2-evaluating-model.html#how-wrong-is-the-model",
    "href": "files/lectures/09-2-evaluating-model.html#how-wrong-is-the-model",
    "title": "Evaluating Regression Models",
    "section": "How Wrong is the Model?",
    "text": "How Wrong is the Model?\n\nRecall the data model,\n\n\nY_i | \\beta_0, \\beta_1, \\sigma \\overset{\\text{ind}}{\\sim} N(\\beta_0 + \\beta_1X_i,\\sigma^2)\n\n\nWhat are the assumptions on our model?\n\nThe observed data, Y_i, is independent of other observed data, Y_j for i \\neq j.\nY can be written as a linear function of X, \\mu=\\beta_0 + \\beta_1 X.\nAt any X, Y varys normally around \\mu with consistent variability, \\sigma."
  },
  {
    "objectID": "files/lectures/09-2-evaluating-model.html#how-wrong-is-the-model-1",
    "href": "files/lectures/09-2-evaluating-model.html#how-wrong-is-the-model-1",
    "title": "Evaluating Regression Models",
    "section": "How Wrong is the Model?",
    "text": "How Wrong is the Model?\n\nThe observed data, Y_i, is independent of other observed data, Y_j for i \\neq j.\n\nWe can technically test for this, but it is not often employed by statisticians.\nInstead, we rely on knowledge of the data collection process.\nIf the data collection process suggests dependence, we should consider a different model.\n\ne.g., a modeling approach that will account for the correlation between observations."
  },
  {
    "objectID": "files/lectures/09-2-evaluating-model.html#how-wrong-is-the-model-2",
    "href": "files/lectures/09-2-evaluating-model.html#how-wrong-is-the-model-2",
    "title": "Evaluating Regression Models",
    "section": "How Wrong is the Model?",
    "text": "How Wrong is the Model?\n\nThe observed data, Y_i, is independent of other observed data, Y_j for i \\neq j.\nIn this example, the data are daily counts of bike rides.\n\nThere is inherently some correlation – the ridership today is likely correlated to the ridership yesterday.\nHowever, this can be explained by the time of year and features associated with the time of year, like temperature.\nWe should ask ourselves if it’s reasonable to assume that the temperature cancels out the time correlation.\n\n“Reasonable” does not mean “perfect,” but instead, “good enough to proceed.”"
  },
  {
    "objectID": "files/lectures/09-2-evaluating-model.html#how-wrong-is-the-model-3",
    "href": "files/lectures/09-2-evaluating-model.html#how-wrong-is-the-model-3",
    "title": "Evaluating Regression Models",
    "section": "How Wrong is the Model?",
    "text": "How Wrong is the Model?\n\nReturning to our bike data, we can look at the scatterplot of temperature (X) and number of rides (Y).\n\nWhat kind of relationship do we suspect?"
  },
  {
    "objectID": "files/lectures/09-2-evaluating-model.html#how-wrong-is-the-model-4",
    "href": "files/lectures/09-2-evaluating-model.html#how-wrong-is-the-model-4",
    "title": "Evaluating Regression Models",
    "section": "How Wrong is the Model?",
    "text": "How Wrong is the Model?\n\nWe can overlay a line of best fit using geom_smooth(method = \"lm\")."
  },
  {
    "objectID": "files/lectures/09-2-evaluating-model.html#how-wrong-is-the-model-5",
    "href": "files/lectures/09-2-evaluating-model.html#how-wrong-is-the-model-5",
    "title": "Evaluating Regression Models",
    "section": "How Wrong is the Model?",
    "text": "How Wrong is the Model?\n\nOr we can overlay a locally estimated scatterplot smoothing (LOESS) line using geom_smooth()."
  },
  {
    "objectID": "files/lectures/09-2-evaluating-model.html#how-wrong-is-the-model-6",
    "href": "files/lectures/09-2-evaluating-model.html#how-wrong-is-the-model-6",
    "title": "Evaluating Regression Models",
    "section": "How Wrong is the Model?",
    "text": "How Wrong is the Model?\n\nHow different are the two lines?"
  },
  {
    "objectID": "files/lectures/09-2-evaluating-model.html#how-wrong-is-the-model-7",
    "href": "files/lectures/09-2-evaluating-model.html#how-wrong-is-the-model-7",
    "title": "Evaluating Regression Models",
    "section": "How Wrong is the Model?",
    "text": "How Wrong is the Model?\n\nWhen does “visualization” suggest that a linear model is not appropriate?\n\nDoes the LOESS line deviates substantially from the linear line?\nHere, we see that the LOESS line is fairly close to the linear line.\nThus, it is reasonable to assume a linear relationship between X and Y.\n\nAre there “issues” with this approach?\n\nWhen we have more than one predictor, yes.\nWe are only visualizing one predictor at a time.\nWe will need to use other tools to assess linearity in multiple regression."
  },
  {
    "objectID": "files/lectures/09-2-evaluating-model.html#how-wrong-is-the-model-8",
    "href": "files/lectures/09-2-evaluating-model.html#how-wrong-is-the-model-8",
    "title": "Evaluating Regression Models",
    "section": "How Wrong is the Model?",
    "text": "How Wrong is the Model?\n\nTo examine linearity in multiple regression, we will perform a posterior predictive check.\nIf the combined model assumptions are reasonable, then our posterior model should be able to simulate ridership data that’s similar to the original 500 rides observations.\n\n\n\n\n\nFor each of the 20,000 posterior plausible parameter sets, \\{\\beta_0, \\beta_1, \\sigma\\}, we can predict ridership data from the observed temperature data.\n\nThis gives us 20,000 unique datasets of predicted ridership, each with n=500."
  },
  {
    "objectID": "files/lectures/09-2-evaluating-model.html#how-wrong-is-the-model-9",
    "href": "files/lectures/09-2-evaluating-model.html#how-wrong-is-the-model-9",
    "title": "Evaluating Regression Models",
    "section": "How Wrong is the Model?",
    "text": "How Wrong is the Model?\n\nRecall our analysis from last lecture,\n\n\nbike_model &lt;- stan_glm(rides ~ temp_feel, \n                       data = bikes, \n                       family = gaussian,\n                       prior_intercept = normal(5000, 1000), \n                       prior = normal(100, 40), \n                       prior_aux = exponential(0.0008), # sigma\n                       chains = 4, iter = 5000*2, seed = 84735) \n\n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 3.3e-05 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.33 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 1: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 1: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 1: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 1: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 1: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 1: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 1: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 1: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 1: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 1: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 1: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.128 seconds (Warm-up)\nChain 1:                0.208 seconds (Sampling)\nChain 1:                0.336 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 1e-05 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.1 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 2: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 2: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 2: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 2: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 2: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 2: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 2: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 2: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 2: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 2: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 2: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.161 seconds (Warm-up)\nChain 2:                0.216 seconds (Sampling)\nChain 2:                0.377 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 7e-06 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.07 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 3: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 3: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 3: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 3: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 3: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 3: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 3: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 3: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 3: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 3: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 3: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 0.13 seconds (Warm-up)\nChain 3:                0.2 seconds (Sampling)\nChain 3:                0.33 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 6e-06 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.06 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 4: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 4: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 4: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 4: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 4: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 4: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 4: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 4: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 4: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 4: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 4: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 0.114 seconds (Warm-up)\nChain 4:                0.209 seconds (Sampling)\nChain 4:                0.323 seconds (Total)\nChain 4:"
  },
  {
    "objectID": "files/lectures/09-2-evaluating-model.html#how-wrong-is-the-model-10",
    "href": "files/lectures/09-2-evaluating-model.html#how-wrong-is-the-model-10",
    "title": "Evaluating Regression Models",
    "section": "How Wrong is the Model?",
    "text": "How Wrong is the Model?\n\nRecall our analysis from last lecture,\n\n\ntidy(bike_model, conf.int = TRUE) %&gt;% select(-std.error)\n\n\n  \n\n\n\n\nThis gives the model,\n\n\\hat{y} = -2195.3 + 82.2x \\ \\ \\ \\to \\ \\ \\ \\hat{\\text{rides}} = -2195.3 + 82.2\\text{temp}"
  },
  {
    "objectID": "files/lectures/09-2-evaluating-model.html#how-wrong-is-the-model-11",
    "href": "files/lectures/09-2-evaluating-model.html#how-wrong-is-the-model-11",
    "title": "Evaluating Regression Models",
    "section": "How Wrong is the Model?",
    "text": "How Wrong is the Model?\n\nHow do we perform the posterior predictive check?\nThe pp_check() function plots a number of the corresponding simulated datasets.\n\n\npp_check(bike_model, nreps=50, seed = 84735) + xlab(\"Number of Rides\") + theme_bw()"
  },
  {
    "objectID": "files/lectures/09-2-evaluating-model.html#how-accurate-are-posterior-predictive-models",
    "href": "files/lectures/09-2-evaluating-model.html#how-accurate-are-posterior-predictive-models",
    "title": "Evaluating Regression Models",
    "section": "How Accurate are Posterior Predictive Models?",
    "text": "How Accurate are Posterior Predictive Models?\n\nAlthough we are interested in predicting the future, we can begin by evaluating how well it predicts the data that was used to build the model.\nWe can use the posterior_predict() function to generate posterior predictive summaries to either our data or new data.\n\n\npredictions &lt;- posterior_predict(bike_model, newdata=bikes, seed = 84735)\ndim(predictions)\n\n[1] 20000   500\n\n\n\nWe have 20,000 posterior predictive datasets, each with 500 predicted ride counts."
  },
  {
    "objectID": "files/lectures/09-2-evaluating-model.html#how-accurate-are-posterior-predictive-models-1",
    "href": "files/lectures/09-2-evaluating-model.html#how-accurate-are-posterior-predictive-models-1",
    "title": "Evaluating Regression Models",
    "section": "How Accurate are Posterior Predictive Models?",
    "text": "How Accurate are Posterior Predictive Models?\n\nWe can also use the ppc_intervals() function to visualize the posterior predictive intervals for each observation.\n\n\nppc_intervals(bikes$rides, yrep = predictions, x = bikes$temp_feel, \n              prob = 0.5, prob_outer = 0.95) + theme_bw()"
  },
  {
    "objectID": "files/lectures/09-2-evaluating-model.html#how-accurate-are-posterior-predictive-models-2",
    "href": "files/lectures/09-2-evaluating-model.html#how-accurate-are-posterior-predictive-models-2",
    "title": "Evaluating Regression Models",
    "section": "How Accurate are Posterior Predictive Models?",
    "text": "How Accurate are Posterior Predictive Models?\n\nLet Y_1, Y_2, ..., Y_n denote n observed outcomes. Each Y_i has a corresponding posterior predictive model with mean Y_i' and standard deviation \\text{sd}_i\nMedian Absolute Error (MAE): the typical difference between the observed value and posterior predictive mean.\n\n\\text{MAE} = \\text{median} | Y_i - Y_i'|\n\nScaled Median Absolute Error (MAE scaled): the typical difference between the observed value and posterior predictive mean.\n\n\\text{MAE scaled} = \\text{median} \\frac{| Y_i - Y_i'|}{\\text{sd}_i}"
  },
  {
    "objectID": "files/lectures/09-2-evaluating-model.html#how-accurate-are-posterior-predictive-models-3",
    "href": "files/lectures/09-2-evaluating-model.html#how-accurate-are-posterior-predictive-models-3",
    "title": "Evaluating Regression Models",
    "section": "How Accurate are Posterior Predictive Models?",
    "text": "How Accurate are Posterior Predictive Models?\n\nLet Y_1, Y_2, ..., Y_n denote n observed outcomes. Each Y_i has a corresponding posterior predictive model with mean Y_i' and standard deviation \\text{sd}_i\nWithin 50: Proportion of observed values that fall within their 50% posterior prediction interval.\nWithin 95: Proportion of observed values that fall within their 95% posterior prediction interval.\nWe get these four values out of the prediction_summary() function,\n\n\nset.seed(84735)\nprediction_summary(bike_model, data = bikes)"
  },
  {
    "objectID": "files/lectures/09-2-evaluating-model.html#how-accurate-are-posterior-predictive-models-4",
    "href": "files/lectures/09-2-evaluating-model.html#how-accurate-are-posterior-predictive-models-4",
    "title": "Evaluating Regression Models",
    "section": "How Accurate are Posterior Predictive Models?",
    "text": "How Accurate are Posterior Predictive Models?\n\nLet’s now interpret these values.\n\n\nset.seed(84735)\nprediction_summary(bike_model, data = bikes)\n\n\n  \n\n\n\n\nThe MAE of 989.7 means that the typical difference between the observed ride count and the posterior predictive mean is about 990 rides.\n\nThe scaled MAE of 0.77 means that the typical difference between the observed ride count and the posterior predictive mean is about 0.77 standard deviations.\n\nOnly 43.8% of test observations fall within their respective 50% prediction interval, while 95.8% fall within their respective 95% prediction interval."
  },
  {
    "objectID": "files/lectures/09-2-evaluating-model.html#how-accurate-are-posterior-predictive-models-5",
    "href": "files/lectures/09-2-evaluating-model.html#how-accurate-are-posterior-predictive-models-5",
    "title": "Evaluating Regression Models",
    "section": "How Accurate are Posterior Predictive Models?",
    "text": "How Accurate are Posterior Predictive Models?\n\nLooking at these side by side,"
  },
  {
    "objectID": "files/lectures/09-2-evaluating-model.html#how-accurate-are-posterior-predictive-models-6",
    "href": "files/lectures/09-2-evaluating-model.html#how-accurate-are-posterior-predictive-models-6",
    "title": "Evaluating Regression Models",
    "section": "How Accurate are Posterior Predictive Models?",
    "text": "How Accurate are Posterior Predictive Models?\n\nWhat is our final conclusion? Does our Bayesian model produce accurate predictions?\nUnfortunately, the answer is subjective and dependent on context.\n\nIs an MAE of 990 rides acceptable for the business?\nIs having only 43.8% of observations within the 50% prediction interval acceptable?\n\nPer usual, we are just the statisticians. The business must decide what is acceptable."
  },
  {
    "objectID": "files/lectures/09-2-evaluating-model.html#how-accurate-are-posterior-predictive-models-7",
    "href": "files/lectures/09-2-evaluating-model.html#how-accurate-are-posterior-predictive-models-7",
    "title": "Evaluating Regression Models",
    "section": "How Accurate are Posterior Predictive Models?",
    "text": "How Accurate are Posterior Predictive Models?\n\nHow can we improve the model’s posterior predictive accuracy?\n\nAdd additional predictors or terms (e.g., interactions) to the model.\nConsider different distributions for the outcome variable.\n\ne.g., Poisson, negative binomial, binomial, etc.\n\n\nCollect more data.\n\nThis is always the cheeky answer."
  },
  {
    "objectID": "files/lectures/09-2-evaluating-model.html#practice-homework",
    "href": "files/lectures/09-2-evaluating-model.html#practice-homework",
    "title": "Evaluating Regression Models",
    "section": "Practice / Homework",
    "text": "Practice / Homework\n\nFrom the Bayes Rules! textbook:\n\n10.3\n10.5\n10.20\n10.21"
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#introduction",
    "href": "files/lectures/03-2-probability-distributions.html#introduction",
    "title": "Random Variables and their Distributions",
    "section": "Introduction",
    "text": "Introduction\n\nToday we will review the statistical distributions needed for this course.\nDiscrete distributions:\n\nBinomial\nPoisson\n\nContinuous distributions:\n\nUniform\nNormal\nGamma\nBeta"
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#basic-definitions",
    "href": "files/lectures/03-2-probability-distributions.html#basic-definitions",
    "title": "Random Variables and their Distributions",
    "section": "Basic Definitions",
    "text": "Basic Definitions\n\nDiscrete random variable: a variable that can assume only a finite or countably infinite number of distinct values.\nProbability distribution of a random variable: collection of probabilities for each value of the random variable.\nNotation:\n\nUppercase letter (e.g., Y) denotes a random variable.\nLowercase letter (e.g., y) denotes a particular value that the random variable may assume.\n\nThe specific observed value, y, is not random."
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#probability-distributions-for-discrete-rv",
    "href": "files/lectures/03-2-probability-distributions.html#probability-distributions-for-discrete-rv",
    "title": "Random Variables and their Distributions",
    "section": "Probability Distributions for Discrete RV",
    "text": "Probability Distributions for Discrete RV\n\nProbability function for \\boldsymbol Y: sum of the the probabilities of all sample points in S that are assigned the value y\n\nP[Y = y] = p(y): the probability that Y takes on the value y.\n\nProbability distribution for \\boldsymbol Y: a formula, table, or graph that provides p(y) \\ \\forall \\ y.\nTheorem:\n\nFor any discrete probability distribution, the following must be true:\n\n0 \\le p(y) \\le 1 \\  \\forall \\ y\n\\sum_y p(y) = 1 \\ \\forall \\ p(y) &gt; 0."
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#expected-values-for-discrete-rv",
    "href": "files/lectures/03-2-probability-distributions.html#expected-values-for-discrete-rv",
    "title": "Random Variables and their Distributions",
    "section": "Expected Values for Discrete RV",
    "text": "Expected Values for Discrete RV\n\nExpected value: Let Y be a discrete random variable with probability function p(y). Then the expected value of Y, E[Y], is defined to be\n\n\nE(Y) = \\sum_{y} y p(y)\n\n\nWhen p(y) is an accurate characterization of the population frequency distribution, then the expected value is the population mean.\n\n\nE[Y] = \\mu"
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#expected-values-for-discrete-rv-1",
    "href": "files/lectures/03-2-probability-distributions.html#expected-values-for-discrete-rv-1",
    "title": "Random Variables and their Distributions",
    "section": "Expected Values for Discrete RV",
    "text": "Expected Values for Discrete RV\n\nTheorem:\n\nLet Y be a discrete random variable with probability function p(y) and g(Y) be a real-valued function of Y (i.e., a transformed variable). Then the expected value of g(Y) is given by\n\n\n\nE[g(Y)] = \\sum_{y} g(y) p(y)"
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#expected-values-for-discrete-rv-2",
    "href": "files/lectures/03-2-probability-distributions.html#expected-values-for-discrete-rv-2",
    "title": "Random Variables and their Distributions",
    "section": "Expected Values for Discrete RV",
    "text": "Expected Values for Discrete RV\n\nVariance: if Y is a random variable with mean E[Y] = \\mu, the variance of a random variable Y is defined to be the expected value of (Y-\\mu)^2.\n\n\nV[Y] = E\\left[ (Y-\\mu)^2 \\right]\n\n\nIf p(y) is an accurate characterization of the population frequency distribution, then V(Y) is the population variance,\n\n\nV[Y] = \\sigma^2\n\n\nStandard deviation: the positive square root of V[Y]."
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#expected-values-for-discrete-rv-3",
    "href": "files/lectures/03-2-probability-distributions.html#expected-values-for-discrete-rv-3",
    "title": "Random Variables and their Distributions",
    "section": "Expected Values for Discrete RV",
    "text": "Expected Values for Discrete RV\n\nThere is an alternative (and easier) way to calculate the variance manually,\nTheorem:\n\nLet Y be a discrete random variable with probability function p(y) and mean E[Y] = \\mu. Then,\n\n\nV[Y] = \\sigma^2 = E\\left[(Y-\\mu)^2\\right] = E\\left[Y^2\\right] - \\mu^2"
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#expected-values-for-discrete-rv-4",
    "href": "files/lectures/03-2-probability-distributions.html#expected-values-for-discrete-rv-4",
    "title": "Random Variables and their Distributions",
    "section": "Expected Values for Discrete RV",
    "text": "Expected Values for Discrete RV\n\nThe probability distribution for a random variable Y is given below.\n\n\n\n\ny\np(y)\n\n\n\n\n0\n1/8\n\n\n1\n1/4\n\n\n2\n3/8\n\n\n3\n1/4\n\n\n\n\nFind the mean of Y."
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#expected-values-for-discrete-rv-5",
    "href": "files/lectures/03-2-probability-distributions.html#expected-values-for-discrete-rv-5",
    "title": "Random Variables and their Distributions",
    "section": "Expected Values for Discrete RV",
    "text": "Expected Values for Discrete RV\n\nThe probability distribution for a random variable Y is given below.\n\n\n\n\ny\np(y)\n\n\n\n\n0\n1/8\n\n\n1\n1/4\n\n\n2\n3/8\n\n\n3\n1/4\n\n\n\n\nFind the variance and standard deviation of Y."
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#binomial-probability-distribution",
    "href": "files/lectures/03-2-probability-distributions.html#binomial-probability-distribution",
    "title": "Random Variables and their Distributions",
    "section": "Binomial Probability Distribution",
    "text": "Binomial Probability Distribution\n\nBinomial experiment:\n\nThe experiment consists of a fixed number, n, of identical trials.\nEach trial results in one of two outcomes: success (S) or failure (F).\nThe probability of success on a single trial is equal to some value p and remains the same from trial to trial.\n\nThe probability of failure is equal to q = (1-p).\n\nThe trials are independent.\nThe random variable of interest is Y, the number of successes observed during the n trials."
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#binomial-probability-distribution-1",
    "href": "files/lectures/03-2-probability-distributions.html#binomial-probability-distribution-1",
    "title": "Random Variables and their Distributions",
    "section": "Binomial Probability Distribution",
    "text": "Binomial Probability Distribution\n\nA random variable Y is said to have a binomial distribution based on n trials with success probability p iff\n\n\np(y) = {n \\choose y}p^y q^{n-y}, \\text{ where } y = 0, 1, 2, ..., n, \\text{ and } 0 \\le p \\le1\n\n\nLet Y be a binomial random variable based on n trials and success probability p. Then\n\n\nE[Y] = \\mu = np \\ \\ \\ \\text{and} \\ \\ \\ V[Y] = \\sigma^2 = npq\n\n\nSee Wackerly pg. 107 for derivation."
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#binomial-probability-distribution-2",
    "href": "files/lectures/03-2-probability-distributions.html#binomial-probability-distribution-2",
    "title": "Random Variables and their Distributions",
    "section": "Binomial Probability Distribution",
    "text": "Binomial Probability Distribution\n\nWe can use R to find information related to the binomial distribution.\n\nP[X = x]: dbinom(x, size, prob)\nP[X \\le x]: pbinom(q, size, prob)\nP[X &gt; x]: pbinom(q, size, prob, lower.tail = FALSE)\n\nIn the functions,\n\nx or q is the value of X we are interested in\nsize is the sample size (n)\nprob is the probability of success, \\pi\nlower.tail has two options:\n\nTRUE (default) returns P[X \\le x]\nFALSE returns P[X &gt; x]"
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#binomial-probability-distribution-3",
    "href": "files/lectures/03-2-probability-distributions.html#binomial-probability-distribution-3",
    "title": "Random Variables and their Distributions",
    "section": "Binomial Probability Distribution",
    "text": "Binomial Probability Distribution\n\nThe manufacturer of a dairy drink wishes to compare a new formula (B) with that of the standard formula (A). Each of four judges perform a blinded taste test and report which glass he or she most enjoyed. Suppose that the two formulas are equally attractive.\n\nWhat is the probability distribution? \nWhat is the mean of the distribution? \nWhat is the variance of the distribution?"
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#binomial-probability-distribution-4",
    "href": "files/lectures/03-2-probability-distributions.html#binomial-probability-distribution-4",
    "title": "Random Variables and their Distributions",
    "section": "Binomial Probability Distribution",
    "text": "Binomial Probability Distribution\n\nThe manufacturer of a dairy drink wishes to compare a new formula (B) with that of the standard formula (A). Each of four judges perform a blinded taste test and report which glass he or she most enjoyed. Suppose that the two formulas are equally attractive.\nUse R to find:\n\nP[X = 2]\nP[X &gt; 2]\nP[X &lt; 4]"
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#binomial-probability-distribution-5",
    "href": "files/lectures/03-2-probability-distributions.html#binomial-probability-distribution-5",
    "title": "Random Variables and their Distributions",
    "section": "Binomial Probability Distribution",
    "text": "Binomial Probability Distribution\n\nThe manufacturer of a dairy drink wishes to compare a new formula (B) with that of the standard formula (A). Each of four judges perform a blinded taste test and report which glass he or she most enjoyed. Suppose that the two formulas are equally attractive.\nUse R to find:\n\nP[X = 2]\n\n\n\ndbinom(x = 2, size = 4, prob = 0.5)\n\n[1] 0.375"
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#binomial-probability-distribution-6",
    "href": "files/lectures/03-2-probability-distributions.html#binomial-probability-distribution-6",
    "title": "Random Variables and their Distributions",
    "section": "Binomial Probability Distribution",
    "text": "Binomial Probability Distribution\n\nThe manufacturer of a dairy drink wishes to compare a new formula (B) with that of the standard formula (A). Each of four judges perform a blinded taste test and report which glass he or she most enjoyed. Suppose that the two formulas are equally attractive.\nUse R to find:\n\nP[X &gt; 2]\n\n\n\npbinom(q = 2, size = 4, prob = 0.5, lower.tail = FALSE)\n\n[1] 0.3125"
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#binomial-probability-distribution-7",
    "href": "files/lectures/03-2-probability-distributions.html#binomial-probability-distribution-7",
    "title": "Random Variables and their Distributions",
    "section": "Binomial Probability Distribution",
    "text": "Binomial Probability Distribution\n\nThe manufacturer of a dairy drink wishes to compare a new formula (B) with that of the standard formula (A). Each of four judges perform a blinded taste test and report which glass he or she most enjoyed. Suppose that the two formulas are equally attractive.\nUse R to find:\n\nP[X &lt; 4] = P[X \\le 3]\n\n\n\npbinom(q = 3, size = 4, prob = 0.5)\n\n[1] 0.9375"
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#poisson-probability-distribution",
    "href": "files/lectures/03-2-probability-distributions.html#poisson-probability-distribution",
    "title": "Random Variables and their Distributions",
    "section": "Poisson Probability Distribution",
    "text": "Poisson Probability Distribution\n\nWe often use the Poisson distribution to model count data.\nA random variable Y is said to have a Poisson probability distribution iff\n\n\np(y) = \\frac{\\lambda^y}{y!}e^{-\\lambda}, \\text{ where } y=0,1,2,..., \\text{ and } \\lambda &gt; 0\n\n\nIf Y is a random variable with a Poisson distribution with parameter \\lambda, then\n\n\nE[Y] = \\mu = \\lambda \\text{ and } V[Y] = \\sigma^2 = \\lambda\n\n\nSee Wackerly pg. 134 for derivation."
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#poisson-probability-distribution-1",
    "href": "files/lectures/03-2-probability-distributions.html#poisson-probability-distribution-1",
    "title": "Random Variables and their Distributions",
    "section": "Poisson Probability Distribution",
    "text": "Poisson Probability Distribution\n\nWe can use R to find information related to the Poisson distribution.\n\nP[X = x]: dpois(x, lambda)\n\nP[X \\le x]: ppois(q, lambda)\n\nP[X &gt; x]: ppois(q, lambda, lower.tail = FALSE)\n\nIn the functions:\n\nx or q is the value of X we are interested in\n\nlambda is the rate of occurrence\nlower.tail has two options:\n\nTRUE (default) returns P[X \\le x]\n\nFALSE returns P[X &gt; x]"
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#poisson-probability-distribution-2",
    "href": "files/lectures/03-2-probability-distributions.html#poisson-probability-distribution-2",
    "title": "Random Variables and their Distributions",
    "section": "Poisson Probability Distribution",
    "text": "Poisson Probability Distribution\n\nCustomers arrive at a checkout counter in a department store according to a Poisson distribution at an average of seven per hour.\n\nWhat is the probability distribution? \nWhat is the mean of the distribution? \nWhat is the variance of the distribution?"
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#poisson-probability-distribution-3",
    "href": "files/lectures/03-2-probability-distributions.html#poisson-probability-distribution-3",
    "title": "Random Variables and their Distributions",
    "section": "Poisson Probability Distribution",
    "text": "Poisson Probability Distribution\n\nCustomers arrive at a checkout counter in a department store according to a Poisson distribution at an average of seven per hour. Use R to find the following probabilities.\n\nNo more than three customers arrive.\nAt least two customers arrive.\nExactly five customers arrive."
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#poisson-probability-distribution-4",
    "href": "files/lectures/03-2-probability-distributions.html#poisson-probability-distribution-4",
    "title": "Random Variables and their Distributions",
    "section": "Poisson Probability Distribution",
    "text": "Poisson Probability Distribution\n\nCustomers arrive at a checkout counter in a department store according to a Poisson distribution at an average of seven per hour. Use R to find the following probabilities.\n\nNo more than three customers arrive.\n\n\n\nppois(q = 3, lambda = 7)\n\n[1] 0.08176542"
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#poisson-probability-distribution-5",
    "href": "files/lectures/03-2-probability-distributions.html#poisson-probability-distribution-5",
    "title": "Random Variables and their Distributions",
    "section": "Poisson Probability Distribution",
    "text": "Poisson Probability Distribution\n\nCustomers arrive at a checkout counter in a department store according to a Poisson distribution at an average of seven per hour. Use R to find the following probabilities.\n\nAt least two customers arrive.\n\n\n\nppois(q = 1, lambda = 7, lower.tail = FALSE)\n\n[1] 0.9927049"
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#poisson-probability-distribution-6",
    "href": "files/lectures/03-2-probability-distributions.html#poisson-probability-distribution-6",
    "title": "Random Variables and their Distributions",
    "section": "Poisson Probability Distribution",
    "text": "Poisson Probability Distribution\n\nCustomers arrive at a checkout counter in a department store according to a Poisson distribution at an average of seven per hour. Use R to find the following probabilities.\n\nExactly five customers arrive.\n\n\n\ndpois(x = 5, lambda = 7)\n\n[1] 0.1277167"
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#probability-distributions-for-continuous-rv",
    "href": "files/lectures/03-2-probability-distributions.html#probability-distributions-for-continuous-rv",
    "title": "Random Variables and their Distributions",
    "section": "Probability Distributions for Continuous RV",
    "text": "Probability Distributions for Continuous RV\n\nThe distribution function of Y (any random varaible), denoted by F(y), is such that\n\nF(y) = P[Y \\le y] \\text{ for } -\\infty &lt; y &lt; \\infty\n\nTheorem: Properties of a Distribution Function\n\nIf F(y) is a distribution function, then\n\nF(-\\infty)   \\equiv \\underset{y \\to -\\infty}{\\lim} F(y) = 0\nF(\\infty)    \\equiv \\underset{y \\to \\infty}{\\lim} F(y) = 1\nF(y) is a nondecreasing function of y.\n\nIf y_1 and y_2 are any values such that y_1 &lt; y_2, then F(y_1) \\le F(y_2)."
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#probability-distributions-for-continuous-rv-1",
    "href": "files/lectures/03-2-probability-distributions.html#probability-distributions-for-continuous-rv-1",
    "title": "Random Variables and their Distributions",
    "section": "Probability Distributions for Continuous RV",
    "text": "Probability Distributions for Continuous RV\n\nContinuous random variable:\n\nA random variable Y with distribution function F(y) is said to be continuous if F(y) is continuous for -\\infty &lt; y &lt; \\infty.\n\nIf Y is a continuous random variable, then for any real number y, P[Y = y] = 0.\n\ni.e., we must remember to find the probability of an interval."
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#probability-distributions-for-continuous-rv-2",
    "href": "files/lectures/03-2-probability-distributions.html#probability-distributions-for-continuous-rv-2",
    "title": "Random Variables and their Distributions",
    "section": "Probability Distributions for Continuous RV",
    "text": "Probability Distributions for Continuous RV\n\nProbability density function:\n\nLet F(y) be the cumulative density function for a continuous random variable, Y. Then\n\n\np[Y = y] = f(y) = \\frac{dF(y)}{dy} = F'(y).\n\nTheorem: Properties of a Density Function\n\nIf f(y) is a density function for a continuous random variable, then\n\nf(y) \\ge 0 \\ \\forall y, \\ -\\infty &lt; y &lt; \\infty.\n\\int_{-\\infty}^{\\infty} f(y) dy = 1."
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#probability-distributions-for-continuous-rv-3",
    "href": "files/lectures/03-2-probability-distributions.html#probability-distributions-for-continuous-rv-3",
    "title": "Random Variables and their Distributions",
    "section": "Probability Distributions for Continuous RV",
    "text": "Probability Distributions for Continuous RV\n\nCumulative density function:\n\nLet f(y) be the probability density function for a continuous random variable, Y. Then\n\n\nP[Y \\le y] = F(y) = \\int_{-\\infty}^y f(t) dt, \\text{ for } y \\in {\\rm I\\!R}."
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#probability-distributions-for-continuous-rv-4",
    "href": "files/lectures/03-2-probability-distributions.html#probability-distributions-for-continuous-rv-4",
    "title": "Random Variables and their Distributions",
    "section": "Probability Distributions for Continuous RV",
    "text": "Probability Distributions for Continuous RV\n\nTheorem\n\nIf the random variable Y has density function f(y) and a &lt; b, then the probability that Y falls in the interval [a, b] is\n\n\n\nP[a \\le Y \\le b] = \\int_a^b f(y) dy."
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#expected-values-for-continuous-rv",
    "href": "files/lectures/03-2-probability-distributions.html#expected-values-for-continuous-rv",
    "title": "Random Variables and their Distributions",
    "section": "Expected Values for Continuous RV",
    "text": "Expected Values for Continuous RV\n\nExpected value:\n\nThe expected value of a continuous variable Y is\n\n\n\nE[Y] = \\int_{-\\infty}^{\\infty} y f(y) \\ dy\n\n\nThis is the continuous version of the expected value for a discrete random variable,\n\n\nE[Y] = \\sum_y y p(y)"
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#expected-values-for-continuous-rv-1",
    "href": "files/lectures/03-2-probability-distributions.html#expected-values-for-continuous-rv-1",
    "title": "Random Variables and their Distributions",
    "section": "Expected Values for Continuous RV",
    "text": "Expected Values for Continuous RV\n\nTheorem:\n\nLet g(Y) be a function of Y; then the expected value of g(Y) is given by\n\n\n\nE\\left[ g(Y) \\right] = \\int_{-\\infty}^{\\infty} g(y) f(y) \\ dy\n\n\nTheorem:\n\nLet c be a constant and let g(Y), g_1(Y), g_2(Y), …, g_k(Y) be functions of a continuous random variable, Y. Then the following results hold:\n\nE[c] = c\nE\\left[cg(Y)] = cE[g(Y)\\right]\nE\\left[g_1(Y)+...+g_k(Y)\\right] = E\\left[ g_1(Y) \\right] + ... + E\\left[ g_k(Y) \\right]"
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#uniform-probability-distribution",
    "href": "files/lectures/03-2-probability-distributions.html#uniform-probability-distribution",
    "title": "Random Variables and their Distributions",
    "section": "Uniform Probability Distribution",
    "text": "Uniform Probability Distribution\n\nUniform Distribution"
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#uniform-probability-distribution-1",
    "href": "files/lectures/03-2-probability-distributions.html#uniform-probability-distribution-1",
    "title": "Random Variables and their Distributions",
    "section": "Uniform Probability Distribution",
    "text": "Uniform Probability Distribution\n\nA random variable Y is said to have a uniform distribution iff\n\n\nf(y) = \\frac{1}{\\theta_2 - \\theta_1}, \\ \\theta_1 \\le y \\le \\theta_2\n\n\nIf \\theta_1 &lt; \\theta_2 and Y is a uniformly distributed r.v. on the interval (\\theta_1, \\theta_2), then\n\n\nE[Y] = \\mu = \\frac{\\theta_1+\\theta_2}{2} \\ \\ \\ \\text{and} \\ \\ \\ V[Y] = \\sigma^2 = \\frac{(\\theta_2-\\theta_1)^2}{12}\n\n\nSee Wackerly pg. 176 for derivation."
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#uniform-probability-distribution-2",
    "href": "files/lectures/03-2-probability-distributions.html#uniform-probability-distribution-2",
    "title": "Random Variables and their Distributions",
    "section": "Uniform Probability Distribution",
    "text": "Uniform Probability Distribution\n\nAn industrial psychologist has determined that it takes a worker between 9 and 15 minutes to complete a task on an automobile assembly line. If the time to complete the task is uniformly distributed over the interval 9 \\le y \\le 15, then determine:\n\nThe probability distribution. \nThe mean of the distribution. \nThe variance and standard deviation of the distribution."
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#uniform-probability-distribution-3",
    "href": "files/lectures/03-2-probability-distributions.html#uniform-probability-distribution-3",
    "title": "Random Variables and their Distributions",
    "section": "Uniform Probability Distribution",
    "text": "Uniform Probability Distribution\n\nWe can use R to find information related to the uniform distribution:\n\nP[X \\le x]: punif(q, min, max)\n\nP[X \\ge x]: punif(q, min, max, lower.tail = FALSE)\n\nIn the functions:\n\nq is the value of X we are interested in\n\nmin is the lower bound of the distribution\n\nmax is the upper bound of the distribution\n\nlower.tail has two options:\n\nTRUE (default) returns P[X \\le x]\n\nFALSE returns P[X \\ge x]"
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#uniform-probability-distribution-4",
    "href": "files/lectures/03-2-probability-distributions.html#uniform-probability-distribution-4",
    "title": "Random Variables and their Distributions",
    "section": "Uniform Probability Distribution",
    "text": "Uniform Probability Distribution\n\nAn industrial psychologist has determined that it takes a worker between 9 and 15 minutes to complete a task on an automobile assembly line. If the time to complete the task is uniformly distributed over the interval 9 \\le y \\le 15, then determine the following probabilities:\n\nA worker takes fewer than 13 minutes.\nA worker takes at least 11 minutes.\nA worker takes between 14 and 15 minutes."
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#uniform-probability-distribution-5",
    "href": "files/lectures/03-2-probability-distributions.html#uniform-probability-distribution-5",
    "title": "Random Variables and their Distributions",
    "section": "Uniform Probability Distribution",
    "text": "Uniform Probability Distribution\n\nAn industrial psychologist has determined that it takes a worker between 9 and 15 minutes to complete a task on an automobile assembly line. If the time to complete the task is uniformly distributed over the interval 9 \\le y \\le 15, then determine the following probabilities:\n\nA worker takes fewer than 13 minutes.\n\n\n\npunif(13, 9, 15)\n\n[1] 0.6666667"
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#uniform-probability-distribution-6",
    "href": "files/lectures/03-2-probability-distributions.html#uniform-probability-distribution-6",
    "title": "Random Variables and their Distributions",
    "section": "Uniform Probability Distribution",
    "text": "Uniform Probability Distribution\n\nAn industrial psychologist has determined that it takes a worker between 9 and 15 minutes to complete a task on an automobile assembly line. If the time to complete the task is uniformly distributed over the interval 9 \\le y \\le 15, then determine the following probabilities:\n\nA worker takes at least 11 minutes.\n\n\n\npunif(11, 9, 15, lower.tail = TRUE)\n\n[1] 0.3333333"
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#uniform-probability-distribution-7",
    "href": "files/lectures/03-2-probability-distributions.html#uniform-probability-distribution-7",
    "title": "Random Variables and their Distributions",
    "section": "Uniform Probability Distribution",
    "text": "Uniform Probability Distribution\n\nAn industrial psychologist has determined that it takes a worker between 9 and 15 minutes to complete a task on an automobile assembly line. If the time to complete the task is uniformly distributed over the interval 9 \\le y \\le 15, then determine the following probabilities:\n\nA worker takes between 14 and 15 minutes.\n\n\n\npunif(15, 9, 15) - punif(14, 9, 15)\n\n[1] 0.1666667"
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#normal-probability-distribution",
    "href": "files/lectures/03-2-probability-distributions.html#normal-probability-distribution",
    "title": "Random Variables and their Distributions",
    "section": "Normal Probability Distribution",
    "text": "Normal Probability Distribution\n\nNormal Distribution"
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#normal-probability-distribution-1",
    "href": "files/lectures/03-2-probability-distributions.html#normal-probability-distribution-1",
    "title": "Random Variables and their Distributions",
    "section": "Normal Probability Distribution",
    "text": "Normal Probability Distribution\n\nA random variable Y is said to have a normal distribution iff, for \\sigma &gt; 0 and -\\infty &lt; \\mu &lt; \\infty,\n\n\nf(y) = \\frac{1}{\\sigma \\sqrt{2\\pi}} e^{-(y-\\mu)^2/(2\\sigma^2)}\n\n\nIf Y is a random variable normally distributed with parameters \\mu and \\sigma, then\n\n\nE[Y] = \\mu =  \\ \\ \\ \\text{and} \\ \\ \\ V[Y] = \\sigma^2"
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#normal-probability-distribution-2",
    "href": "files/lectures/03-2-probability-distributions.html#normal-probability-distribution-2",
    "title": "Random Variables and their Distributions",
    "section": "Normal Probability Distribution",
    "text": "Normal Probability Distribution\n\nWe can use R to find information related to the normal distribution.\n\nP[X \\le x]: pnorm(q, mean, sd)\n\nP[X \\ge x]: pnorm(q, mean, sd, lower.tail = FALSE)\n\nIn the functions:\n\nq is the value of X we are interested in\n\nmean is the population mean \\mu\n\nsd is the standard deviation \\sigma\n\nlower.tail has two options:\n\nTRUE (default) returns P[X \\le x]\n\nFALSE returns P[X \\ge x]"
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#normal-probability-distribution-3",
    "href": "files/lectures/03-2-probability-distributions.html#normal-probability-distribution-3",
    "title": "Random Variables and their Distributions",
    "section": "Normal Probability Distribution",
    "text": "Normal Probability Distribution\n\nA random variable Y is said to have a standard normal distribution iff\n\n\nY \\sim N(\\mu=0,\\sigma=1)\n\n\nThe normal distribution is then simplified to\n\n\nf(y) = \\frac{1}{\\sqrt{2\\pi}} e^{-y^2/2}\n\n\nNote that in all cases of the normal distribution, we assume -\\infty &lt; y &lt; \\infty."
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#normal-probability-distribution-4",
    "href": "files/lectures/03-2-probability-distributions.html#normal-probability-distribution-4",
    "title": "Random Variables and their Distributions",
    "section": "Normal Probability Distribution",
    "text": "Normal Probability Distribution\n\nWhen using pnorm(), the default values for mean and sd are 1 and 0.\nThus, if we have the standard normal our R functions simplify to:\n\nP[Z \\le z]: pnorm(z)\n\nP[Z \\ge z]: pnorm(z, lower.tail = FALSE)\n\nIn the functions:\n\nq is the z-score value of interest\n\nlower.tail = TRUE returns P[Z \\le z]\n\nlower.tail = FALSE returns P[Z \\ge z]"
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#normal-probability-distribution-5",
    "href": "files/lectures/03-2-probability-distributions.html#normal-probability-distribution-5",
    "title": "Random Variables and their Distributions",
    "section": "Normal Probability Distribution",
    "text": "Normal Probability Distribution\n\nA geneticist working for a seed company develops a new carrot for growing in heavy clay soil. After measuring 5000 of these carrots, it can be said that the carrot length, Y, is normally distributed with \\mu = 11.5 cm and \\sigma = 1.15 cm. Determine\n\nThe probability distribution. \nThe mean of the distribution. \nThe variance and standard deviation of the distribution."
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#normal-probability-distribution-6",
    "href": "files/lectures/03-2-probability-distributions.html#normal-probability-distribution-6",
    "title": "Random Variables and their Distributions",
    "section": "Normal Probability Distribution",
    "text": "Normal Probability Distribution\n\nA geneticist working for a seed company develops a new carrot for growing in heavy clay soil. After measuring 5000 of these carrots, it can be said that the carrot length, Y, is normally distributed with \\mu = 11.5 cm and \\sigma = 1.15 cm.\n\nWhat is the probability that a carrot will be between 10 and 13 cm?\nWhat is the probability that a carrot will be less than 9 cm?\nWhat is the probability that a carrot will be 12 cm or larger?"
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#normal-probability-distribution-7",
    "href": "files/lectures/03-2-probability-distributions.html#normal-probability-distribution-7",
    "title": "Random Variables and their Distributions",
    "section": "Normal Probability Distribution",
    "text": "Normal Probability Distribution\n\nA geneticist working for a seed company develops a new carrot for growing in heavy clay soil. After measuring 5000 of these carrots, it can be said that the carrot length, Y, is normally distributed with \\mu = 11.5 cm and \\sigma = 1.15 cm.\n\nWhat is the probability that a carrot will be between 10 and 13 cm?\n\n\n\npnorm(q = 13, mean = 11.5, sd = 1.15) - pnorm(q = 10, mean = 11.5, sd = 1.15)\n\n[1] 0.807885"
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#normal-probability-distribution-8",
    "href": "files/lectures/03-2-probability-distributions.html#normal-probability-distribution-8",
    "title": "Random Variables and their Distributions",
    "section": "Normal Probability Distribution",
    "text": "Normal Probability Distribution\n\nA geneticist working for a seed company develops a new carrot for growing in heavy clay soil. After measuring 5000 of these carrots, it can be said that the carrot length, Y, is normally distributed with \\mu = 11.5 cm and \\sigma = 1.15 cm.\n\nWhat is the probability that a carrot will be less than 9 cm?\n\n\n\npnorm(q = 9, mean = 11.5, sd = 1.15)\n\n[1] 0.01485583"
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#normal-probability-distribution-9",
    "href": "files/lectures/03-2-probability-distributions.html#normal-probability-distribution-9",
    "title": "Random Variables and their Distributions",
    "section": "Normal Probability Distribution",
    "text": "Normal Probability Distribution\n\nA geneticist working for a seed company develops a new carrot for growing in heavy clay soil. After measuring 5000 of these carrots, it can be said that the carrot length, Y, is normally distributed with \\mu = 11.5 cm and \\sigma = 1.15 cm.\n\nWhat is the probability that a carrot will be 12 cm or larger?\n\n\n\npnorm(q = 12, mean = 11.5, sd = 1.15, lower.tail = FALSE)\n\n[1] 0.3318601"
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#gamma-probability-distribution",
    "href": "files/lectures/03-2-probability-distributions.html#gamma-probability-distribution",
    "title": "Random Variables and their Distributions",
    "section": "Gamma Probability Distribution",
    "text": "Gamma Probability Distribution\n\nGamma Distribution"
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#gamma-probability-distribution-1",
    "href": "files/lectures/03-2-probability-distributions.html#gamma-probability-distribution-1",
    "title": "Random Variables and their Distributions",
    "section": "Gamma Probability Distribution",
    "text": "Gamma Probability Distribution\n\nA random variable Y is said to have a gamma distribution with parameters \\alpha &gt; 0 and \\beta &gt; 0 iff,\n\n\nf(y) = \\frac{y^{\\alpha-1} e^{-y/\\beta}}{\\beta^{\\alpha} \\Gamma(\\alpha)}, \\ 0 \\le y &lt; \\infty\n\n\nNote that \\Gamma(\\alpha) = \\int_{0}^{\\infty} y^{\\alpha-1} e^{-y} \\ dy.\nIf Y has a gamma distribution with parameters \\alpha and \\beta, then\n\n\nE[Y] = \\mu = \\alpha\\beta \\ \\ \\ \\text{and} \\ \\ \\ V[Y] = \\sigma^2  = \\alpha\\beta^2\n\n\nSee Wackerly pg. 187 for derivation."
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#gamma-probability-distribution-2",
    "href": "files/lectures/03-2-probability-distributions.html#gamma-probability-distribution-2",
    "title": "Random Variables and their Distributions",
    "section": "Gamma Probability Distribution",
    "text": "Gamma Probability Distribution\n\nWe can use R to find information related to the Gamma distribution.\n\nP[X \\le x]: pgamma(q, shape, rate)\n\nP[X \\ge x]: pgamma(q, shape, rate, lower.tail = FALSE)\n\nIn the functions:\n\nq is the value of X we are interested in\n\nshape is the shape parameter, \\alpha\n\nscale is the scale parameter, \\beta\n\nAlternatively, can parameterize with rate = 1/\\beta, rate = 1 / scale\n\nlower.tail has two options:\n\nTRUE (default) returns P[X \\le x]\n\nFALSE returns P[X \\ge x]"
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#gamma-probability-distribution-3",
    "href": "files/lectures/03-2-probability-distributions.html#gamma-probability-distribution-3",
    "title": "Random Variables and their Distributions",
    "section": "Gamma Probability Distribution",
    "text": "Gamma Probability Distribution\n\nAnnual incomes for heads of household in an affluent section of a city have approximately a gamma distribution with \\alpha=32 and \\beta=2500. Determine:\n\nThe probability distribution. \nThe mean of the distribution. \nThe variance and standard deviation of the distribution."
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#gamma-probability-distribution-4",
    "href": "files/lectures/03-2-probability-distributions.html#gamma-probability-distribution-4",
    "title": "Random Variables and their Distributions",
    "section": "Gamma Probability Distribution",
    "text": "Gamma Probability Distribution\n\nAnnual incomes for heads of household in an affluent section of a city have approximately a gamma distribution with \\alpha=32 and \\beta=2500.\n\nWhat proportion have incomes in excess of $100,000?\nWhat proportion have incomes between $75,000 and $150,000?"
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#gamma-probability-distribution-5",
    "href": "files/lectures/03-2-probability-distributions.html#gamma-probability-distribution-5",
    "title": "Random Variables and their Distributions",
    "section": "Gamma Probability Distribution",
    "text": "Gamma Probability Distribution\n\nAnnual incomes for heads of household in a section of a city have approximately a gamma distribution with \\alpha=32 and \\beta=2500.\n\nWhat proportion have incomes in excess of $30,000?\n\n\n\npgamma(q = 100000, shape = 32, scale = 2500, lower.tail = FALSE)\n\n[1] 0.08552057"
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#gamma-probability-distribution-6",
    "href": "files/lectures/03-2-probability-distributions.html#gamma-probability-distribution-6",
    "title": "Random Variables and their Distributions",
    "section": "Gamma Probability Distribution",
    "text": "Gamma Probability Distribution\n\nAnnual incomes for heads of household in an affluent section of a city have approximately a gamma distribution with \\alpha=32 and \\beta=2500.\n\nWhat proportion have incomes between $75,000 and $150,000?\n\n\n\npgamma(q = 150000, shape = 32, scale = 2500) - pgamma(q = 75000, shape = 32, scale = 2500)\n\n[1] 0.6186147"
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#beta-probability-distribution",
    "href": "files/lectures/03-2-probability-distributions.html#beta-probability-distribution",
    "title": "Random Variables and their Distributions",
    "section": "Beta Probability Distribution",
    "text": "Beta Probability Distribution\n\nBeta Distribution"
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#beta-probability-distribution-1",
    "href": "files/lectures/03-2-probability-distributions.html#beta-probability-distribution-1",
    "title": "Random Variables and their Distributions",
    "section": "Beta Probability Distribution",
    "text": "Beta Probability Distribution\n\nA random variable Y is said to have a beta distribution with parameters \\alpha &gt; 0 and \\beta &gt; 0 iff,\n\n\nf(y) = \\frac{y^{\\alpha-1}(1-y)^{\\beta-1}}{B(\\alpha,\\beta)}, \\ 0 \\le y \\le 1\n\n\nNote: B(\\alpha,\\beta) = \\int_0^1 y^{\\alpha-1}(1-y)^{\\beta-1} \\ dy = \\frac{\\Gamma(\\alpha) \\Gamma(\\beta)}{\\Gamma(\\alpha+\\beta)}.\nIf Y has a beta distribution with parameters \\alpha &gt; 0 and \\beta &gt; 0, then\n\n\nE[Y] = \\mu = \\frac{\\alpha}{\\alpha+\\beta} \\ \\ \\ \\text{and} \\ \\ \\ V[Y] = \\sigma^2  = \\frac{\\alpha\\beta}{(\\alpha+\\beta)^2(\\alpha+\\beta+1)}"
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#beta-probability-distribution-2",
    "href": "files/lectures/03-2-probability-distributions.html#beta-probability-distribution-2",
    "title": "Random Variables and their Distributions",
    "section": "Beta Probability Distribution",
    "text": "Beta Probability Distribution\n\nWe can use R to find information related to the Beta distribution.\n\nP[X \\le x]: pbeta(q, shape1, shape2)\n\nP[X \\ge x]: pbeta(q, shape1, shape2, lower.tail = FALSE)\n\nIn the functions:\n\nq is the value of X we are interested in – must be in [0, 1]!\nshape1 is the first shape parameter, \\alpha\n\nshape2 is the second shape parameter, \\beta\n\nlower.tail has two options:\n\nTRUE (default) returns P[X \\le x]\n\nFALSE returns P[X \\ge x]"
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#beta-probability-distribution-3",
    "href": "files/lectures/03-2-probability-distributions.html#beta-probability-distribution-3",
    "title": "Random Variables and their Distributions",
    "section": "Beta Probability Distribution",
    "text": "Beta Probability Distribution\n\nIn a survey of cupcake preferences, 8 respondents liked the new cupcake flavor and 2 did not. We will model the proportion of all respondents who would like the cupcake flavor using a Beta distribution with \\alpha = 8 and \\beta = 2. Determine:\n\nThe probability distribution. \nThe mean of the distribution. \nThe variance and standard deviation of the distribution."
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#beta-probability-distribution-4",
    "href": "files/lectures/03-2-probability-distributions.html#beta-probability-distribution-4",
    "title": "Random Variables and their Distributions",
    "section": "Beta Probability Distribution",
    "text": "Beta Probability Distribution\n\nIn a survey of cupcake preferences, 8 respondents liked the new cupcake flavor and 2 did not. We will model the proportion of all respondents who would like the cupcake flavor using a Beta distribution with \\alpha = 8 and \\beta = 2. What is the probability that:\n\nfewer than 60% of respondents like the new flavor?\nmore than 90% of respondents like the new flavor?\nsomewhere between 70% and 90% of respondents like the new flavor?"
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#beta-probability-distribution-5",
    "href": "files/lectures/03-2-probability-distributions.html#beta-probability-distribution-5",
    "title": "Random Variables and their Distributions",
    "section": "Beta Probability Distribution",
    "text": "Beta Probability Distribution\n\nIn a survey of cupcake preferences, 8 respondents liked the new cupcake flavor and 2 did not. We will model the proportion of all respondents who would like the cupcake flavor using a Beta distribution with \\alpha = 8 and \\beta = 2. What is the probability that:\n\nfewer than 60% of respondents like the new flavor?\n\n\n\npbeta(q = 0.6, shape1 = 8, shape2 = 2)\n\n[1] 0.07054387"
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#beta-probability-distribution-6",
    "href": "files/lectures/03-2-probability-distributions.html#beta-probability-distribution-6",
    "title": "Random Variables and their Distributions",
    "section": "Beta Probability Distribution",
    "text": "Beta Probability Distribution\n\nIn a survey of cupcake preferences, 8 respondents liked the new cupcake flavor and 2 did not. We will model the proportion of all respondents who would like the cupcake flavor using a Beta distribution with \\alpha = 8 and \\beta = 2. What is the probability that:\n\nmore than 90% of respondents like the new flavor?\n\n\n\npbeta(q = 0.9, shape1 = 8, shape2 = 2, lower.tail = FALSE)\n\n[1] 0.225159"
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#beta-probability-distribution-7",
    "href": "files/lectures/03-2-probability-distributions.html#beta-probability-distribution-7",
    "title": "Random Variables and their Distributions",
    "section": "Beta Probability Distribution",
    "text": "Beta Probability Distribution\n\nIn a survey of cupcake preferences, 8 respondents liked the new cupcake flavor and 2 did not. We will model the proportion of all respondents who would like the cupcake flavor using a Beta distribution with \\alpha = 8 and \\beta = 2. What is the probability that:\n\nsomewhere between 70% and 90% of respondents like the new flavor?\n\n\n\npbeta(q = 0.9, shape1 = 8, shape2 = 2) - pbeta(q = 0.7, shape1 = 8, shape2 = 2)\n\n[1] 0.5788377"
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#wrap-up",
    "href": "files/lectures/03-2-probability-distributions.html#wrap-up",
    "title": "Random Variables and their Distributions",
    "section": "Wrap Up",
    "text": "Wrap Up\n\nYou now are ready to work on Assignment 1: Probability Distributions.\n\nDue Tuesday, September 16.\n.qmd file is available to download on Canvas.\n\nGoals:\n\nIdentify the applicable distribution.\nFind probabilities using said distributions.\n\nNext week:\n\nMonday: Thinking like a Bayesian\nWednesday: Beta-Binomial"
  },
  {
    "objectID": "files/lectures/07-1-approx-posterior.html#introduction",
    "href": "files/lectures/07-1-approx-posterior.html#introduction",
    "title": "Approximating the Posterior",
    "section": "Introduction",
    "text": "Introduction\n\nWe have learned how to think like a Bayesian:\n\nPrior distribution\nData distribution\nPosterior distribution\n\nWe have learned three conjugate families:\n\nBeta-Binomial (binary outcomes)\nGamma-Poisson (count outcomes)\nNormal-Normal (continuous outcomes)\n\nOnce we have a posterior model, we must be able to apply the results.\n\nPosterior estimation\nHypothesis testing\nPrediction"
  },
  {
    "objectID": "files/lectures/07-1-approx-posterior.html#introduction-1",
    "href": "files/lectures/07-1-approx-posterior.html#introduction-1",
    "title": "Approximating the Posterior",
    "section": "Introduction",
    "text": "Introduction\n\nRecall, we have the posterior pdf,\n\nf(\\theta|y) = \\frac{f(\\theta) L(\\theta|y)}{f(y)} \\propto f(\\theta)L(\\theta|y)\n\nNow, in the denominator, we need to remember,\n\nf(y) = \\int_{\\theta_1} \\int_{\\theta_2} \\cdot \\cdot \\cdot \\int_{\\theta_k} f(\\theta) L(\\theta|y) d\\theta_k \\cdot \\cdot \\cdot d\\theta_2 d\\theta_1\n\nBecause this is … not fun … we will approximate the posterior via simulation."
  },
  {
    "objectID": "files/lectures/07-1-approx-posterior.html#introduction-2",
    "href": "files/lectures/07-1-approx-posterior.html#introduction-2",
    "title": "Approximating the Posterior",
    "section": "Introduction",
    "text": "Introduction\n\nWe are going to explore two simulation techniques:\n\ngrid approximation\nMarkov chain Monte Carlo (MCMC)\n\nEither method will produce a sample of N values for \\theta.\n\n\\left \\{ \\theta^{(1)}, \\theta^{(2)}, ..., \\theta^{(N)} \\right \\}\n\nThese \\theta_i will have properties that reflect those of the posterior model for \\theta.\nTo help us, we will apply these simulation techniques to Beta-Binomial and Gamma-Poisson models.\n\nNote that these models do not require simulation! We know their posteriors!\nThat’s why we are starting there – we can link the concepts to what we know. :)"
  },
  {
    "objectID": "files/lectures/07-1-approx-posterior.html#introduction-3",
    "href": "files/lectures/07-1-approx-posterior.html#introduction-3",
    "title": "Approximating the Posterior",
    "section": "Introduction",
    "text": "Introduction\n\nNote: we will use the following packages that may be new to you:\n\njanitor\nrstan\nbayesplot\n\nIf you are using the server provided by HMCSE, they have been installed for you.\nIf you are working at home, please check to see if you have the libraries, then install if you do not.\n\ninstall.packages(c(\"janitor\", \"rstan\", \"bayesplot\"))"
  },
  {
    "objectID": "files/lectures/07-1-approx-posterior.html#grid-approximation",
    "href": "files/lectures/07-1-approx-posterior.html#grid-approximation",
    "title": "Approximating the Posterior",
    "section": "Grid Approximation",
    "text": "Grid Approximation\n\n\n\n\nSuppose there is an image that you can’t view in its entirety.\nWe can see snippets along a grid that sweeps from left to right across the image.\nThe finer the grid, the clearer the image; if the grid is fine enough, the result is a good approximation."
  },
  {
    "objectID": "files/lectures/07-1-approx-posterior.html#grid-approximation-1",
    "href": "files/lectures/07-1-approx-posterior.html#grid-approximation-1",
    "title": "Approximating the Posterior",
    "section": "Grid Approximation",
    "text": "Grid Approximation\n\n\n\n\nThis is the general idea behind Bayesian grid approximation.\nOur target image is the posterior pdf, f(\\theta|y).\n\nIt is not necessary to observe all possible f(\\theta|y) \\ \\forall \\theta for us to understand its structure.\nInstead, we evaluate f(\\theta|y) at a finite, discrete grid of possible \\theta values.\nThen, we take random samples from this discretized pdf to approximate the full posterior pdf."
  },
  {
    "objectID": "files/lectures/07-1-approx-posterior.html#grid-approximation-2",
    "href": "files/lectures/07-1-approx-posterior.html#grid-approximation-2",
    "title": "Approximating the Posterior",
    "section": "Grid Approximation",
    "text": "Grid Approximation\n\nGrid approximation produces a sample of N independent \\theta values,\n\n\\left\\{ \\theta^{(1)}, \\theta^{(2)}, ..., \\theta^{(N)} \\right\\},\nfrom a discretized approximation of the posterior pdf, f(\\theta|y)."
  },
  {
    "objectID": "files/lectures/07-1-approx-posterior.html#grid-approximation-3",
    "href": "files/lectures/07-1-approx-posterior.html#grid-approximation-3",
    "title": "Approximating the Posterior",
    "section": "Grid Approximation",
    "text": "Grid Approximation\n\nAlgorithm:\n\nDefine a discrete grid of possible \\theta values.\nEvaluate the prior pdf, f(\\theta), and the likelihood function, L(\\theta|y) at each \\theta grid value.\nObtain a discrete approximation of the posterior pdf, f(\\theta|y) by:\n\nCalculating the product f(\\theta) L(\\theta|y) at each \\theta grid value,\nNormalize the products from (a) to sum to 1 across all \\theta.\n\nRandomly sample N \\theta grid values with respect to their corresponding normalized posterior probabilities."
  },
  {
    "objectID": "files/lectures/07-1-approx-posterior.html#grid-approximation---example",
    "href": "files/lectures/07-1-approx-posterior.html#grid-approximation---example",
    "title": "Approximating the Posterior",
    "section": "Grid Approximation - Example",
    "text": "Grid Approximation - Example\n\nWe will use the following Beta-Binomial model to learn how to do grid approximation:\n\n\n\\begin{align*}\nY|\\pi &\\sim \\text{Bin}(10, \\pi) \\\\\n\\pi &\\sim \\text{Beta}(2, 2)\n\\end{align*}\n\n\nNote that\n\nY is the number of successes in 10 independent trials.\nEvery trial has probability of success, \\pi.\nOur prior understanding about \\pi is captured by a \\text{Beta}(2,2) model."
  },
  {
    "objectID": "files/lectures/07-1-approx-posterior.html#grid-approximation---example-1",
    "href": "files/lectures/07-1-approx-posterior.html#grid-approximation---example-1",
    "title": "Approximating the Posterior",
    "section": "Grid Approximation - Example",
    "text": "Grid Approximation - Example\n\nIf we observe Y = 9 successes, we know that the updated posterior model for \\pi is \\text{Beta}(11, 3).\n\nY + \\alpha = 9+2\nn - Y + \\beta = 10-9+2\n\nThus,\n\n\n\\begin{align*}\nY|\\pi &\\sim \\text{Bin}(10, \\pi) \\\\\n\\pi &\\sim \\text{Beta}(2, 2) \\\\\n\\pi | Y &\\sim \\text{Beta}(11, 3)\n\\end{align*}"
  },
  {
    "objectID": "files/lectures/07-1-approx-posterior.html#grid-approximation-4",
    "href": "files/lectures/07-1-approx-posterior.html#grid-approximation-4",
    "title": "Approximating the Posterior",
    "section": "Grid Approximation",
    "text": "Grid Approximation\n\nInstead of using the posterior we know, let’s approximate it using grid approximation.\nFirst step: define a discrete grid of possible \\theta values.\n\nSo, let’s consider \\pi \\in \\{0, 0.2, 0.4, 0.6, 0.8, 1\\}.\n\n\n\nlibrary(tidyverse)\ngrid_data &lt;- tibble(pi_grid = seq(from = 0, to = 1, length = 6))"
  },
  {
    "objectID": "files/lectures/07-1-approx-posterior.html#grid-approximation-5",
    "href": "files/lectures/07-1-approx-posterior.html#grid-approximation-5",
    "title": "Approximating the Posterior",
    "section": "Grid Approximation",
    "text": "Grid Approximation\n\nInstead of using the posterior we know, let’s approximate it using grid approximation.\nFirst step: define a discrete grid of possible \\theta values.\n\nSo, let’s consider \\pi \\in \\{0, 0.2, 0.4, 0.6, 0.8, 1\\}.\n\n\n\nlibrary(tidyverse)\ngrid_data &lt;- tibble(pi_grid = seq(from = 0, to = 1, length = 6))"
  },
  {
    "objectID": "files/lectures/07-1-approx-posterior.html#grid-approximation-6",
    "href": "files/lectures/07-1-approx-posterior.html#grid-approximation-6",
    "title": "Approximating the Posterior",
    "section": "Grid Approximation",
    "text": "Grid Approximation\n\nInstead of using the posterior we know, let’s approximate it using grid approximation.\nSecond step: evaluate the prior pdf, f(\\theta), and the likelihood function, L(\\theta|y) at each \\theta grid value.\n\nWe will use dbeta() and dbinom() to evaluate the \\text{Beta}(2,2) prior and \\text{Bin}(10, \\pi) likelihood with Y=9 at each \\pi in pi_grid.\n\n\n\ngrid_data &lt;- grid_data %&gt;%\n  mutate(prior = dbeta(pi_grid, 2, 2),\n         likelihood = dbinom(9, 10, pi_grid))"
  },
  {
    "objectID": "files/lectures/07-1-approx-posterior.html#grid-approximation-7",
    "href": "files/lectures/07-1-approx-posterior.html#grid-approximation-7",
    "title": "Approximating the Posterior",
    "section": "Grid Approximation",
    "text": "Grid Approximation\n\nInstead of using the posterior we know, let’s approximate it using grid approximation.\nSecond step: evaluate the prior pdf, f(\\theta), and the likelihood function, L(\\theta|y) at each \\theta grid value.\n\nWe will use dbeta() and dbinom() to evaluate the \\text{Beta}(2,2) prior and \\text{Bin}(10, \\pi) likelihood with Y=9 at each \\pi in pi_grid.\n\n\n\ngrid_data &lt;- grid_data %&gt;%\n  mutate(prior = dbeta(pi_grid, 2, 2),\n         likelihood = dbinom(9, 10, pi_grid))"
  },
  {
    "objectID": "files/lectures/07-1-approx-posterior.html#grid-approximation-8",
    "href": "files/lectures/07-1-approx-posterior.html#grid-approximation-8",
    "title": "Approximating the Posterior",
    "section": "Grid Approximation",
    "text": "Grid Approximation\n\nInstead of using the posterior we know, let’s approximate it using grid approximation.\nThird step: obtain a discrete approximation of the posterior pdf, f(\\theta|y) by calculating the product f(\\theta) L(\\theta|y) at each \\theta grid value and normalizing the products to sum to 1 across all \\theta.\n\n\ngrid_data &lt;- grid_data %&gt;%\n  mutate(unnormalized = likelihood*prior,\n         posterior = unnormalized / sum(unnormalized))"
  },
  {
    "objectID": "files/lectures/07-1-approx-posterior.html#grid-approximation-9",
    "href": "files/lectures/07-1-approx-posterior.html#grid-approximation-9",
    "title": "Approximating the Posterior",
    "section": "Grid Approximation",
    "text": "Grid Approximation\n\nInstead of using the posterior we know, let’s approximate it using grid approximation.\nThird step: obtain a discrete approximation of the posterior pdf, f(\\theta|y) by calculating the product f(\\theta) L(\\theta|y) at each \\theta grid value and normalizing the products to sum to 1 across all \\theta.\n\n\ngrid_data &lt;- grid_data %&gt;%\n  mutate(unnormalized = likelihood*prior,\n         posterior = unnormalized / sum(unnormalized))\n\n\nWe can verify,\n\n\ngrid_data %&gt;%\n  summarize(sum(unnormalized), sum(posterior))"
  },
  {
    "objectID": "files/lectures/07-1-approx-posterior.html#grid-approximation-10",
    "href": "files/lectures/07-1-approx-posterior.html#grid-approximation-10",
    "title": "Approximating the Posterior",
    "section": "Grid Approximation",
    "text": "Grid Approximation\n\nInstead of using the posterior we know, let’s approximate it using grid approximation.\nThird step: obtain a discrete approximation of the posterior pdf, f(\\theta|y) by calculating the product f(\\theta) L(\\theta|y) at each \\theta grid value and normalizing the products to sum to 1 across all \\theta.\n\n\ngrid_data &lt;- grid_data %&gt;%\n  mutate(unnormalized = likelihood*prior,\n         posterior = unnormalized / sum(unnormalized))"
  },
  {
    "objectID": "files/lectures/07-1-approx-posterior.html#grid-approximation-11",
    "href": "files/lectures/07-1-approx-posterior.html#grid-approximation-11",
    "title": "Approximating the Posterior",
    "section": "Grid Approximation",
    "text": "Grid Approximation\n\nInstead of using the posterior we know, let’s approximate it using grid approximation.\nWe now have a glimpse into the actual posterior pdf.\n\nWe can plot it to see what it looks like,"
  },
  {
    "objectID": "files/lectures/07-1-approx-posterior.html#grid-approximation-12",
    "href": "files/lectures/07-1-approx-posterior.html#grid-approximation-12",
    "title": "Approximating the Posterior",
    "section": "Grid Approximation",
    "text": "Grid Approximation\n\nAs we increase the number of possible \\theta values, the better we can “see” the resulting posterior.\nWhat happens if we try the following: n=50, n=100, n=500, n=1000?"
  },
  {
    "objectID": "files/lectures/07-1-approx-posterior.html#grid-approximation-13",
    "href": "files/lectures/07-1-approx-posterior.html#grid-approximation-13",
    "title": "Approximating the Posterior",
    "section": "Grid Approximation",
    "text": "Grid Approximation\n\nAs we increase the number of possible \\theta values, the better we can “see” the resulting posterior.\nWhat happens if we try the following: n=50, n=100, n=500, n=1000?"
  },
  {
    "objectID": "files/lectures/07-1-approx-posterior.html#markov-chain-monte-carlo-mcmc",
    "href": "files/lectures/07-1-approx-posterior.html#markov-chain-monte-carlo-mcmc",
    "title": "Approximating the Posterior",
    "section": "Markov chain Monte Carlo (MCMC)",
    "text": "Markov chain Monte Carlo (MCMC)\n\nMarkov chain Monte Carlo (MCMC) is an application of Markov chains to simulate probability models.\nMCMC samples are not taken directly from the posterior pdf, f(\\theta | y)… and they are not independent.\n\nEach subsequent value depends on the previous value.\n\nSuppose we have an N-length MCMC sample, \\left\\{ \\theta^{(1)}, \\theta^{(2)}, \\theta^{(3)}, ..., \\theta^{(N)} \\right\\}\n\n\\theta^{(2)} is drawn from a model that depends on \\theta^{(1)}.\n\\theta^{(3)} is drawn from a model that depends on \\theta^{(2)}.\netc."
  },
  {
    "objectID": "files/lectures/07-1-approx-posterior.html#markov-chain-monte-carlo-mcmc-1",
    "href": "files/lectures/07-1-approx-posterior.html#markov-chain-monte-carlo-mcmc-1",
    "title": "Approximating the Posterior",
    "section": "Markov chain Monte Carlo (MCMC)",
    "text": "Markov chain Monte Carlo (MCMC)\n\nThe (i+1)st chain value, \\theta^{(i+1)} is drawn from a model that depends on data y and the previous chain value, \\theta^{(i)}.\n\nf\\left( \\theta^{(i+1)} | \\theta^{(i)}, y \\right)\n\nIt is important for us to note that the pdf from which a Markov chain is simulated is not equivalent to the posterior pdf!\n\nf\\left( \\theta^{(i+1)} | \\theta^{(i)}, y  \\right) \\ne f\\left(\\theta^{(i+1)}|y \\right)"
  },
  {
    "objectID": "files/lectures/07-1-approx-posterior.html#using-rstan",
    "href": "files/lectures/07-1-approx-posterior.html#using-rstan",
    "title": "Approximating the Posterior",
    "section": "Using rstan",
    "text": "Using rstan\n\nWe will use rstan:\n\ndefine the Bayesian model structure in rstan notation\nsimulate the posterior\n\nAgain, we will use the Beta-Binomial model from earlier.\n\ndata: in our example, Y is the observed number of successes in 10 trials.\n\nWe need to tell rstan that Y is an integer between 0 and 10.\n\nparameters: in our example, our model depends on \\pi.\n\nWe need to tell rstan that \\pi can be any real number between 0 and 1.\n\nmodel: in our example, we need to specify Y \\sim \\text{Bin}(10, \\pi) and \\pi \\sim \\text{Beta}(2,2)."
  },
  {
    "objectID": "files/lectures/07-1-approx-posterior.html#using-rstan-1",
    "href": "files/lectures/07-1-approx-posterior.html#using-rstan-1",
    "title": "Approximating the Posterior",
    "section": "Using rstan",
    "text": "Using rstan\n\n# STEP 1: DEFINE the model\nbb_model &lt;- \"\n  data {\n    int&lt;lower = 0, upper = 10&gt; Y;\n  }\n  parameters {\n    real&lt;lower = 0, upper = 1&gt; pi;\n  }\n  model {\n    Y ~ binomial(10, pi);\n    pi ~ beta(2, 2);\n  }\n\""
  },
  {
    "objectID": "files/lectures/07-1-approx-posterior.html#using-rstan-2",
    "href": "files/lectures/07-1-approx-posterior.html#using-rstan-2",
    "title": "Approximating the Posterior",
    "section": "Using rstan",
    "text": "Using rstan\n\nThen, when we go to simulate, we first put in the model information\n\nmodel code: the character string defining the model (in our case, bb_model).\ndata: a list of the observed data.\n\nIn this example, we are using Y = 9 - a single data point.\n\n\nThen, we put in the Markov chain information,\n\nchains: how many parallel Markov chains to run.\n\nThis will be the number of distinct \\theta values we want.\n\niter: desired number of iterations, or length of Markov chain.\n\nHalf are thrown out as “burn in” samples.\n\n“burn in”? Think: pancakes!\n\n\nseed: used to set the seed of the RNG."
  },
  {
    "objectID": "files/lectures/07-1-approx-posterior.html#using-rstan-3",
    "href": "files/lectures/07-1-approx-posterior.html#using-rstan-3",
    "title": "Approximating the Posterior",
    "section": "Using rstan",
    "text": "Using rstan\n\n# STEP 2: simulate the posterior\nbb_sim &lt;- stan(model_code = bb_model, data = list(Y = 9), \n               chains = 4, iter = 5000*2, seed = 84735)\n\n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 4e-06 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.04 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 1: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 1: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 1: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 1: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 1: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 1: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 1: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 1: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 1: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 1: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 1: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.013 seconds (Warm-up)\nChain 1:                0.015 seconds (Sampling)\nChain 1:                0.028 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 0 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 2: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 2: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 2: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 2: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 2: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 2: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 2: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 2: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 2: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 2: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 2: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.013 seconds (Warm-up)\nChain 2:                0.015 seconds (Sampling)\nChain 2:                0.028 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 1e-06 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.01 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 3: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 3: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 3: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 3: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 3: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 3: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 3: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 3: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 3: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 3: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 3: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 0.013 seconds (Warm-up)\nChain 3:                0.013 seconds (Sampling)\nChain 3:                0.026 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 3e-06 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.03 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 4: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 4: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 4: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 4: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 4: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 4: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 4: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 4: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 4: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 4: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 4: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 0.013 seconds (Warm-up)\nChain 4:                0.015 seconds (Sampling)\nChain 4:                0.028 seconds (Total)\nChain 4:"
  },
  {
    "objectID": "files/lectures/07-1-approx-posterior.html#using-rstan-4",
    "href": "files/lectures/07-1-approx-posterior.html#using-rstan-4",
    "title": "Approximating the Posterior",
    "section": "Using rstan",
    "text": "Using rstan\n\nNow, we need to extract the values,\n\n\nas.array(bb_sim, pars = \"pi\") %&gt;% head(4)\n\n, , parameters = pi\n\n          chains\niterations   chain:1   chain:2   chain:3   chain:4\n      [1,] 0.8278040 0.7523560 0.6386998 0.9627572\n      [2,] 0.9087546 0.8922386 0.6254761 0.9413518\n      [3,] 0.6143859 0.8682356 0.7222360 0.9489624\n      [4,] 0.8462156 0.8792812 0.8100192 0.9413812\n\n\n\nRemember, these are not a random sample from the posterior!\nThey are also not independent!\nEach chain forms a dependent 5,000 length Markov chain of \\left\\{ \\pi^{(1)}, \\pi^{(2)}, ..., \\pi^{(5000)}\\right\\}\n\nEach chain will move along the sample space of plausible values for \\pi."
  },
  {
    "objectID": "files/lectures/07-1-approx-posterior.html#using-rstan-5",
    "href": "files/lectures/07-1-approx-posterior.html#using-rstan-5",
    "title": "Approximating the Posterior",
    "section": "Using rstan",
    "text": "Using rstan\n\nWe will look at the trace plot (using mcmc_trace() from bayesplot package) to see what the values did longitudinally.\n\n\n\nmcmc_trace(bb_sim, pars = \"pi\", size = 0.1)"
  },
  {
    "objectID": "files/lectures/07-1-approx-posterior.html#using-rstan-6",
    "href": "files/lectures/07-1-approx-posterior.html#using-rstan-6",
    "title": "Approximating the Posterior",
    "section": "Using rstan",
    "text": "Using rstan\n\nWe can also look at the mcmc_hist() and mcmc_dens() functions,"
  },
  {
    "objectID": "files/lectures/07-1-approx-posterior.html#diagnostics",
    "href": "files/lectures/07-1-approx-posterior.html#diagnostics",
    "title": "Approximating the Posterior",
    "section": "Diagnostics",
    "text": "Diagnostics\n\nSimulations are not perfect…\n\nWhat does a good Markov chain look like?\nHow can we tell if the Markov chain sample produces a reasonable approximation of the posterior?\nHow big should our Markov chain sample size be?\n\nUnfortunately there is no one answer here.\n\nYou will learn through experience, much like other nuanced areas of statistics."
  },
  {
    "objectID": "files/lectures/07-1-approx-posterior.html#diagnostics-1",
    "href": "files/lectures/07-1-approx-posterior.html#diagnostics-1",
    "title": "Approximating the Posterior",
    "section": "Diagnostics",
    "text": "Diagnostics\n\nLet’s now discuss diagnostic tools.\n\nTrace plots\nParallel chains\nEffective sample size\nAutocorrelation\n\\hat{R}"
  },
  {
    "objectID": "files/lectures/07-1-approx-posterior.html#trace-plots",
    "href": "files/lectures/07-1-approx-posterior.html#trace-plots",
    "title": "Approximating the Posterior",
    "section": "Trace Plots",
    "text": "Trace Plots\n\n\n\n\nChain A has not stabilized after 5000 iterations.\n\nIt has not “found” or does not know how to explore the range of posterior plausible \\pi values.\nThe downward trend also hints against independent noise."
  },
  {
    "objectID": "files/lectures/07-1-approx-posterior.html#trace-plots-1",
    "href": "files/lectures/07-1-approx-posterior.html#trace-plots-1",
    "title": "Approximating the Posterior",
    "section": "Trace Plots",
    "text": "Trace Plots\n\n\n\n\nWe say that Chain A is mixing slowly.\n\nThe more Markov chains behave like fast mixing (noisy) independent samples, the smaller the error in the resulting posterior approximation."
  },
  {
    "objectID": "files/lectures/07-1-approx-posterior.html#trace-plots-2",
    "href": "files/lectures/07-1-approx-posterior.html#trace-plots-2",
    "title": "Approximating the Posterior",
    "section": "Trace Plots",
    "text": "Trace Plots\n\n\n\n\nChain B is not great, either – it gets stuck when looking at a smaller value of \\pi."
  },
  {
    "objectID": "files/lectures/07-1-approx-posterior.html#trace-plots-3",
    "href": "files/lectures/07-1-approx-posterior.html#trace-plots-3",
    "title": "Approximating the Posterior",
    "section": "Trace Plots",
    "text": "Trace Plots\n\nRealistically, we are only going to do simulations when we can’t specify the posterior and must approximate\n\ni.e., we won’t be able to compare the simulation results to the “true” results.\n\nIf we see bad trace plots:\n\nCheck the model (… or your code). Are the assumed prior and data models appropriate?\nRun the chain for more iterations. Sometimes we just need a longer run to iron out issues."
  },
  {
    "objectID": "files/lectures/07-1-approx-posterior.html#parallel-chains",
    "href": "files/lectures/07-1-approx-posterior.html#parallel-chains",
    "title": "Approximating the Posterior",
    "section": "Parallel Chains",
    "text": "Parallel Chains\n\nLet’s now consider a smaller simulation, where n=50 (recall, overall n=100, but half is for burn-in).\nRun the following code:\n\n\nbb_sim_short &lt;- stan(model_code = bb_model, data = list(Y = 9), \n                     chains = 4, iter = 50*2, seed = 84735)"
  },
  {
    "objectID": "files/lectures/07-1-approx-posterior.html#parallel-chains-1",
    "href": "files/lectures/07-1-approx-posterior.html#parallel-chains-1",
    "title": "Approximating the Posterior",
    "section": "Parallel Chains",
    "text": "Parallel Chains\n\nLet’s now consider a smaller simulation, where n=50 (recall, overall n=100, but half is for burn-in).\nRun the following code:\n\n\nbb_sim_short &lt;- stan(model_code = bb_model, data = list(Y = 9), \n                     chains = 4, iter = 50*2, seed = 84735)\n\n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 1e-06 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.01 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: WARNING: There aren't enough warmup iterations to fit the\nChain 1:          three stages of adaptation as currently configured.\nChain 1:          Reducing each adaptation stage to 15%/75%/10% of\nChain 1:          the given number of warmup iterations:\nChain 1:            init_buffer = 7\nChain 1:            adapt_window = 38\nChain 1:            term_buffer = 5\nChain 1: \nChain 1: Iteration:  1 / 100 [  1%]  (Warmup)\nChain 1: Iteration: 10 / 100 [ 10%]  (Warmup)\nChain 1: Iteration: 20 / 100 [ 20%]  (Warmup)\nChain 1: Iteration: 30 / 100 [ 30%]  (Warmup)\nChain 1: Iteration: 40 / 100 [ 40%]  (Warmup)\nChain 1: Iteration: 50 / 100 [ 50%]  (Warmup)\nChain 1: Iteration: 51 / 100 [ 51%]  (Sampling)\nChain 1: Iteration: 60 / 100 [ 60%]  (Sampling)\nChain 1: Iteration: 70 / 100 [ 70%]  (Sampling)\nChain 1: Iteration: 80 / 100 [ 80%]  (Sampling)\nChain 1: Iteration: 90 / 100 [ 90%]  (Sampling)\nChain 1: Iteration: 100 / 100 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0 seconds (Warm-up)\nChain 1:                0 seconds (Sampling)\nChain 1:                0 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 0 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: WARNING: There aren't enough warmup iterations to fit the\nChain 2:          three stages of adaptation as currently configured.\nChain 2:          Reducing each adaptation stage to 15%/75%/10% of\nChain 2:          the given number of warmup iterations:\nChain 2:            init_buffer = 7\nChain 2:            adapt_window = 38\nChain 2:            term_buffer = 5\nChain 2: \nChain 2: Iteration:  1 / 100 [  1%]  (Warmup)\nChain 2: Iteration: 10 / 100 [ 10%]  (Warmup)\nChain 2: Iteration: 20 / 100 [ 20%]  (Warmup)\nChain 2: Iteration: 30 / 100 [ 30%]  (Warmup)\nChain 2: Iteration: 40 / 100 [ 40%]  (Warmup)\nChain 2: Iteration: 50 / 100 [ 50%]  (Warmup)\nChain 2: Iteration: 51 / 100 [ 51%]  (Sampling)\nChain 2: Iteration: 60 / 100 [ 60%]  (Sampling)\nChain 2: Iteration: 70 / 100 [ 70%]  (Sampling)\nChain 2: Iteration: 80 / 100 [ 80%]  (Sampling)\nChain 2: Iteration: 90 / 100 [ 90%]  (Sampling)\nChain 2: Iteration: 100 / 100 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0 seconds (Warm-up)\nChain 2:                0 seconds (Sampling)\nChain 2:                0 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 0 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: WARNING: There aren't enough warmup iterations to fit the\nChain 3:          three stages of adaptation as currently configured.\nChain 3:          Reducing each adaptation stage to 15%/75%/10% of\nChain 3:          the given number of warmup iterations:\nChain 3:            init_buffer = 7\nChain 3:            adapt_window = 38\nChain 3:            term_buffer = 5\nChain 3: \nChain 3: Iteration:  1 / 100 [  1%]  (Warmup)\nChain 3: Iteration: 10 / 100 [ 10%]  (Warmup)\nChain 3: Iteration: 20 / 100 [ 20%]  (Warmup)\nChain 3: Iteration: 30 / 100 [ 30%]  (Warmup)\nChain 3: Iteration: 40 / 100 [ 40%]  (Warmup)\nChain 3: Iteration: 50 / 100 [ 50%]  (Warmup)\nChain 3: Iteration: 51 / 100 [ 51%]  (Sampling)\nChain 3: Iteration: 60 / 100 [ 60%]  (Sampling)\nChain 3: Iteration: 70 / 100 [ 70%]  (Sampling)\nChain 3: Iteration: 80 / 100 [ 80%]  (Sampling)\nChain 3: Iteration: 90 / 100 [ 90%]  (Sampling)\nChain 3: Iteration: 100 / 100 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 0 seconds (Warm-up)\nChain 3:                0 seconds (Sampling)\nChain 3:                0 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 0 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: WARNING: There aren't enough warmup iterations to fit the\nChain 4:          three stages of adaptation as currently configured.\nChain 4:          Reducing each adaptation stage to 15%/75%/10% of\nChain 4:          the given number of warmup iterations:\nChain 4:            init_buffer = 7\nChain 4:            adapt_window = 38\nChain 4:            term_buffer = 5\nChain 4: \nChain 4: Iteration:  1 / 100 [  1%]  (Warmup)\nChain 4: Iteration: 10 / 100 [ 10%]  (Warmup)\nChain 4: Iteration: 20 / 100 [ 20%]  (Warmup)\nChain 4: Iteration: 30 / 100 [ 30%]  (Warmup)\nChain 4: Iteration: 40 / 100 [ 40%]  (Warmup)\nChain 4: Iteration: 50 / 100 [ 50%]  (Warmup)\nChain 4: Iteration: 51 / 100 [ 51%]  (Sampling)\nChain 4: Iteration: 60 / 100 [ 60%]  (Sampling)\nChain 4: Iteration: 70 / 100 [ 70%]  (Sampling)\nChain 4: Iteration: 80 / 100 [ 80%]  (Sampling)\nChain 4: Iteration: 90 / 100 [ 90%]  (Sampling)\nChain 4: Iteration: 100 / 100 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 0 seconds (Warm-up)\nChain 4:                0 seconds (Sampling)\nChain 4:                0 seconds (Total)\nChain 4:"
  },
  {
    "objectID": "files/lectures/07-1-approx-posterior.html#parallel-chains-2",
    "href": "files/lectures/07-1-approx-posterior.html#parallel-chains-2",
    "title": "Approximating the Posterior",
    "section": "Parallel Chains",
    "text": "Parallel Chains\n\nLet’s now generate the trace and density plots for the smaller simulation:\n\n\nggarrange(mcmc_trace(bb_sim_short, pars = \"pi\") + ggtitle(\"trace\") + theme_bw() + theme(legend.position=\"none\"),\n          mcmc_dens_overlay(bb_sim_short, pars = \"pi\") + ylab(\"density\") + ggtitle(\"density\") + theme_bw(),\n          ncol=2, nrow=1)"
  },
  {
    "objectID": "files/lectures/07-1-approx-posterior.html#parallel-chains-3",
    "href": "files/lectures/07-1-approx-posterior.html#parallel-chains-3",
    "title": "Approximating the Posterior",
    "section": "Parallel Chains",
    "text": "Parallel Chains"
  },
  {
    "objectID": "files/lectures/07-1-approx-posterior.html#parallel-chains-4",
    "href": "files/lectures/07-1-approx-posterior.html#parallel-chains-4",
    "title": "Approximating the Posterior",
    "section": "Parallel Chains",
    "text": "Parallel Chains\n\nNow you try 10,000 iterations. Update the following code:\n\n\nbb_sim_full &lt;- stan(model_code = bb_model, \n                    data = list(Y = 9), \n                    chains = 4, \n                    iter = 50*2, \n                    seed = 84735)"
  },
  {
    "objectID": "files/lectures/07-1-approx-posterior.html#parallel-chains-5",
    "href": "files/lectures/07-1-approx-posterior.html#parallel-chains-5",
    "title": "Approximating the Posterior",
    "section": "Parallel Chains",
    "text": "Parallel Chains\n\nCreate the trace and density plots. Recall the code,\n\n\nggarrange(\n  mcmc_trace(bb_sim, pars = \"pi\") + theme_bw() + theme(legend.position=\"none\") + ggtitle(\"trace\"),\n  mcmc_dens_overlay(bb_sim, pars = \"pi\") +  ylab(\"density\") + ggtitle(\"density\") + theme_bw(),\n  ncol = 2, nrow = 1)"
  },
  {
    "objectID": "files/lectures/07-1-approx-posterior.html#parallel-chains-6",
    "href": "files/lectures/07-1-approx-posterior.html#parallel-chains-6",
    "title": "Approximating the Posterior",
    "section": "Parallel Chains",
    "text": "Parallel Chains\n\nCreate the trace and density plots."
  },
  {
    "objectID": "files/lectures/07-1-approx-posterior.html#effective-sample-size",
    "href": "files/lectures/07-1-approx-posterior.html#effective-sample-size",
    "title": "Approximating the Posterior",
    "section": "Effective Sample Size",
    "text": "Effective Sample Size\n\nThe more a dependent Markov chain behaves like an independent sample, the smaller the error in the resulting posterior approximation.\n\nPlots are great, but numerical assessment can provide more nuanced information.\n\nEffective sample size (N_{\\text{eff}}): the number of independent sample values it would take to produce an equivalently accurate posterior approximation.\nEffective sample size ratio:\n\n\\frac{N_{\\text{eff}}}{N}\n\nGenerally, we look for the effective sample size, N_{\\text{eff}}, to be greater than 10% of the actual sample size, N."
  },
  {
    "objectID": "files/lectures/07-1-approx-posterior.html#effective-sample-size-1",
    "href": "files/lectures/07-1-approx-posterior.html#effective-sample-size-1",
    "title": "Approximating the Posterior",
    "section": "Effective Sample Size",
    "text": "Effective Sample Size\n\nWe will use the neff_ratio() function to find this ratio.\nIn our example data,\n\n\n# Calculate the effective sample size ratio - N = 50\nneff_ratio(bb_sim, pars = c(\"pi\"))\n\n[1] 0.3462257\n\n# Calculate the effective sample size ratio - N = 10000\nneff_ratio(bb_sim_short, pars = c(\"pi\"))\n\n[1] 0.4950171\n\n\n\nBecause the N_{\\text{eff}} is over 10%, we are not concerned and can proceed."
  },
  {
    "objectID": "files/lectures/07-1-approx-posterior.html#autocorrelation",
    "href": "files/lectures/07-1-approx-posterior.html#autocorrelation",
    "title": "Approximating the Posterior",
    "section": "Autocorrelation",
    "text": "Autocorrelation\n\nAutocorrelation allows us to evaluate if our Markov chain sufficiently mimics the behavior of an independent sample.\nAutocorrelation:\n\nLag 1 autocorrelation measures the correlation between pairs of Markov chain values that are one “step” apart (e.g., \\pi_i and \\pi_{(i-1)}; e.g., \\pi_4 and \\pi_3).\nLag 2 autocorrelation measures the correlation between pairs of Markov chain values that are two “steps apart (e.g., \\pi_i and \\pi_{(i-2)}; e.g., \\pi_4 and \\pi_2).\nLag k autocorrelation measures the correlation between pairs of Markov chain values that are k “steps” apart (e.g., \\pi_i and \\pi_{(i-k)}; e.g., \\pi_4 and \\pi_{(4-k)})."
  },
  {
    "objectID": "files/lectures/07-1-approx-posterior.html#autocorrelation-1",
    "href": "files/lectures/07-1-approx-posterior.html#autocorrelation-1",
    "title": "Approximating the Posterior",
    "section": "Autocorrelation",
    "text": "Autocorrelation\n\nAutocorrelation allows us to evaluate if our Markov chain sufficiently mimics the behavior of an independent sample.\nStrong autocorrelation or dependence is a bad thing.\n\nIt goes hand in hand with small effective sample size ratios.\nThese provide a warning sign that our resulting posterior approximations might be unreliable."
  },
  {
    "objectID": "files/lectures/07-1-approx-posterior.html#autocorrelation-2",
    "href": "files/lectures/07-1-approx-posterior.html#autocorrelation-2",
    "title": "Approximating the Posterior",
    "section": "Autocorrelation",
    "text": "Autocorrelation\n\n\n\n\nNo obvious patterns in the trace plot; dependence is relatively weak.\nAutocorrelation plot quickly drops off and is effectively 0 by lag 5.\nConfirmation that our Markov chain is mixing quickly.\n\ni.e., quickly moving around the range of posterior plausible \\pi values\ni.e., at least mimicking an independent sample."
  },
  {
    "objectID": "files/lectures/07-1-approx-posterior.html#autocorrelation-3",
    "href": "files/lectures/07-1-approx-posterior.html#autocorrelation-3",
    "title": "Approximating the Posterior",
    "section": "Autocorrelation",
    "text": "Autocorrelation\n\n\n\n\nThis is an “unhealthy” Markov chain.\nTrace plot shows strong trends \\to autocorrelation in the Markov chain values.\nSlow decrease in autocorrelation plot indicates that the dependence between chain values does not quickly fade away.\n\nAt lag 20, the autocorrelation is still \\sim 90%."
  },
  {
    "objectID": "files/lectures/07-1-approx-posterior.html#fast-vs.-slow-mixing-markov-chains",
    "href": "files/lectures/07-1-approx-posterior.html#fast-vs.-slow-mixing-markov-chains",
    "title": "Approximating the Posterior",
    "section": "Fast vs. Slow Mixing Markov Chains",
    "text": "Fast vs. Slow Mixing Markov Chains\n\nFast mixing chains:\n\nThe chains move “quickly” around the range of posterior plausible values\nThe autocorrelation among the chain values drops off quickly.\nThe effective sample size ratio is reasonably large.\n\nSlow mixing chains:\n\nThe chains move “slowly” around the range of posterior plauslbe values.\nThe autocorrelation among the chainv alues drops off very slowly.\nThe effective sample size ratio is small.\n\nWhat do we do if we have a slow mixing chain?\n\nIncrease the chain size :)\nThin the Markov chain :|"
  },
  {
    "objectID": "files/lectures/07-1-approx-posterior.html#thinning-markov-chains",
    "href": "files/lectures/07-1-approx-posterior.html#thinning-markov-chains",
    "title": "Approximating the Posterior",
    "section": "Thinning Markov Chains",
    "text": "Thinning Markov Chains\n\nLet’s thin our original results, bb_sim, to every tenth value using the thin argument in stan().\n\n\nthinned_sim &lt;- stan(model_code = bb_model, data = list(Y = 9), \n                    chains = 4, iter = 5000*2, seed = 84735, thin = 10)\n\nmcmc_trace(thinned_sim, pars = \"pi\")\nmcmc_acf(thinned_sim, pars = \"pi\")"
  },
  {
    "objectID": "files/lectures/07-1-approx-posterior.html#thinning-markov-chains-1",
    "href": "files/lectures/07-1-approx-posterior.html#thinning-markov-chains-1",
    "title": "Approximating the Posterior",
    "section": "Thinning Markov Chains",
    "text": "Thinning Markov Chains\n\nLet’s thin our original results, bb_sim, to every tenth value using the thin argument in stan().\n\n\nthinned_sim &lt;- stan(model_code = bb_model, data = list(Y = 9), \n                    chains = 4, iter = 5000*2, seed = 84735, thin = 10)\n\n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 2e-06 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.02 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 1: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 1: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 1: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 1: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 1: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 1: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 1: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 1: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 1: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 1: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 1: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.009 seconds (Warm-up)\nChain 1:                0.011 seconds (Sampling)\nChain 1:                0.02 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 0 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 2: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 2: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 2: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 2: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 2: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 2: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 2: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 2: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 2: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 2: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 2: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.01 seconds (Warm-up)\nChain 2:                0.011 seconds (Sampling)\nChain 2:                0.021 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 0 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 3: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 3: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 3: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 3: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 3: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 3: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 3: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 3: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 3: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 3: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 3: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 0.01 seconds (Warm-up)\nChain 3:                0.024 seconds (Sampling)\nChain 3:                0.034 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 0 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 4: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 4: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 4: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 4: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 4: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 4: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 4: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 4: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 4: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 4: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 4: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 0.009 seconds (Warm-up)\nChain 4:                0.011 seconds (Sampling)\nChain 4:                0.02 seconds (Total)\nChain 4:"
  },
  {
    "objectID": "files/lectures/07-1-approx-posterior.html#thinning-markov-chains-2",
    "href": "files/lectures/07-1-approx-posterior.html#thinning-markov-chains-2",
    "title": "Approximating the Posterior",
    "section": "Thinning Markov Chains",
    "text": "Thinning Markov Chains\n\nLet’s thin our original results, bb_sim, to every tenth value using the thin argument in stan()."
  },
  {
    "objectID": "files/lectures/07-1-approx-posterior.html#thinning-markov-chains-3",
    "href": "files/lectures/07-1-approx-posterior.html#thinning-markov-chains-3",
    "title": "Approximating the Posterior",
    "section": "Thinning Markov Chains",
    "text": "Thinning Markov Chains\n\nWarning!\n\nThe benefits of reduced autocorrelation do not necessarily outweigh the loss of chain values.\ni.e., 5,000 Markov chain values with stronger autocorrelation may be a better posterior approximation than 500 chain values with weaker autocorrelation.\n\nThe effectiveness depends on the algorithm used to construct the Markov chain.\n\nFolks advise against thinning unless you need memory space on your computer."
  },
  {
    "objectID": "files/lectures/07-1-approx-posterior.html#hatr",
    "href": "files/lectures/07-1-approx-posterior.html#hatr",
    "title": "Approximating the Posterior",
    "section": "\\hat{R}",
    "text": "\\hat{R}\n\\hat{R} \\approx \\sqrt{\\frac{\\text{var}_{\\text{combined}}}{\\text{var}_{\\text{within}}}}\n\nwhere\n\n\\text{var}_{\\text{combined}} is the variability in \\theta across all chains combined.\n\\text{var}_{\\text{within}} is the typical variability within any individual chain.\n\n\\hat{R} compares the variability in sampled \\theta values across all chains combined with the variability within each individual change.\n\nIdeally, \\hat{R} \\approx 1, showing stability across chains.\n\\hat{R} &gt; 1 indicates instability with the variability in the combined chains larger than that of the variability within the chains.\n\\hat{R} &gt; 1.05 raises red flags about the stability of the simulation."
  },
  {
    "objectID": "files/lectures/07-1-approx-posterior.html#hatr-1",
    "href": "files/lectures/07-1-approx-posterior.html#hatr-1",
    "title": "Approximating the Posterior",
    "section": "\\hat{R}",
    "text": "\\hat{R}\n\nWe can use the rhat() function from the bayesplot package to find \\hat{R}.\n\n\nrhat(bb_sim, pars = \"pi\")\n\n[1] 1.000245\n\n\n\nWe can see that our simulation is stable.\nIf we were to find \\hat{R} for the other (obviously bad) simulation, it would be 5.35 😱"
  },
  {
    "objectID": "files/lectures/07-1-approx-posterior.html#homework-practice",
    "href": "files/lectures/07-1-approx-posterior.html#homework-practice",
    "title": "Approximating the Posterior",
    "section": "Homework / Practice",
    "text": "Homework / Practice\n\nFrom the Bayes Rules! textbook:\n\n6.5\n6.6\n6.7\n6.13\n6.14\n6.15\n6.17"
  },
  {
    "objectID": "files/lectures/04-2-thinking-bayesian.html#introduction",
    "href": "files/lectures/04-2-thinking-bayesian.html#introduction",
    "title": "Thinking Like a Bayesian",
    "section": "Introduction",
    "text": "Introduction\n\nBefore today:\n\nRefresher on probability theory\nWhat does each distribution do?\n\nbeta \\to outcomes limited to [0, 1]\nbinomial \\to binary outcomes\ngamma \\to continuous & positive outcomes; skewed right\nnormal \\to continuous outcomes; mound-shaped & symmetric distribution\nPoisson \\to count outcomes; skewed right\nuniform \\to each outcome has equal probability; rectangular distribution\n\n\nToday: building up Bayesian analysis concepts"
  },
  {
    "objectID": "files/lectures/04-2-thinking-bayesian.html#thinking-like-a-bayesian",
    "href": "files/lectures/04-2-thinking-bayesian.html#thinking-like-a-bayesian",
    "title": "Thinking Like a Bayesian",
    "section": "Thinking Like a Bayesian",
    "text": "Thinking Like a Bayesian\n\nBayesian analysis involves updating beliefs based on observed data."
  },
  {
    "objectID": "files/lectures/04-2-thinking-bayesian.html#thinking-like-a-bayesian-1",
    "href": "files/lectures/04-2-thinking-bayesian.html#thinking-like-a-bayesian-1",
    "title": "Thinking Like a Bayesian",
    "section": "Thinking Like a Bayesian",
    "text": "Thinking Like a Bayesian\n\nThis is the natural Bayesian knowledge-building process of:\n\nacknowledging your preconceptions (prior distribution),\nusing data (data distribution) to update your knowledge (posterior distribution), and\nrepeating (posterior distribution \\to new prior distribution)"
  },
  {
    "objectID": "files/lectures/04-2-thinking-bayesian.html#thinking-like-a-bayesian-2",
    "href": "files/lectures/04-2-thinking-bayesian.html#thinking-like-a-bayesian-2",
    "title": "Thinking Like a Bayesian",
    "section": "Thinking Like a Bayesian",
    "text": "Thinking Like a Bayesian\n\nBayesian and frequentist analyses share a common goal: to learn from data about the world around us.\n\nBoth Bayesian and frequentist analyses use data to fit models, make predictions, and evaluate hypotheses.\nWhen working with the same data, they will typically produce a similar set of conclusions.\n\nStatisticians typically identify as either a “Bayesian” or “frequentist” …\n\n🚫 We are not going to “take sides.”\n✅ We will see these as tools in our toolbox."
  },
  {
    "objectID": "files/lectures/04-2-thinking-bayesian.html#thinking-like-a-bayesian-3",
    "href": "files/lectures/04-2-thinking-bayesian.html#thinking-like-a-bayesian-3",
    "title": "Thinking Like a Bayesian",
    "section": "Thinking Like a Bayesian",
    "text": "Thinking Like a Bayesian\n\nBayesian probability: the relative plausibility of an event.\n\nConsiders prior belief."
  },
  {
    "objectID": "files/lectures/04-2-thinking-bayesian.html#thinking-like-a-bayesian-4",
    "href": "files/lectures/04-2-thinking-bayesian.html#thinking-like-a-bayesian-4",
    "title": "Thinking Like a Bayesian",
    "section": "Thinking Like a Bayesian",
    "text": "Thinking Like a Bayesian\n\nFrequentist probability: the long-run relative frequency of a repeatable event.\n\nDoes not consider prior belief."
  },
  {
    "objectID": "files/lectures/04-2-thinking-bayesian.html#thinking-like-a-bayesian-5",
    "href": "files/lectures/04-2-thinking-bayesian.html#thinking-like-a-bayesian-5",
    "title": "Thinking Like a Bayesian",
    "section": "Thinking Like a Bayesian",
    "text": "Thinking Like a Bayesian\n\nThe Bayesian framework depends upon prior information, data, and the balance between them.\n\nThe balance between the prior information and data is determined by the relative strength of each\n\n\n\n\nWhen we have little data, our posterior can rely more on prior knowledge.\nAs we collect more data, the prior can lose its influence."
  },
  {
    "objectID": "files/lectures/04-2-thinking-bayesian.html#thinking-like-a-bayesian-6",
    "href": "files/lectures/04-2-thinking-bayesian.html#thinking-like-a-bayesian-6",
    "title": "Thinking Like a Bayesian",
    "section": "Thinking Like a Bayesian",
    "text": "Thinking Like a Bayesian\n\nWe can also use this approach to combine analysis results."
  },
  {
    "objectID": "files/lectures/04-2-thinking-bayesian.html#thinking-like-a-bayesian-7",
    "href": "files/lectures/04-2-thinking-bayesian.html#thinking-like-a-bayesian-7",
    "title": "Thinking Like a Bayesian",
    "section": "Thinking Like a Bayesian",
    "text": "Thinking Like a Bayesian\n\nWe will use an example to work through Bayesian logic.\nThe Collins Dictionary named “fake news” the 2017 term of the year.\n\nFake, misleading, and biased news has proliferated along with online news and social media platforms which allow users to post articles with little quality control.\n\nWe want to flag articles as “real” or “fake.”\nWe’ll examine a sample of 150 articles which were posted on Facebook and fact checked by five BuzzFeed journalists (Shu et al. 2017)."
  },
  {
    "objectID": "files/lectures/04-2-thinking-bayesian.html#thinking-like-a-bayesian-8",
    "href": "files/lectures/04-2-thinking-bayesian.html#thinking-like-a-bayesian-8",
    "title": "Thinking Like a Bayesian",
    "section": "Thinking Like a Bayesian",
    "text": "Thinking Like a Bayesian\n\nInformation about each article is stored in the fake_news dataset in the bayesrules package.\n\n\nfake_news &lt;- bayesrules::fake_news\nprint(colnames(fake_news))\n\n [1] \"title\"                   \"text\"                   \n [3] \"url\"                     \"authors\"                \n [5] \"type\"                    \"title_words\"            \n [7] \"text_words\"              \"title_char\"             \n [9] \"text_char\"               \"title_caps\"             \n[11] \"text_caps\"               \"title_caps_percent\"     \n[13] \"text_caps_percent\"       \"title_excl\"             \n[15] \"text_excl\"               \"title_excl_percent\"     \n[17] \"text_excl_percent\"       \"title_has_excl\"         \n[19] \"anger\"                   \"anticipation\"           \n[21] \"disgust\"                 \"fear\"                   \n[23] \"joy\"                     \"sadness\"                \n[25] \"surprise\"                \"trust\"                  \n[27] \"negative\"                \"positive\"               \n[29] \"text_syllables\"          \"text_syllables_per_word\""
  },
  {
    "objectID": "files/lectures/04-2-thinking-bayesian.html#thinking-like-a-bayesian-9",
    "href": "files/lectures/04-2-thinking-bayesian.html#thinking-like-a-bayesian-9",
    "title": "Thinking Like a Bayesian",
    "section": "Thinking Like a Bayesian",
    "text": "Thinking Like a Bayesian\n\nWe could build a simple news filter which uses the following rule: since most articles are real, we should read and believe all articles.\n\nWhile this filter would solve the problem of disregarding real articles, we would read lots of fake news.\nIt also only takes into account the overall rates of, not the typical features of, real and fake news."
  },
  {
    "objectID": "files/lectures/04-2-thinking-bayesian.html#thinking-like-a-bayesian-10",
    "href": "files/lectures/04-2-thinking-bayesian.html#thinking-like-a-bayesian-10",
    "title": "Thinking Like a Bayesian",
    "section": "Thinking Like a Bayesian",
    "text": "Thinking Like a Bayesian\n\nSuppose that the most recent article posted to a social media platform is titled: The president has a funny secret!\n\nSome features of this title probably set off some red flags.\nFor example, the usage of an exclamation point might seem like an odd choice for a real news article.\n\nIn the dataset, what is the split of real and fake articles?"
  },
  {
    "objectID": "files/lectures/04-2-thinking-bayesian.html#thinking-like-a-bayesian-11",
    "href": "files/lectures/04-2-thinking-bayesian.html#thinking-like-a-bayesian-11",
    "title": "Thinking Like a Bayesian",
    "section": "Thinking Like a Bayesian",
    "text": "Thinking Like a Bayesian\n\nIn the dataset, what is the split of real and fake articles?\n\n\n\n\n  \n\n\n\n\nOur data backs up our instinct on the article,\n\n\n\n\n  \n\n\n\n\nIn this dataset, 26.67% (16 of 60) of fake news titles but only 2.22% (2 of 90) of real news titles use an exclamation point."
  },
  {
    "objectID": "files/lectures/04-2-thinking-bayesian.html#thinking-like-a-bayesian-12",
    "href": "files/lectures/04-2-thinking-bayesian.html#thinking-like-a-bayesian-12",
    "title": "Thinking Like a Bayesian",
    "section": "Thinking Like a Bayesian",
    "text": "Thinking Like a Bayesian\n\nWe now have two pieces of contradictory information.\n\nOur prior information suggested that incoming articles are most likely real.\nHowever, the exclamation point data is more consistent with fake news.\n\n\n\n\nThinking like Bayesians, we know that balancing both pieces of information is important in developing a posterior understanding of whether the article is fake."
  },
  {
    "objectID": "files/lectures/04-2-thinking-bayesian.html#building-a-bayesian-model",
    "href": "files/lectures/04-2-thinking-bayesian.html#building-a-bayesian-model",
    "title": "Thinking Like a Bayesian",
    "section": "Building a Bayesian Model",
    "text": "Building a Bayesian Model\n\nOur fake news analysis studies two variables:\n\nan article’s fake vs real status and\nits use of exclamation points.\n\nWe can represent the randomness in these variables using probability models.\nWe will now build:\n\na prior probability model for our prior understanding of whether the most recent article is fake;\na model for interpreting the exclamation point data; and, eventually,\na posterior probability model which summarizes the posterior plausibility that the article is fake."
  },
  {
    "objectID": "files/lectures/04-2-thinking-bayesian.html#building-a-bayesian-model-1",
    "href": "files/lectures/04-2-thinking-bayesian.html#building-a-bayesian-model-1",
    "title": "Thinking Like a Bayesian",
    "section": "Building a Bayesian Model",
    "text": "Building a Bayesian Model\n\nLet’s now formalize our prior understanding of whether the new article is fake.\nBased on our fake_news data, we saw that 40% of articles are fake and 60% are real.\n\nBefore reading the new article, there’s a 0.4 prior probability that it’s fake and a 0.6 prior probability it’s not.\n\n\nP\\left[B\\right] = 0.40 \\text{ and } P\\left[B^c\\right] = 0.60\n\nRemember that a valid probability model must:\n\naccount for all possible events (all articles must be fake or real);\nit assigns prior probabilities to each event; and\nthe probabilities sum to one."
  },
  {
    "objectID": "files/lectures/04-2-thinking-bayesian.html#building-a-bayesian-model-2",
    "href": "files/lectures/04-2-thinking-bayesian.html#building-a-bayesian-model-2",
    "title": "Thinking Like a Bayesian",
    "section": "Building a Bayesian Model",
    "text": "Building a Bayesian Model\n\n\n\n\n\n\n\n\ntitle_has_excl\nfake\nreal\n\n\n\n\nFALSE\n73.3% (44)\n97.8% (88)\n\n\nTRUE\n26.7% (16)\n2.2% (2)\n\n\n\n\n\n\n\n\nWe have the following conditional probabilities:\n\nIf an article is fake (B), then there’s a roughly 26.67% chance it uses exclamation points in the title.\nIf an article is real (B^c), then there’s only a roughly 2.22% chance it uses exclamation points."
  },
  {
    "objectID": "files/lectures/04-2-thinking-bayesian.html#building-a-bayesian-model-3",
    "href": "files/lectures/04-2-thinking-bayesian.html#building-a-bayesian-model-3",
    "title": "Thinking Like a Bayesian",
    "section": "Building a Bayesian Model",
    "text": "Building a Bayesian Model\n\nWe have the following conditional probabilities:\n\nIf an article is fake (B), then there’s a roughly 26.67% chance it uses exclamation points in the title.\nIf an article is real (B^c), then there’s only a roughly 2.22% chance it uses exclamation points.\n\nLooking at the probabilities, we can see that 26.67% of fake articles vs. 2.22% of real articles use exclamation points.\n\nExclamation point usage is much more likely among fake news than real news.\nWe have evidence that the article is fake."
  },
  {
    "objectID": "files/lectures/04-2-thinking-bayesian.html#building-a-bayesian-model-4",
    "href": "files/lectures/04-2-thinking-bayesian.html#building-a-bayesian-model-4",
    "title": "Thinking Like a Bayesian",
    "section": "Building a Bayesian Model",
    "text": "Building a Bayesian Model\n\nNote that we know that the incoming article used exclamation points (A), but we do not actually know if the article is fake (B or B^c).\nIn this case, we compared P[A|B] and P[A|B^c] to ascertain the relative likelihood of observing A under different scenarios.\n\nL\\left[B|A\\right] = P\\left[A|B\\right] \\text{ and } L\\left[B^c|A\\right] = P\\left[A|B^c\\right]"
  },
  {
    "objectID": "files/lectures/04-2-thinking-bayesian.html#building-a-bayesian-model-5",
    "href": "files/lectures/04-2-thinking-bayesian.html#building-a-bayesian-model-5",
    "title": "Thinking Like a Bayesian",
    "section": "Building a Bayesian Model",
    "text": "Building a Bayesian Model\n\n\n\nEvent\nB\nBc\nTotal\n\n\n\n\nPrior Probability\n0.4\n0.6\n1.0\n\n\nLikelihood\n0.2667\n0.0222\n0.2889\n\n\n\nL\\left[B|A\\right] = P\\left[A|B\\right] \\text{ and } L\\left[B^c|A\\right] = P\\left[A|B^c\\right]\n\nIt is important for us to note that the likelihood function is not a probability function.\n\nThis is a framework to compare the relative comparability of our exclamation point data with B and B^c."
  },
  {
    "objectID": "files/lectures/04-2-thinking-bayesian.html#building-a-bayesian-model-6",
    "href": "files/lectures/04-2-thinking-bayesian.html#building-a-bayesian-model-6",
    "title": "Thinking Like a Bayesian",
    "section": "Building a Bayesian Model",
    "text": "Building a Bayesian Model\n\n\n\nEvent\nB (fake)\nBc (real)\nTotal\n\n\n\n\nPrior Probability\n0.4\n0.6\n1.0\n\n\nLikelihood\n0.2667\n0.0222\n0.2889\n\n\n\n\nThe prior evidence suggested the article is most likely real,\n\nP[B] = 0.4 &lt; P[B^c] = 0.6\n\nThe data, however, is more consistent with the article being fake,\n\nL[B|A] = 0.2667 &gt; L[B^c|A] = 0.0222"
  },
  {
    "objectID": "files/lectures/04-2-thinking-bayesian.html#building-a-bayesian-model-7",
    "href": "files/lectures/04-2-thinking-bayesian.html#building-a-bayesian-model-7",
    "title": "Thinking Like a Bayesian",
    "section": "Building a Bayesian Model",
    "text": "Building a Bayesian Model\n\nWe can summarize our probabilities in a table,\n\n\n\n\n\nB\nB^c\nTotal\n\n\n\n\nA\n\n\n\n\n\nA^c\n\n\n\n\n\nTotal\n0.4\n0.6\n1\n\n\n\n\nAs found earlier, P[A|B] = 0.2667 and P[A|B^c]=0.0222.\n\n\n\\begin{align*}\nP[A \\cap B] &= P[A|B] \\times P[B] \\\\\n&= 0.2667 \\times 0.4 \\\\\n&= 0.1067\n\\end{align*}"
  },
  {
    "objectID": "files/lectures/04-2-thinking-bayesian.html#building-a-bayesian-model-8",
    "href": "files/lectures/04-2-thinking-bayesian.html#building-a-bayesian-model-8",
    "title": "Thinking Like a Bayesian",
    "section": "Building a Bayesian Model",
    "text": "Building a Bayesian Model\n\nHere’s what we know,\n\n\n\n\n\nB\nB^c\nTotal\n\n\n\n\nA\n0.1067\n\n\n\n\nA^c\n\n\n\n\n\nTotal\n0.4\n0.6\n1\n\n\n\n\nWe also know P[A|B] = 0.2667 and P[A|B^c]=0.0222.\n\n\n\\begin{align*}\nP[A^c \\cap B] &= P(A^c|B) \\times P[B]  \\\\\n&= (1-P[A|B]) \\times P[B] \\\\\n&= (1-0.2667) \\times 0.4 \\\\\n&= 0.2933\n\\end{align*}"
  },
  {
    "objectID": "files/lectures/04-2-thinking-bayesian.html#building-a-bayesian-model-9",
    "href": "files/lectures/04-2-thinking-bayesian.html#building-a-bayesian-model-9",
    "title": "Thinking Like a Bayesian",
    "section": "Building a Bayesian Model",
    "text": "Building a Bayesian Model\n\nHere’s what we know,\n\n\n\n\n\nB\nB^c\nTotal\n\n\n\n\nA\n0.1067\n\n\n\n\nA^c\n0.2933\n\n\n\n\nTotal\n0.4\n0.6\n1\n\n\n\n\nWe also know P[A|B] = 0.2667 and P[A|B^c]=0.0222.\n\n\n\\begin{align*}\nP[A \\cap B^c] &= P[A|B^c] \\times P[B^c]  \\\\\n&= 0.0222 \\times 0.6 \\\\\n&= 0.0133\n\\end{align*}"
  },
  {
    "objectID": "files/lectures/04-2-thinking-bayesian.html#building-a-bayesian-model-10",
    "href": "files/lectures/04-2-thinking-bayesian.html#building-a-bayesian-model-10",
    "title": "Thinking Like a Bayesian",
    "section": "Building a Bayesian Model",
    "text": "Building a Bayesian Model\n\nHere’s what we know,\n\n\n\n\n\nB\nB^c\nTotal\n\n\n\n\nA\n0.1067\n0.0133\n\n\n\nA^c\n0.2933\n\n\n\n\nTotal\n0.4\n0.6\n1\n\n\n\n\nWe also know P[A|B] = 0.2667 and P[A|B^c]=0.0222.\n\n\n\\begin{align*}\nP[A^c \\cap B^c] &= P[A^c|B^c] \\times P[B^c]  \\\\\n&= 0.9778 \\times 0.6 \\\\\n&= 0.5867\n\\end{align*}"
  },
  {
    "objectID": "files/lectures/04-2-thinking-bayesian.html#building-a-bayesian-model-11",
    "href": "files/lectures/04-2-thinking-bayesian.html#building-a-bayesian-model-11",
    "title": "Thinking Like a Bayesian",
    "section": "Building a Bayesian Model",
    "text": "Building a Bayesian Model\n\nHere’s what we know,\n\n\n\n\n\nB\nB^c\nTotal\n\n\n\n\nA\n0.1067\n0.0133\n\n\n\nA^c\n0.2933\n0.5867\n\n\n\nTotal\n0.4\n0.6\n1\n\n\n\n\nFinally,\n\n\n\\begin{align*}\n&P[A] = 0.1067 + 0.0133 = 0.12 \\\\\n&P[A^c] = 0.2933 + 0.5867 = 0.88\n\\end{align*}"
  },
  {
    "objectID": "files/lectures/04-2-thinking-bayesian.html#building-a-bayesian-model-12",
    "href": "files/lectures/04-2-thinking-bayesian.html#building-a-bayesian-model-12",
    "title": "Thinking Like a Bayesian",
    "section": "Building a Bayesian Model",
    "text": "Building a Bayesian Model\n\nUsing rules of probability, we have completed the table.\n\n\n\n\n\nB\nB^c\nTotal\n\n\n\n\nA\n0.1067\n0.0133\n0.12\n\n\nA^c\n0.2933\n0.5867\n0.88\n\n\nTotal\n0.4\n0.6\n1"
  },
  {
    "objectID": "files/lectures/04-2-thinking-bayesian.html#building-a-bayesian-model-13",
    "href": "files/lectures/04-2-thinking-bayesian.html#building-a-bayesian-model-13",
    "title": "Thinking Like a Bayesian",
    "section": "Building a Bayesian Model",
    "text": "Building a Bayesian Model\n\nWith more information, we can answer the question: what is the probability that the latest article is fake?\nWe will use the posterior probability, P[B|A], which is found using Bayes’ Rule.\nBayes’ Rule: For events A and B,\n\nP[B|A] = \\frac{P[A \\cap B]}{P[A]} = \\frac{P[B] \\times L[B|A]}{P[A]}\n\nBut really, we can think about it like this,\n\n\\text{posterior} = \\frac{\\text{prior} \\times \\text{likelihood}}{\\text{normalizing constant}}"
  },
  {
    "objectID": "files/lectures/04-2-thinking-bayesian.html#example-set-up",
    "href": "files/lectures/04-2-thinking-bayesian.html#example-set-up",
    "title": "Thinking Like a Bayesian",
    "section": "Example Set Up",
    "text": "Example Set Up\n\nIn 1996, Gary Kasparov played a six-game chess match against the IBM supercomputer Deep Blue.\n\nOf the six games, Kasparov won three, drew two, and lost one.\nThus, Kasparov won the overall match.\n\nKasparov and Deep Blue were to meet again for a six-game match in 1997.\nLet \\pi denote Kasparov’s chances of winning any particular game in the re-match.\n\nThus, \\pi is a measure of his overall skill relative to Deep Blue.\nGiven the complexity of chess, machines, and humans, \\pi is unknown and can vary over time.\n\ni.e., \\pi is a random variable."
  },
  {
    "objectID": "files/lectures/04-2-thinking-bayesian.html#example-set-up-1",
    "href": "files/lectures/04-2-thinking-bayesian.html#example-set-up-1",
    "title": "Thinking Like a Bayesian",
    "section": "Example Set Up",
    "text": "Example Set Up\n\nOur first step is to start with a prior model. This model\n\nIdentifies what values \\pi can take,\nassigns a prior weight or probability to each, and\nthese probabilities sum to 1.\n\nBased on what we were told, the prior model for \\pi in our example,\n\n\n\n\n\\pi\n0.2\n0.5\n0.8\nTotal\n\n\n\n\nf(\\pi)\n0.10\n0.25\n0.65\n1"
  },
  {
    "objectID": "files/lectures/04-2-thinking-bayesian.html#example-set-up-2",
    "href": "files/lectures/04-2-thinking-bayesian.html#example-set-up-2",
    "title": "Thinking Like a Bayesian",
    "section": "Example Set Up",
    "text": "Example Set Up\n\nBased on what we were told, the prior model for \\pi in our example,\n\n\n\n\n\\pi\n0.2\n0.5\n0.8\nTotal\n\n\n\n\nf(\\pi)\n0.10\n0.25\n0.65\n1\n\n\n\n\nNote that this is an incredibly simple model.\n\nThe win probability can technically be any number \\in [0, 1].\nHowever, this prior assumes that \\pi has a discrete set of possibilities: 20%, 50%, or 80%."
  },
  {
    "objectID": "files/lectures/04-2-thinking-bayesian.html#example-set-up-3",
    "href": "files/lectures/04-2-thinking-bayesian.html#example-set-up-3",
    "title": "Thinking Like a Bayesian",
    "section": "Example Set Up",
    "text": "Example Set Up\n\nIn the second step of our analysis, we collect and process data which can inform our understanding of \\pi.\nHere, Y = the number of the six games in the 1997 re-match that Kasparov wins.\n\nAs chess match outcome isn’t predetermined, Y is a random variable that can take any value in \\{0, 1, 2, 3, 4, 5, 6\\}.\n\nNote that Y inherently depends upon \\pi.\n\nIf \\pi = 0.80, Y would also be high (on average).\nIf \\pi = 0.20, Y would also be low (on average).\n\nThus, we must model this dependence of Y on \\pi using a conditional probability model."
  },
  {
    "objectID": "files/lectures/04-2-thinking-bayesian.html#binomial-data-model",
    "href": "files/lectures/04-2-thinking-bayesian.html#binomial-data-model",
    "title": "Thinking Like a Bayesian",
    "section": "Binomial Data Model",
    "text": "Binomial Data Model\n\nWe must make two assumptions about the chess match:\n\nGames are independent (the outcome of one game does not influence the outcome of another).\nKasparov has an equal probability of winning any game in the match.\n\ni.e., probability of winning does not increase or decrease as the match goes on.\n\n\nWe will use a binomial model for this problem.\n\nIn our case,\n\n\nY|\\pi \\sim \\text{Bin}(6, \\pi)"
  },
  {
    "objectID": "files/lectures/04-2-thinking-bayesian.html#binomial-data-model-1",
    "href": "files/lectures/04-2-thinking-bayesian.html#binomial-data-model-1",
    "title": "Thinking Like a Bayesian",
    "section": "Binomial Data Model",
    "text": "Binomial Data Model\n\nLet’s assume \\pi = 0.8.\nThe probability that he would win all 6 games is approximately 26%.\n\nf(y=6|\\pi=0.8) = {6 \\choose 6} 0.8^6 (1-0.8)^{6-6}, \n\ndbinom(6, 6, 0.8)\n\n[1] 0.262144"
  },
  {
    "objectID": "files/lectures/04-2-thinking-bayesian.html#binomial-data-model-2",
    "href": "files/lectures/04-2-thinking-bayesian.html#binomial-data-model-2",
    "title": "Thinking Like a Bayesian",
    "section": "Binomial Data Model",
    "text": "Binomial Data Model\n\nLet’s assume \\pi = 0.8.\nThe probability that he would win none of the games is approximately 0%.\n\nf(y=0|\\pi=0.8) = {6 \\choose 0} 0.8^0 (1-0.8)^{6-0}, \n\ndbinom(0, 6, 0.8)\n\n[1] 6.4e-05"
  },
  {
    "objectID": "files/lectures/04-2-thinking-bayesian.html#binomial-data-model-3",
    "href": "files/lectures/04-2-thinking-bayesian.html#binomial-data-model-3",
    "title": "Thinking Like a Bayesian",
    "section": "Binomial Data Model",
    "text": "Binomial Data Model\n\nWe want to reproduce Figure 2.5 from the Bayes Rules! textbook (from Section 2.3.2).\n\n\n\nEach group will complete the graph for a specified value of \\pi.\n\nCampus: \\pi=0.2\nZoom 1: \\pi=0.5\nZoom 2: \\pi=0.8"
  },
  {
    "objectID": "files/lectures/04-2-thinking-bayesian.html#binomial-data-model-4",
    "href": "files/lectures/04-2-thinking-bayesian.html#binomial-data-model-4",
    "title": "Thinking Like a Bayesian",
    "section": "Binomial Data Model",
    "text": "Binomial Data Model\n\nNote that the Binomial gives us the theoretical model of the data we might observe.\n\nKasparov only won one of the six games against Deep Blue in 1997 (Y=1).\n\nNext step: how compatible this particular data is with the various possible \\pi?\n\nWhat is the likelihood of Kasparov winning Y=1 game under each possible \\pi?\n\nRecall, f(y|\\pi) = L(\\pi|Y=y). When Y=1,\n\n\n\\begin{align*}\nL(\\pi | y = 1) &= f(y=1|\\pi) \\\\\n&= {6 \\choose 1} \\pi^1 (1-\\pi)^{6-1} \\\\\n&= 6\\pi(1-\\pi)^5\n\\end{align*}\n\n\nNote that we do not expect all likelihoods to sum to 1."
  },
  {
    "objectID": "files/lectures/04-2-thinking-bayesian.html#binomial-data-model-5",
    "href": "files/lectures/04-2-thinking-bayesian.html#binomial-data-model-5",
    "title": "Thinking Like a Bayesian",
    "section": "Binomial Data Model",
    "text": "Binomial Data Model\n\nUse your results from earlier to tell me the resulting likelihood values.\n\n\n\n\n\n\n\n\n\n\n\\pi\n0.2\n0.5\n0.8\n\n\n\n\nL(\\pi|y=1)"
  },
  {
    "objectID": "files/lectures/04-2-thinking-bayesian.html#binomial-data-model-6",
    "href": "files/lectures/04-2-thinking-bayesian.html#binomial-data-model-6",
    "title": "Thinking Like a Bayesian",
    "section": "Binomial Data Model",
    "text": "Binomial Data Model\n\nUse your results from earlier to tell me the resulting likelihood values.\n\n\n\n\n\\pi\n0.2\n0.5\n0.8\n\n\n\n\nL(\\pi|y=1)\n0.3932\n0.0938\n0.0015\n\n\n\n\nAs we can see, the likelihoods do not sum to 1."
  },
  {
    "objectID": "files/lectures/04-2-thinking-bayesian.html#normalizing-constant",
    "href": "files/lectures/04-2-thinking-bayesian.html#normalizing-constant",
    "title": "Thinking Like a Bayesian",
    "section": "Normalizing Constant",
    "text": "Normalizing Constant\n\nBayes’ Rule requires three pieces of information:\n\nPrior\nLikelihood\nNormalizing constant\n\nNormalizing constant: ensures that the sum of all probabilities is equal to 1.\n\nIt can be a scalar or a function.\nEvery probability distribution that does not sum to 1 will have a normalizing constant."
  },
  {
    "objectID": "files/lectures/04-2-thinking-bayesian.html#normalizing-constant-1",
    "href": "files/lectures/04-2-thinking-bayesian.html#normalizing-constant-1",
    "title": "Thinking Like a Bayesian",
    "section": "Normalizing Constant",
    "text": "Normalizing Constant\n\nWe now must determine the total probability that Kasparov would win Y=1 games across all possible win probabilities \\pi, f(y=1).\n\n\n\\begin{align*}\nf(y=1) =& \\sum_{\\pi} L(\\pi |y=1)f(\\pi) \\\\\n=& L(\\pi=0.2|y=1)f(\\pi=0.2) + L(\\pi=0.5|y=1)f(\\pi=0.5) + \\\\\n& L(\\pi=0.8|y=1)f(\\pi=0.8)\n\\end{align*}\n\n\nWork with your group to find the normalizing constant."
  },
  {
    "objectID": "files/lectures/04-2-thinking-bayesian.html#normalizing-constant-2",
    "href": "files/lectures/04-2-thinking-bayesian.html#normalizing-constant-2",
    "title": "Thinking Like a Bayesian",
    "section": "Normalizing Constant",
    "text": "Normalizing Constant\n\nWe now must determine the total probability that Kasparov would win Y=1 games across all possible win probabilities \\pi, f(y=1).\n\n\n\\begin{align*}\nf(y=1) =& \\sum_{\\pi} L(\\pi |y=1)f(\\pi) \\\\\n=& L(\\pi=0.2|y=1)f(\\pi=0.2) + L(\\pi=0.5|y=1)f(\\pi=0.5) + \\\\\n& L(\\pi=0.8|y=1)f(\\pi=0.8) \\\\\n\\approx& 0.3932 \\cdot 0.10 + 0.0938 \\cdot 0.25 + 0.0015 \\cdot 0.65 \\\\\n\\approx& 0.0637\n\\end{align*}\n\n\nAcross all possible values of \\pi, there is about a 6% chance that Kasparov would have won only one game."
  },
  {
    "objectID": "files/lectures/04-2-thinking-bayesian.html#posterior-probability-model",
    "href": "files/lectures/04-2-thinking-bayesian.html#posterior-probability-model",
    "title": "Thinking Like a Bayesian",
    "section": "Posterior Probability Model",
    "text": "Posterior Probability Model\n\nNow recall,\n\n\\text{posterior} = \\frac{\\text{prior} \\times \\text{likelihood}}{\\text{normalizing constant}}\n\nIn our example, where y = 1,\n\nf(\\pi | y=1) = \\frac{f(\\pi) L(\\pi | y = 1)}{f(y=1)} \\  \\text{for} \\ \\pi \\in \\{ 0.2, 0.5, 0.8\\}\n\nWork with your group to find the posterior probabilities.\n\nYou will have one posterior probability for each value of \\pi."
  },
  {
    "objectID": "files/lectures/04-2-thinking-bayesian.html#posterior-probability-model-1",
    "href": "files/lectures/04-2-thinking-bayesian.html#posterior-probability-model-1",
    "title": "Thinking Like a Bayesian",
    "section": "Posterior Probability Model",
    "text": "Posterior Probability Model\n\nNote!! We do not have to calculate the normalizing constant!\nWe can note that f(Y=y) = 1/c.\nThen, we say that\n\n\n\\begin{align*}\nf(\\pi | y) &= \\frac{f(\\pi) L(\\pi|y)}{f(y)} \\\\\n& \\propto  f(\\pi) L(\\pi|y) \\\\\n\\\\\n\\text{posterior} &\\propto \\text{prior} \\cdot \\text{likelihood}\n\\end{align*}"
  },
  {
    "objectID": "files/lectures/04-2-thinking-bayesian.html#wrap-up",
    "href": "files/lectures/04-2-thinking-bayesian.html#wrap-up",
    "title": "Thinking Like a Bayesian",
    "section": "Wrap Up",
    "text": "Wrap Up\n\nToday we learned how to, in general, approach Bayesian analysis.\nNext week, we will formalize what we observed today and learn about the conjugate families.\n\nBeta-binomial\nGamma-Poisson\nNormal Normal"
  },
  {
    "objectID": "files/lectures/04-2-thinking-bayesian.html#homework-practice",
    "href": "files/lectures/04-2-thinking-bayesian.html#homework-practice",
    "title": "Thinking Like a Bayesian",
    "section": "Homework / Practice",
    "text": "Homework / Practice\n\nFrom the Bayes Rules! textbook:\n\n1.3\n1.4\n1.8\n2.4\n2.9"
  },
  {
    "objectID": "files/lectures/10-1-modeling-binary-logistic.html#introduction",
    "href": "files/lectures/10-1-modeling-binary-logistic.html#introduction",
    "title": "Logistic Regression",
    "section": "Introduction",
    "text": "Introduction\n\nLast week, we focused on linear regression appropriate for continuous outcomes that are approximately normally distributed.\n\n\n\\hat{y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_{i1} + \\hat{\\beta}_2 x_{i2} + ... + \\hat{\\beta}_k x_{ik}\n\n\nInterpretation of \\hat{\\beta}_0 (y-intercept): The expected value of y when all predictors are equal to zero.\nInterpretation of \\hat{\\beta}_i (slopes): The expected change in y for a one-unit increase in x_i, holding all other predictors constant."
  },
  {
    "objectID": "files/lectures/10-1-modeling-binary-logistic.html#introduction-1",
    "href": "files/lectures/10-1-modeling-binary-logistic.html#introduction-1",
    "title": "Logistic Regression",
    "section": "Introduction",
    "text": "Introduction\n\nToday we will cover binary logistic regression.\nCategorical data: Logistic\n\n\n\\ln\\left(\\frac{\\hat{\\pi}}{1-\\hat{\\pi}}\\right) = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_{i1} + \\hat{\\beta}_2 x_{i2} + ... + \\hat{\\beta}_k x_{ik}\n\n\nCount data: Poisson/negative binomial\n\n\n\\ln(\\hat{y}) = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_{i1} + \\hat{\\beta}_2 x_{i2} + ... + \\hat{\\beta}_k x_{ik}"
  },
  {
    "objectID": "files/lectures/10-1-modeling-binary-logistic.html#introduction-2",
    "href": "files/lectures/10-1-modeling-binary-logistic.html#introduction-2",
    "title": "Logistic Regression",
    "section": "Introduction",
    "text": "Introduction\n\nWithout going into statistical theory, the left hand side of the equation is affected by the model’s link function.\n\nNormal: Identity link\n\nWe are modeling y directly\n\nLogistic: Logit link\n\nWe are modeling the logit of \\pi\n\nPoisson/Negative Binomial: Log link\n\nWe are modeling the log of y"
  },
  {
    "objectID": "files/lectures/10-1-modeling-binary-logistic.html#introduction-3",
    "href": "files/lectures/10-1-modeling-binary-logistic.html#introduction-3",
    "title": "Logistic Regression",
    "section": "Introduction",
    "text": "Introduction\n\nThe left hand side dictates how we provide interpretations.\n\nIf we are not modeling y directly, we need to transform back to the original scale for interpretation purposes.\n\nThere are other link functions that can be used in generalized linear models, as long as the model converges.\n\nWhen done, it is to transform the left hand side to be more interpretable.\n\nRemember that if we have \\ln() on the left hand side, we can exponentiate to get back to the original scale.\n\nThis means that our interpretations will be in terms of multiplicative changes rather than additive changes.\nFor each increase in x, we will multiply the left hand side by some factor rather than add some amount to it."
  },
  {
    "objectID": "files/lectures/10-1-modeling-binary-logistic.html#example",
    "href": "files/lectures/10-1-modeling-binary-logistic.html#example",
    "title": "Logistic Regression",
    "section": "Example",
    "text": "Example\n\nMickey Mouse and his friends have been organizing a series of rescue missions around the clubhouse. Each mission is led by one of four possible leaders (Mickey, Minnie, Donald, or Goofy) and the goal is to determine which factors make a mission most likely to succeed.\nFor each mission, the team records:\n\nmission_success: whether the rescue mission succeeded (1) or failed (0),\ncharacter: the leader of the mission (Mickey, Minnie, Donald, or Goofy), and\nstealth_score: a numerical measure (0–10) of how quietly and carefully the mission was executed; higher values indicate more stealthy, well-coordinated missions.\n\nLet’s now set up a logistic regression to predict mission success as a function of who led the mission, the stealth score, and the interaction between the two. In R, our model will be\n\n\n\\text{mission\\_success} \\sim \\text{character} + \\text{stealth\\_score} + \\text{character}:\\text{stealth\\_score}"
  },
  {
    "objectID": "files/lectures/10-1-modeling-binary-logistic.html#example-1",
    "href": "files/lectures/10-1-modeling-binary-logistic.html#example-1",
    "title": "Logistic Regression",
    "section": "Example",
    "text": "Example\n\nRunning the model,\n\n\nmickey_model &lt;- stan_glm(mission_success ~ character + stealth_score + character:stealth_score,\n                         data = mickey, \n                         family = binomial,\n                         prior_intercept = normal(0, 1, autoscale = TRUE),\n                         prior = normal(0, 1, autoscale = TRUE),\n                         chains = 4, iter = 5000*2, seed = 8716,\n                         prior_PD = FALSE)\n\n\nSAMPLING FOR MODEL 'bernoulli' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 3.1e-05 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.31 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 1: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 1: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 1: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 1: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 1: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 1: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 1: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 1: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 1: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 1: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 1: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.383 seconds (Warm-up)\nChain 1:                0.399 seconds (Sampling)\nChain 1:                0.782 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'bernoulli' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 9e-06 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.09 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 2: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 2: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 2: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 2: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 2: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 2: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 2: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 2: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 2: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 2: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 2: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.403 seconds (Warm-up)\nChain 2:                0.42 seconds (Sampling)\nChain 2:                0.823 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'bernoulli' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 7e-06 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.07 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 3: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 3: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 3: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 3: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 3: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 3: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 3: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 3: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 3: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 3: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 3: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 0.395 seconds (Warm-up)\nChain 3:                0.425 seconds (Sampling)\nChain 3:                0.82 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'bernoulli' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 6e-06 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.06 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 4: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 4: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 4: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 4: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 4: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 4: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 4: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 4: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 4: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 4: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 4: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 0.383 seconds (Warm-up)\nChain 4:                0.351 seconds (Sampling)\nChain 4:                0.734 seconds (Total)\nChain 4:"
  },
  {
    "objectID": "files/lectures/10-1-modeling-binary-logistic.html#example-2",
    "href": "files/lectures/10-1-modeling-binary-logistic.html#example-2",
    "title": "Logistic Regression",
    "section": "Example",
    "text": "Example\n\nExamining the results,\n\n\ntidy(mickey_model, conf.int = TRUE)"
  },
  {
    "objectID": "files/lectures/10-1-modeling-binary-logistic.html#example-3",
    "href": "files/lectures/10-1-modeling-binary-logistic.html#example-3",
    "title": "Logistic Regression",
    "section": "Example",
    "text": "Example\n\nThis results in the model,\n\n\n\\begin{align*}\n\\ln\\left(\\frac{\\hat{\\pi}}{1-\\hat{\\pi}}\\right) =& \\  -0.193  \\\\\n& \\ -1.005 \\text{ Donald} + 0.228 \\text{ Goofy} + 1.917 \\text{ Minnie} + 0.296 \\text{ stealth} \\  \\\\\n& \\ + 0.185 \\text{ Donald} \\times \\text{stealth} -0.198 \\text{ Goofy}\\times \\text{stealth} \\\\\n& \\ \\ \\ \\ \\ \\ +  0.403 \\text{ Minnie} \\times \\text{stealth}\n\\end{align*}"
  },
  {
    "objectID": "files/lectures/10-1-modeling-binary-logistic.html#example-4",
    "href": "files/lectures/10-1-modeling-binary-logistic.html#example-4",
    "title": "Logistic Regression",
    "section": "Example",
    "text": "Example\n\nInterpreting the coefficients:\n\nIntercept (\\hat{\\beta}_0): The log-odds of mission success when Mickey leads (reference category) and the stealth score is 0.\nCharacter coefficients (\\hat{\\beta}_1, \\hat{\\beta}_2, \\hat{\\beta}_3): The change in log-odds of mission success when Minnie, Donald, or Goofy lead, respectively, compared to Mickey, holding stealth score constant.\nStealth score coefficient (\\hat{\\beta}_4): The change in log-odds of mission success for a one-unit increase in stealth score when Mickey leads.\nInteraction coefficients (\\hat{\\beta}_5, \\hat{\\beta}_6, \\hat{\\beta}_7): The additional change in log-odds of mission success for a one-unit increase in stealth score when Minnie, Donald, or Goofy lead, respectively, compared to Mickey."
  },
  {
    "objectID": "files/lectures/10-1-modeling-binary-logistic.html#example-5",
    "href": "files/lectures/10-1-modeling-binary-logistic.html#example-5",
    "title": "Logistic Regression",
    "section": "Example",
    "text": "Example\n\nWhat if we want to separate into individual models for each friend?"
  },
  {
    "objectID": "files/lectures/10-1-modeling-binary-logistic.html#example-6",
    "href": "files/lectures/10-1-modeling-binary-logistic.html#example-6",
    "title": "Logistic Regression",
    "section": "Example",
    "text": "Example\n\nDonald’s model:"
  },
  {
    "objectID": "files/lectures/10-1-modeling-binary-logistic.html#example-7",
    "href": "files/lectures/10-1-modeling-binary-logistic.html#example-7",
    "title": "Logistic Regression",
    "section": "Example",
    "text": "Example\n\nGoofy’s model:"
  },
  {
    "objectID": "files/lectures/10-1-modeling-binary-logistic.html#example-8",
    "href": "files/lectures/10-1-modeling-binary-logistic.html#example-8",
    "title": "Logistic Regression",
    "section": "Example",
    "text": "Example\n\nMinnie’s model:"
  },
  {
    "objectID": "files/lectures/10-1-modeling-binary-logistic.html#example-9",
    "href": "files/lectures/10-1-modeling-binary-logistic.html#example-9",
    "title": "Logistic Regression",
    "section": "Example",
    "text": "Example\n\nMickey’s model:"
  },
  {
    "objectID": "files/lectures/10-1-modeling-binary-logistic.html#odds-ratios",
    "href": "files/lectures/10-1-modeling-binary-logistic.html#odds-ratios",
    "title": "Logistic Regression",
    "section": "Odds Ratios",
    "text": "Odds Ratios\n\nRecall that we do not want to interpret in terms of the log odds (i.e., in terms of)\n\n\n\\ln\\left(\\frac{\\hat{\\pi}}{1-\\hat{\\pi}}\\right)\n\n\nInstead, we want to interpret in terms of the odds themselves (i.e., in terms of)\n\n\n\\frac{\\hat{\\pi}}{1-\\hat{\\pi}}"
  },
  {
    "objectID": "files/lectures/10-1-modeling-binary-logistic.html#odds-ratios-1",
    "href": "files/lectures/10-1-modeling-binary-logistic.html#odds-ratios-1",
    "title": "Logistic Regression",
    "section": "Odds Ratios",
    "text": "Odds Ratios\n\nTo convert from log-odds to odds, we exponentiate the coefficients.\n\n\n\\text{odds ratio}_i = \\exp\\{ \\hat{\\beta}_i \\}\n\n\nNote (1), if we are reporting odds ratios rather than \\hat{\\beta}_i, then:\n\nAn odds ratio of 1 \\to no effect.\nAn odds ratio greater than 1 \\to increase in odds.\nAn odds ratio less than 1 \\to decrease in odds.\n\nNote (2), if we are reporting odds ratios rather than \\hat{\\beta}_i, then we also want to report the intervals for the odds ratios and not the intervals for \\hat{\\beta}_i."
  },
  {
    "objectID": "files/lectures/10-1-modeling-binary-logistic.html#example-10",
    "href": "files/lectures/10-1-modeling-binary-logistic.html#example-10",
    "title": "Logistic Regression",
    "section": "Example",
    "text": "Example\n\nWe can request OR and their corresponding CI this out of tidy(),\n\n\ntidy(mickey_model, conf.int = TRUE, exponentiate = TRUE)"
  },
  {
    "objectID": "files/lectures/10-1-modeling-binary-logistic.html#odds-ratios-2",
    "href": "files/lectures/10-1-modeling-binary-logistic.html#odds-ratios-2",
    "title": "Logistic Regression",
    "section": "Odds Ratios",
    "text": "Odds Ratios\n\nBut… we have an interaction in our model?\nWe can take the same approach as finding the individual slopes!\n\nTake the bounds and add/subtract as necessary.\n\nRemember that we need to add on the original scale, not the converted odds ratios."
  },
  {
    "objectID": "files/lectures/10-1-modeling-binary-logistic.html#example-11",
    "href": "files/lectures/10-1-modeling-binary-logistic.html#example-11",
    "title": "Logistic Regression",
    "section": "Example",
    "text": "Example\n\nLet’s look at our model results, but only look at the CI results for stealth score and the interactions.\n\n\ntidy(mickey_model, conf.int = TRUE) %&gt;% \n  select(term, conf.low, conf.high) %&gt;%\n  filter(str_detect(term, \"stealth\"))"
  },
  {
    "objectID": "files/lectures/10-1-modeling-binary-logistic.html#example-12",
    "href": "files/lectures/10-1-modeling-binary-logistic.html#example-12",
    "title": "Logistic Regression",
    "section": "Example",
    "text": "Example\n\nFor Mickey,\n\nLB: .000111\nUB: 0.6076\n\nFor Donald,\n\nLB: .000111 - 0.2135 = -0.2134\nUB: 0.6076 + 0.5965 = 1.2041\n\nFor Goofy,\n\nLB: .000111 - 0.5299 = -0.5298\nUB: 0.6076 + 0.1372 = 0.7448\n\nFor Minnie,\n\nLB: .000111 - 0.2653 = -0.2652\nUB: 0.6076 + 0.5519 = 1.1595"
  },
  {
    "objectID": "files/lectures/10-1-modeling-binary-logistic.html#example-13",
    "href": "files/lectures/10-1-modeling-binary-logistic.html#example-13",
    "title": "Logistic Regression",
    "section": "Example",
    "text": "Example\n\nComparing side by side with CI for OR,\n\n\n\n\nFriend\nCI for \\beta\nCI for OR\n\n\n\n\nMickey\n(0.000111, 0.6076)\n(1.00, 1.84)\n\n\nDonald\n(-0.2134, 1.204)\n(0.81, 3.33)\n\n\nGoofy\n(-0.5298, 0.7448)\n(0.59, 2.11)\n\n\nMinnie\n(-0.2652, 1.1595)\n(0.77, 3.19)"
  },
  {
    "objectID": "files/lectures/10-1-modeling-binary-logistic.html#example-14",
    "href": "files/lectures/10-1-modeling-binary-logistic.html#example-14",
    "title": "Logistic Regression",
    "section": "Example",
    "text": "Example\n\nHow would I report the results in a table?\n\n\n\n\nFriend\nOR (95% CI for OR)\n\n\n\n\nMickey\n1.23 (1.00, 1.84)\n\n\nDonald\n1.62 (0.81, 3.33)\n\n\nGoofy\n1.11 (0.59, 2.11)\n\n\nMinnie\n1.53 (0.77, 3.19)\n\n\n\n\nFor all characters, as stealth score increases, the odds of mission success increase.\nThe strength of this effect varies by character, but the 95% CIs for all characters include 1, indicating a null effect."
  },
  {
    "objectID": "files/lectures/10-1-modeling-binary-logistic.html#visualizing-the-model",
    "href": "files/lectures/10-1-modeling-binary-logistic.html#visualizing-the-model",
    "title": "Logistic Regression",
    "section": "Visualizing the Model",
    "text": "Visualizing the Model\n\nWe can put together a data visualization to help us understand what’s going on in our model.\nIn this example, we will have strength on the x-axis and define lines by the friend.\n\ni.e., I am graphing the individual models we saw earlier.\n\nFirst, we have to find predicted values,\n\n\nmickey &lt;- mickey %&gt;%\n  mutate(p_mickey = exp(-0.193 + 0.296*stealth_score) / (1+exp(-0.193 + 0.296*stealth_score)),\n         p_minnie = exp(1.724 + 0.426*stealth_score) / (1+exp(1.724 + 0.426*stealth_score)),\n         p_donald = exp(-1.198 + 0.481*stealth_score) / (1+exp(-1.198 + 0.481*stealth_score)),\n         p_goofy  = exp(0.035 + 0.101*stealth_score) / (1+exp(0.035 + 0.101*stealth_score)))"
  },
  {
    "objectID": "files/lectures/10-1-modeling-binary-logistic.html#example-15",
    "href": "files/lectures/10-1-modeling-binary-logistic.html#example-15",
    "title": "Logistic Regression",
    "section": "Example",
    "text": "Example\n\nThen, we can graph using these predicted values\n\n\nmickey %&gt;% ggplot(aes(x = stealth_score, y = mission_success)) +\n  geom_point(size = 3) +\n  geom_line(aes(y = p_mickey, color = \"black\")) +\n  geom_line(aes(y = p_minnie, color = \"red\")) +\n  geom_line(aes(y = p_donald, color = \"blue\")) +\n  geom_line(aes(y = p_goofy, color = \"green\")) +\n  labs(x = \"Stealth Score\",\n       y = \"Probability of Mission Success\",\n       color = \"Leader\") +\n  scale_color_manual(labels = c(\"Mickey\", \"Minnie\", \"Donald\", \"Goofy\"),\n                     values = c(\"black\", \"red\", \"blue\", \"green\")) +\n  theme_bw()"
  },
  {
    "objectID": "files/lectures/10-1-modeling-binary-logistic.html#example-16",
    "href": "files/lectures/10-1-modeling-binary-logistic.html#example-16",
    "title": "Logistic Regression",
    "section": "Example",
    "text": "Example"
  },
  {
    "objectID": "files/lectures/10-1-modeling-binary-logistic.html#wrap-up",
    "href": "files/lectures/10-1-modeling-binary-logistic.html#wrap-up",
    "title": "Logistic Regression",
    "section": "Wrap Up",
    "text": "Wrap Up\n\nBinary logistic regression is used for binary outcomes (e.g., success/failure).\nRecall we also have\n\nOrdinal logistic regression \\to for ordered categorical outcomes (e.g., low/medium/high).\nMultinomial logistic regression \\to for unordered categorical outcomes (e.g., red/blue/green).\n\nWednesday:\n\nPoisson and Negative Binomial regression for count data.\nHow to “read” subject-matter articles to extract information for analysis."
  },
  {
    "objectID": "files/lectures/10-1-modeling-binary-logistic.html#practice-homework",
    "href": "files/lectures/10-1-modeling-binary-logistic.html#practice-homework",
    "title": "Logistic Regression",
    "section": "Practice / Homework",
    "text": "Practice / Homework\n\nFrom the Bayes Rules! textbook:\n\n13.6, 13.7\n13.10, 13.11"
  },
  {
    "objectID": "files/lectures/01-1-probability-theory.html#introduction",
    "href": "files/lectures/01-1-probability-theory.html#introduction",
    "title": "Overview of Probability Theory",
    "section": "Introduction",
    "text": "Introduction\n\nToday we will review very basic probability rules to help us build towards analysis under the Bayeisan framework.\nWe will discuss\n\nBasic terminology\nBasic properties\nTypes of probabilities\nTypes of events"
  },
  {
    "objectID": "files/lectures/01-1-probability-theory.html#terminology-for-probability",
    "href": "files/lectures/01-1-probability-theory.html#terminology-for-probability",
    "title": "Overview of Probability Theory",
    "section": "Terminology for Probability",
    "text": "Terminology for Probability\n\nExperiment: A process that results in one and only one of many possible observations.\nSimple outcomes: The possible results of our experiment.\nSample space: Collection of possible outcomes of the experiment.\nEvent: A collection of one or more of the outcomes of the experiment.\nExample: rolling a die once.\n\nOutcome: the result of the die.\nSample space: {1, 2, 3, 4, 5, 6}\nEvents: rolling an odd; rolling a multiple of 3; rolling a 3 or better."
  },
  {
    "objectID": "files/lectures/01-1-probability-theory.html#properties-of-probability",
    "href": "files/lectures/01-1-probability-theory.html#properties-of-probability",
    "title": "Overview of Probability Theory",
    "section": "Properties of Probability",
    "text": "Properties of Probability\n\nThere are two properties of probability that we must keep at the forefront of our mind.\nFirst, probability always falls between 0 and 1. Mathematically,\n\n0 \\le P[E_i] \\le 1\n\nWhat does p=0 imply?\nWhat does p=0.5 imply?\nWhat does p=1 imply?"
  },
  {
    "objectID": "files/lectures/01-1-probability-theory.html#properties-of-probability-1",
    "href": "files/lectures/01-1-probability-theory.html#properties-of-probability-1",
    "title": "Overview of Probability Theory",
    "section": "Properties of Probability",
    "text": "Properties of Probability\n\nThere are two properties of probability that we must keep at the forefront of our mind.\nSecond, the sum of all simple events for an experiment is always 1. Mathematically,\n\n\\sum_{i=1}^n P[E_i] = P[E_1] + ... + P[E_n] = 1\n\nIf there are 2 events and we know P[E_1]=0.7, what is P[E_2]? \nIf there are 4 events and we know P[E_1]=P[E_2]=0.1, P[E_3]=0.6, what is P[E_4]?"
  },
  {
    "objectID": "files/lectures/01-1-probability-theory.html#assigning-probabilities",
    "href": "files/lectures/01-1-probability-theory.html#assigning-probabilities",
    "title": "Overview of Probability Theory",
    "section": "Assigning Probabilities",
    "text": "Assigning Probabilities\n\nHow do we assign probabilities?\n\nSubjective probability\nClassicial probability rule\nRelative frequency"
  },
  {
    "objectID": "files/lectures/01-1-probability-theory.html#subjective-probability",
    "href": "files/lectures/01-1-probability-theory.html#subjective-probability",
    "title": "Overview of Probability Theory",
    "section": "Subjective Probability",
    "text": "Subjective Probability\n\nSubjective probability is the probability assigned to an event based on subjective judgement, experience, information, and belief.\nExamples:\n\nP[UWF wins national championship]\nP[tomato plant eaten by hornworms]\nP[A in this course]"
  },
  {
    "objectID": "files/lectures/01-1-probability-theory.html#classical-probability",
    "href": "files/lectures/01-1-probability-theory.html#classical-probability",
    "title": "Overview of Probability Theory",
    "section": "Classical Probability",
    "text": "Classical Probability\n\nLet A be an event for an experiment with equally likely outcomes,\n\nP[A] = \\frac{\\text{Number of outcomes favorable to $A$}}{\\text{Total number of outcomes for the experiment}}\n\nExamples:\n\nP[2 heads on 3 coin tosses]\nP[at least 2 heads on 4 coin tosses]\nP[even when rolling die]"
  },
  {
    "objectID": "files/lectures/01-1-probability-theory.html#relative-frequency",
    "href": "files/lectures/01-1-probability-theory.html#relative-frequency",
    "title": "Overview of Probability Theory",
    "section": "Relative Frequency",
    "text": "Relative Frequency\n\nIf an experiment is repeated n times and an event A is observed f times, then\n\nP[A] = \\frac{f}{n} = \\frac{\\text{Frequency of $A$}}{\\text{Sample size}}\n\nExample:\n\nP[car is a lemon] given 10/500 sampled cars from a factory are lemons.\nP[person is a homeowner] given 730/1000 sampled individuals own a home."
  },
  {
    "objectID": "files/lectures/01-1-probability-theory.html#contingency-tables",
    "href": "files/lectures/01-1-probability-theory.html#contingency-tables",
    "title": "Overview of Probability Theory",
    "section": "Contingency Tables",
    "text": "Contingency Tables\n\nSuppose 100 employees at Target were asked whether they are in favor of or against extending store hours during the holiday season.\n\n\n\n\n\n\n\n\nDepartment\n\n\nIn Favor\n\n\nAgainst\n\n\nTotal\n\n\n\n\nElectronics\n\n\n12\n\n\n8\n\n\n20\n\n\n\n\nClothing\n\n\n18\n\n\n12\n\n\n30\n\n\n\n\nGrocery\n\n\n25\n\n\n15\n\n\n40\n\n\n\n\nCustomer Service\n\n\n5\n\n\n5\n\n\n10\n\n\n\n\nTotal\n\n\n60\n\n\n40\n\n\n100"
  },
  {
    "objectID": "files/lectures/01-1-probability-theory.html#marginal-probability",
    "href": "files/lectures/01-1-probability-theory.html#marginal-probability",
    "title": "Overview of Probability Theory",
    "section": "Marginal Probability",
    "text": "Marginal Probability\n\nA marginal probability is the probability of a single event occurring without considering any other variables.\n\nIn a contingency table, marginal probabilities are found outside the body of the table.\n\nIt tells us the likelihood of one category happening overall, regardless of how it combines (or interacts) with other categories.\nIn our Target example:\n\nWhat is the probability that a randomly selected employee is in favor?\nWhat is the probability that randomly selected employee works in the Grocery department?"
  },
  {
    "objectID": "files/lectures/01-1-probability-theory.html#marginal-probability-1",
    "href": "files/lectures/01-1-probability-theory.html#marginal-probability-1",
    "title": "Overview of Probability Theory",
    "section": "Marginal Probability",
    "text": "Marginal Probability\n\nRecall,\n\n\n\n\n\n\n\n\nDepartment\n\n\nIn Favor\n\n\nAgainst\n\n\nTotal\n\n\n\n\nElectronics\n\n\n12\n\n\n8\n\n\n20\n\n\n\n\nClothing\n\n\n18\n\n\n12\n\n\n30\n\n\n\n\nGrocery\n\n\n25\n\n\n15\n\n\n40\n\n\n\n\nCustomer Service\n\n\n5\n\n\n5\n\n\n10\n\n\n\n\nTotal\n\n\n60\n\n\n40\n\n\n100\n\n\n\n\n\n\n\nWhat is the probability that a randomly selected employee is in favor?\nWhat is the probability that randomly selected employee works in the Grocery department?"
  },
  {
    "objectID": "files/lectures/01-1-probability-theory.html#joint-probability",
    "href": "files/lectures/01-1-probability-theory.html#joint-probability",
    "title": "Overview of Probability Theory",
    "section": "Joint Probability",
    "text": "Joint Probability\n\nThe joint probability is the probability that two events happen at the same time.\n\nIn a contingency table, joint probabilities are found inside the body of the table.\n\nIt tells us the likelihood that a randomly selected observation falls into both categories simultaneously.\nIn our Target example:\n\nWhat is the probability that an employee is in the Grocery department and in favor of extended hours?\nWhat is the probability that an employee is in Electronics and against extended hours?"
  },
  {
    "objectID": "files/lectures/01-1-probability-theory.html#joint-probability-1",
    "href": "files/lectures/01-1-probability-theory.html#joint-probability-1",
    "title": "Overview of Probability Theory",
    "section": "Joint Probability",
    "text": "Joint Probability\n\nRecall,\n\n\n\n\n\n\n\n\nDepartment\n\n\nIn Favor\n\n\nAgainst\n\n\nTotal\n\n\n\n\nElectronics\n\n\n12\n\n\n8\n\n\n20\n\n\n\n\nClothing\n\n\n18\n\n\n12\n\n\n30\n\n\n\n\nGrocery\n\n\n25\n\n\n15\n\n\n40\n\n\n\n\nCustomer Service\n\n\n5\n\n\n5\n\n\n10\n\n\n\n\nTotal\n\n\n60\n\n\n40\n\n\n100\n\n\n\n\n\n\n\nWhat is the probability that an employee is in the Grocery department and in favor of extended hours?\nWhat is the probability that an employee is in Electronics and against extended hours?"
  },
  {
    "objectID": "files/lectures/01-1-probability-theory.html#conditional-probability",
    "href": "files/lectures/01-1-probability-theory.html#conditional-probability",
    "title": "Overview of Probability Theory",
    "section": "Conditional Probability",
    "text": "Conditional Probability\n\nConditional probability is the probability that one event occurs given that we already know another event has occurred.\n\n“What is the probability of Event A if we know Event B is true?”\nIn a contingency table, conditional probabilities are found by limiting yourself to a specific row or column of the table, then finding the corresponding probability.\n\nIn our Target example,\n\nWhat is the probability that an employee is in favor of extended hours given that they work in the Grocery department?\nWhat is the probability that an employee works in the Electronics department given that they are against extended hours?"
  },
  {
    "objectID": "files/lectures/01-1-probability-theory.html#conditional-probability-1",
    "href": "files/lectures/01-1-probability-theory.html#conditional-probability-1",
    "title": "Overview of Probability Theory",
    "section": "Conditional Probability",
    "text": "Conditional Probability\n\nRecall,\n\n\n\n\n\n\n\n\nDepartment\n\n\nIn Favor\n\n\nAgainst\n\n\nTotal\n\n\n\n\nElectronics\n\n\n12\n\n\n8\n\n\n20\n\n\n\n\nClothing\n\n\n18\n\n\n12\n\n\n30\n\n\n\n\nGrocery\n\n\n25\n\n\n15\n\n\n40\n\n\n\n\nCustomer Service\n\n\n5\n\n\n5\n\n\n10\n\n\n\n\nTotal\n\n\n60\n\n\n40\n\n\n100\n\n\n\n\n\n\n\nWhat is the probability that an employee is in favor of extended hours given that they work in the Grocery department?\nWhat is the probability that an employee works in the Electronics department given that they are against extended hours?"
  },
  {
    "objectID": "files/lectures/01-1-probability-theory.html#mutually-exclusive-events",
    "href": "files/lectures/01-1-probability-theory.html#mutually-exclusive-events",
    "title": "Overview of Probability Theory",
    "section": "Mutually Exclusive Events",
    "text": "Mutually Exclusive Events\n\nEvents that cannot occur together are mutually exclusive or disjoint.\nConsider rolling a single die:\n\nA = an even number = {2, 4, 6}\nB = an odd number = {1, 3, 5}\nC = a number less than 5 = {1, 2, 3, 4}\n\nAre events A and B mutually exclusive?\nAre events A and C mutually exclusive?"
  },
  {
    "objectID": "files/lectures/01-1-probability-theory.html#mutually-exclusive-events-1",
    "href": "files/lectures/01-1-probability-theory.html#mutually-exclusive-events-1",
    "title": "Overview of Probability Theory",
    "section": "Mutually Exclusive Events",
    "text": "Mutually Exclusive Events\n\nConsider rolling a single die:\n\nA = an even number = {2, 4, 6}\nB = an odd number = {1, 3, 5}\nC = a number less than 5 = {1, 2, 3, 4}\n\nAre events A and B mutually exclusive?"
  },
  {
    "objectID": "files/lectures/01-1-probability-theory.html#mutually-exclusive-events-2",
    "href": "files/lectures/01-1-probability-theory.html#mutually-exclusive-events-2",
    "title": "Overview of Probability Theory",
    "section": "Mutually Exclusive Events",
    "text": "Mutually Exclusive Events\n\nConsider rolling a single die:\n\nA = an even number = {2, 4, 6}\nB = an odd number = {1, 3, 5}\nC = a number less than 5 = {1, 2, 3, 4}\n\nAre events A and C mutually exclusive?"
  },
  {
    "objectID": "files/lectures/01-1-probability-theory.html#independent-events",
    "href": "files/lectures/01-1-probability-theory.html#independent-events",
    "title": "Overview of Probability Theory",
    "section": "Independent Events",
    "text": "Independent Events\n\nTwo events are said to be independent if the occurrence of one event does not affect the probability of the occurrence of the other event.\nMathematically,\n\n P[A|B] = P[A] \\text{ or } P[B|A] = P[B]\n\nIn our Target example, is department independent of being in favor of extended hours?\n\nWe are being asked to examine P[department|favor] or P[favor|department]."
  },
  {
    "objectID": "files/lectures/01-1-probability-theory.html#independent-events-1",
    "href": "files/lectures/01-1-probability-theory.html#independent-events-1",
    "title": "Overview of Probability Theory",
    "section": "Independent Events",
    "text": "Independent Events\n\nRecall,\n\n\n\n\n\n\n\n\nDepartment\n\n\nIn Favor\n\n\nAgainst\n\n\nTotal\n\n\n\n\nElectronics\n\n\n12\n\n\n8\n\n\n20\n\n\n\n\nClothing\n\n\n18\n\n\n12\n\n\n30\n\n\n\n\nGrocery\n\n\n25\n\n\n15\n\n\n40\n\n\n\n\nCustomer Service\n\n\n5\n\n\n5\n\n\n10\n\n\n\n\nTotal\n\n\n60\n\n\n40\n\n\n100\n\n\n\n\n\n\n\nWe need to examine P[department|favor] or P[favor|department]. We say that they are independent if\n\nP[department|favor] = P[department]\nP[favor|department] = P[favor]"
  },
  {
    "objectID": "files/lectures/01-1-probability-theory.html#complementary-events",
    "href": "files/lectures/01-1-probability-theory.html#complementary-events",
    "title": "Overview of Probability Theory",
    "section": "Complementary Events",
    "text": "Complementary Events\n\nThe complement of A is the event that includes all the outcomes that are not in A.\n\n\n\n\n\nMathematically,\n\n\n\\begin{align*}\nP[A] &+ P[A^c] = 1 \\\\\nP[A] &= 1 - P[A^c] \\\\\nP[A^c] &= 1 - P[A]\n\\end{align*}"
  },
  {
    "objectID": "files/lectures/01-1-probability-theory.html#complementary-events-1",
    "href": "files/lectures/01-1-probability-theory.html#complementary-events-1",
    "title": "Overview of Probability Theory",
    "section": "Complementary Events",
    "text": "Complementary Events\n\nRecall,\n\n\n\n\n\n\n\n\nDepartment\n\n\nIn Favor\n\n\nAgainst\n\n\nTotal\n\n\n\n\nElectronics\n\n\n12\n\n\n8\n\n\n20\n\n\n\n\nClothing\n\n\n18\n\n\n12\n\n\n30\n\n\n\n\nGrocery\n\n\n25\n\n\n15\n\n\n40\n\n\n\n\nCustomer Service\n\n\n5\n\n\n5\n\n\n10\n\n\n\n\nTotal\n\n\n60\n\n\n40\n\n\n100\n\n\n\n\n\n\n\nFind P[Electronicsc].\nFind P[Electronicsc | Against]."
  },
  {
    "objectID": "files/lectures/01-1-probability-theory.html#wrap-up",
    "href": "files/lectures/01-1-probability-theory.html#wrap-up",
    "title": "Overview of Probability Theory",
    "section": "Wrap Up",
    "text": "Wrap Up\n\nToday we have reviewed probability basics.\n\nNote that this is no replacement for Advanced Probability :)\n\nWe just need to understand general concepts to move foward.\nNext week:\n\nMonday: no class meeting – instead, you will meet with your EES collaborator (at an agreed upon time/date).\n\nPairing document with contact information is linked on Discord. We now have a “EES project” channel.\n\nWednesday: probability distributions"
  },
  {
    "objectID": "files/lectures/09-1-modeling-normal.html#introduction",
    "href": "files/lectures/09-1-modeling-normal.html#introduction",
    "title": "Regression in the Bayesian Framework",
    "section": "Introduction",
    "text": "Introduction\n\nBefore today, we effectively were performing one-sample tests of means, proportions, and counts.\nNow, we will focus on incorporating just a single predictor into our analysis.\n\nOverall question: what is the relationship between Y (outcome) and X (predictor)?\n\nWe now switch to thinking about analysis in terms of regression."
  },
  {
    "objectID": "files/lectures/09-1-modeling-normal.html#working-example",
    "href": "files/lectures/09-1-modeling-normal.html#working-example",
    "title": "Regression in the Bayesian Framework",
    "section": "Working Example",
    "text": "Working Example\n\nCapital Bikeshare is a bike sharing service in the Washington, D.C. area. To best serve its registered members, the company must understand the demand for its service. We will analyze the number of rides taken on a random sample of n days, (Y_1, Y_2, ..., Y_n).\nBeacuse Y_i is a count variable, you might assume that ridership might need the Poisson distribution. However, past bike riding seasons have exhibited bell-shaped daily ridership with a variability in ridership that far exceeds the typical ridership.\n\n(i.e., the Poisson assumption of \\mu = \\sigma does not hold here)\n\nWe will instead assume that the number of rides varies normally around some typical ridership, \\mu, with standard deviation, \\sigma.\n\n Y_i|\\mu,\\sigma \\overset{\\text{ind}}{\\sim} N(\\mu, \\sigma^2)"
  },
  {
    "objectID": "files/lectures/09-1-modeling-normal.html#working-example-1",
    "href": "files/lectures/09-1-modeling-normal.html#working-example-1",
    "title": "Regression in the Bayesian Framework",
    "section": "Working Example",
    "text": "Working Example\n\nIn our example, Y is the number of rides and X is the temperature.\nOur specific goal will be to model the relationship between ridership and temperature:\n\nDoes ridership tend to increase on warmer days?\nIf so, by how much?\nHow strong is this relationship?\n\nIt is reasonable to assume a positive relationship between temperature and number of rides.\n\nAs it warms up outside, folks are more likely to puruse outdoor activities, including biking.\n\nFor learning purposes, let’s focus on the model with a single predictor (y ~ x in R)."
  },
  {
    "objectID": "files/lectures/09-1-modeling-normal.html#building-the-regression-model",
    "href": "files/lectures/09-1-modeling-normal.html#building-the-regression-model",
    "title": "Regression in the Bayesian Framework",
    "section": "Building the Regression Model",
    "text": "Building the Regression Model\n\nSuppose we have n data pairs,\n\n \\{ (Y_1, X_1), (Y_2, X_2), ..., (Y_n, X_n) \\} \n\nwhere Y_i is the number of rides and X_i is the high temperature (oF) on day i.\nAssuming that the relationship is linear, we can model\n\n\\mu_i = \\beta_0 + \\beta_1 X_i,\n\nwhere \\beta_0 and \\beta_1 are model coefficients."
  },
  {
    "objectID": "files/lectures/09-1-modeling-normal.html#building-the-regression-model-1",
    "href": "files/lectures/09-1-modeling-normal.html#building-the-regression-model-1",
    "title": "Regression in the Bayesian Framework",
    "section": "Building the Regression Model",
    "text": "Building the Regression Model\n\nWhat do we mean by “model coefficients?”\n\n\\mu_i = \\beta_0 + \\beta_1 X_i,\n\n\\beta_0 is the baseline for where our model crosses the y-axis, i.e., when X_i=0.\n\nIs this meaningful when we are talking about the average ridership when it is 0oF in DC?"
  },
  {
    "objectID": "files/lectures/09-1-modeling-normal.html#building-the-regression-model-2",
    "href": "files/lectures/09-1-modeling-normal.html#building-the-regression-model-2",
    "title": "Regression in the Bayesian Framework",
    "section": "Building the Regression Model",
    "text": "Building the Regression Model\n\nWhat do we mean by “model coefficients?”\n\n\\mu_i = \\beta_0 + \\beta_1 X_i,\n\n\\beta_1 is the slope, or average change, in the outcome (Y) for a one unit increase in the predictor (X).\n\nInterpretation: for a [1 unit of predictor] increase in [the predictor], [the outcome] [increases or decreases] by [abs(\\beta_1)].\nIn our example, suppose \\beta_1=4.5. For a 1oF increase in the temperature, the ridership increases by 4.5 riders."
  },
  {
    "objectID": "files/lectures/09-1-modeling-normal.html#building-the-regression-model-3",
    "href": "files/lectures/09-1-modeling-normal.html#building-the-regression-model-3",
    "title": "Regression in the Bayesian Framework",
    "section": "Building the Regression Model",
    "text": "Building the Regression Model\n\nWe are now interested in the model\n\nY_i|\\beta_0,\\beta_1,\\sigma\\overset{\\text{ind}}{\\sim} N(\\mu_i, \\sigma^2) \\text{ with } \\mu_i = \\beta_0 + \\beta_1 X_i\n\nNote that \\mu_i is the local mean (for a specific value of X).\n\nIn our data, \\mu_i is the mean ridership for day i.\n\nThe global mean (regardless of the value of X) is given by \\mu.\n\nIn our data, \\mu is the mean ridership, regardless of day.\n\nUnder this model, \\sigma is now measuring the variability from the local mean."
  },
  {
    "objectID": "files/lectures/09-1-modeling-normal.html#building-the-regression-model-4",
    "href": "files/lectures/09-1-modeling-normal.html#building-the-regression-model-4",
    "title": "Regression in the Bayesian Framework",
    "section": "Building the Regression Model",
    "text": "Building the Regression Model\n\nWe know the assumptions for linear regression in the frequentist framework.\nIn Bayesian Normal regression,\n\nThe observations are independent.\nY can be written as a linear function of X, \\mu = \\beta_0 + \\beta_1 X.\nFor any value of X, Y varies normally around \\mu with constant standard deviation \\sigma."
  },
  {
    "objectID": "files/lectures/09-1-modeling-normal.html#building-the-regression-model-5",
    "href": "files/lectures/09-1-modeling-normal.html#building-the-regression-model-5",
    "title": "Regression in the Bayesian Framework",
    "section": "Building the Regression Model",
    "text": "Building the Regression Model\n\nFirst, we will assume that our prior models of \\beta_0, \\beta_1, and \\sigma are independent.\nIt is common to use the a normal prior for \\beta_0 and \\beta_1.\n\n\n\\begin{align*}\n  \\beta_0 &\\sim N(m_0, s_0^2) \\\\\n  \\beta_1 &\\sim N(m_1, s_1^2)\n\\end{align*}\n\n\nThen, it is the default to use the exponential for \\sigma; both are restricted to positive values.\n\n\\sigma \\sim \\text{Exp}(l)\n\nNote that E[\\sigma] = 1/l and \\text{sd}[\\sigma] = 1/l."
  },
  {
    "objectID": "files/lectures/09-1-modeling-normal.html#building-the-regression-model-6",
    "href": "files/lectures/09-1-modeling-normal.html#building-the-regression-model-6",
    "title": "Regression in the Bayesian Framework",
    "section": "Building the Regression Model",
    "text": "Building the Regression Model\n\nThus, our regression model has the following formulation:\n\n\n\\begin{align*}\n&\\text{data}: & Y_i|\\beta_0, \\beta_1, \\sigma & \\overset{\\text{ind}}{\\sim} N(\\mu_i, \\sigma^2) \\text{ with } \\mu_i = \\beta_0 + \\beta_1 X_i \\\\\n&\\text{priors:} &\\beta_0 & \\sim N(m_0, s_0^2) \\\\\n& & \\beta_1  &\\sim N(m_1, s_1^2) \\\\\n& & & \\sigma \\sim \\text{Exp}(l)\n\\end{align*}"
  },
  {
    "objectID": "files/lectures/09-1-modeling-normal.html#tuning-the-prior-models",
    "href": "files/lectures/09-1-modeling-normal.html#tuning-the-prior-models",
    "title": "Regression in the Bayesian Framework",
    "section": "Tuning the Prior Models",
    "text": "Tuning the Prior Models\n\nBased on the past bikeshare analyses, we have the following centered prior understandings. What should our priors be?\n\n\nOn an average temperature day for DC (65oF-70oF), there are typically around 5000 riders, but this could vary between 3000 and 7000 riders.\nFor every one degree increase in temperature, ridership typically increases by 100 rides, but this could vary between 20 and 180 rides.\nAt any given temperature, the daily ridership will tend to vary with a moderate standard deviation of 1250 rides."
  },
  {
    "objectID": "files/lectures/09-1-modeling-normal.html#tuning-the-prior-models-1",
    "href": "files/lectures/09-1-modeling-normal.html#tuning-the-prior-models-1",
    "title": "Regression in the Bayesian Framework",
    "section": "Tuning the Prior Models",
    "text": "Tuning the Prior Models\n\nOn an average temperature day for DC (65oF-70oF), there are typically around 5000 riders, but this could vary between 3000 and 7000 riders.\n\n\\beta_{0\\text{c}} \\sim N(5000, 1000^2)\n\nFor every one degree increase in temperature, ridership typically increases by 100 rides, but this could vary between 20 and 180 rides.\n\n\\beta_{1\\text{c}} \\sim N(100, 40^2)\n\nAt any given temperature, the daily ridership will tend to vary with a moderate standard deviation of 1250 rides.\n\nRecall, E[\\sigma] = 1/l = 1250, so l = 1/1250 = 0.0008.\n\n\n \\sigma \\sim \\text{Exp}(0.0008)"
  },
  {
    "objectID": "files/lectures/09-1-modeling-normal.html#tuning-the-prior-models-2",
    "href": "files/lectures/09-1-modeling-normal.html#tuning-the-prior-models-2",
    "title": "Regression in the Bayesian Framework",
    "section": "Tuning the Prior Models",
    "text": "Tuning the Prior Models\n\\beta_{0\\text{c}} \\sim N(5000, 1000^2) \\ \\ \\ \\beta_{1\\text{c}} \\sim N(100, 40^2) \\ \\ \\ \\sigma \\sim \\text{Exp}(0.0008)"
  },
  {
    "objectID": "files/lectures/09-1-modeling-normal.html#simulating-the-posterior",
    "href": "files/lectures/09-1-modeling-normal.html#simulating-the-posterior",
    "title": "Regression in the Bayesian Framework",
    "section": "Simulating the Posterior",
    "text": "Simulating the Posterior\n\nLet’s now update what we know using the bikes data in the bayesrule package.\nLooking at the basic relationship between the number of rides vs. the temperature (as it feels outside),"
  },
  {
    "objectID": "files/lectures/09-1-modeling-normal.html#simulating-the-posterior-1",
    "href": "files/lectures/09-1-modeling-normal.html#simulating-the-posterior-1",
    "title": "Regression in the Bayesian Framework",
    "section": "Simulating the Posterior",
    "text": "Simulating the Posterior\n\nNote: I am skipping the derivation / the true math behind how to find the posterior in this situation. (It involves multiple integrals!)\nWe will use the stan_glm() function from rstanarm – it contains pre-defined Bayesian regression models.\n\nstan_glm() also applies to the wider family of GzLM (i.e., logistic & Poisson/negbin).\n\n\n\nlibrary(rstanarm)\nbike_model &lt;- stan_glm(rides ~ temp_feel, # data model\n                       data = bikes, # dataset\n                       family = gaussian, # distribution to apply\n                       prior_intercept = normal(5000, 1000), # b0_c\n                       prior = normal(100, 40), # b1_c\n                       prior_aux = exponential(0.0008), # sigma\n                       chains = 4, # 4 chains\n                       iter = 5000*2, # 10000 iterations - throw out first 5000\n                       seed = 84735) # starting place in RNG"
  },
  {
    "objectID": "files/lectures/09-1-modeling-normal.html#simulating-the-posterior-2",
    "href": "files/lectures/09-1-modeling-normal.html#simulating-the-posterior-2",
    "title": "Regression in the Bayesian Framework",
    "section": "Simulating the Posterior",
    "text": "Simulating the Posterior\n\nbike_model &lt;- stan_glm(rides ~ temp_feel, # data model\n                       data = bikes, # dataset\n                       family = gaussian, # distribution to apply\n                       prior_intercept = normal(5000, 1000), # b0_c\n                       prior = normal(100, 40), # b1_c\n                       prior_aux = exponential(0.0008), # sigma\n                       chains = 4, # 4 chains\n                       iter = 5000*2, # 10000 iterations - throw out first 5000\n                       seed = 84735) # starting place in RNG\n\n\nstan_glm() contains three types of information:\n\nData information: The first three arguments specify the structure of our data.\n\nWe want to model ridership by temperature (rides ~ temp_feel) using data = bikes and assuming a Normal data model, aka family = gaussian."
  },
  {
    "objectID": "files/lectures/09-1-modeling-normal.html#simulating-the-posterior-3",
    "href": "files/lectures/09-1-modeling-normal.html#simulating-the-posterior-3",
    "title": "Regression in the Bayesian Framework",
    "section": "Simulating the Posterior",
    "text": "Simulating the Posterior\n\nbike_model &lt;- stan_glm(rides ~ temp_feel, # data model\n                       data = bikes, # dataset\n                       family = gaussian, # distribution to apply\n                       prior_intercept = normal(5000, 1000), # b0_c\n                       prior = normal(100, 40), # b1_c\n                       prior_aux = exponential(0.0008), # sigma\n                       chains = 4, # 4 chains\n                       iter = 5000*2, # 10000 iterations - throw out first 5000\n                       seed = 84735) # starting place in RNG\n\n\nstan_glm() contains three types of information:\n\nPrior information: The prior_intercept, prior, and prior_aux arguments give the priors for \\beta_{0\\text{c}}, \\beta_{1\\text{c}}, \\sigma."
  },
  {
    "objectID": "files/lectures/09-1-modeling-normal.html#simulating-the-posterior-4",
    "href": "files/lectures/09-1-modeling-normal.html#simulating-the-posterior-4",
    "title": "Regression in the Bayesian Framework",
    "section": "Simulating the Posterior",
    "text": "Simulating the Posterior\n\nbike_model &lt;- stan_glm(rides ~ temp_feel, # data model\n                       data = bikes, # dataset\n                       family = gaussian, # distribution to apply\n                       prior_intercept = normal(5000, 1000), # b0_c\n                       prior = normal(100, 40), # b1_c\n                       prior_aux = exponential(0.0008), # sigma\n                       chains = 4, # 4 chains\n                       iter = 5000*2, # 10000 iterations - throw out first 5000\n                       seed = 84735) # starting place in RNG\n\n\nstan_glm() contains three types of information:\n\nMarkov chain information: The chains, iter, and seed arguments specify the number of Markov chains to run, the length or number of iterations for each chain, and the starting place of the RNG."
  },
  {
    "objectID": "files/lectures/09-1-modeling-normal.html#simulating-the-posterior-5",
    "href": "files/lectures/09-1-modeling-normal.html#simulating-the-posterior-5",
    "title": "Regression in the Bayesian Framework",
    "section": "Simulating the Posterior",
    "text": "Simulating the Posterior\n\nWait, how does this work when we are looking at three model parameters?\nWe will have three vectors – one for each model parameter.\n\n\n\\begin{align*}\n  &lt;\\beta_0^{(1)}, & \\ \\beta_0^{(2)}, ..., \\beta_0^{(5000)}&gt; \\\\\n  &lt;\\beta_1^{(1)}, & \\ \\beta_1^{(2)}, ..., \\beta_1^{(5000)} &gt; \\\\\n&lt;\\sigma^{(1)}, & \\ \\sigma^{(2)}, ..., \\sigma^{(5000)} &gt; \\\\\n\\end{align*}"
  },
  {
    "objectID": "files/lectures/09-1-modeling-normal.html#simulation-diagnostics",
    "href": "files/lectures/09-1-modeling-normal.html#simulation-diagnostics",
    "title": "Regression in the Bayesian Framework",
    "section": "Simulation Diagnostics",
    "text": "Simulation Diagnostics\n\nRun the simulation code (from a previous slide).\nThen, run diagnostics:\n\n\nneff_ratio(bike_model)\nrhat(bike_model)\nmcmc_trace(bike_model, size = 0.1)\nmcmc_dens_overlay(bike_model)"
  },
  {
    "objectID": "files/lectures/09-1-modeling-normal.html#simulation-diagnostics-1",
    "href": "files/lectures/09-1-modeling-normal.html#simulation-diagnostics-1",
    "title": "Regression in the Bayesian Framework",
    "section": "Simulation Diagnostics",
    "text": "Simulation Diagnostics\n\nDiagnostics:\n\n\nneff_ratio(bike_model)\n\n(Intercept)   temp_feel       sigma \n    1.03285     1.03505     0.96585 \n\nrhat(bike_model)\n\n(Intercept)   temp_feel       sigma \n  0.9998873   0.9999032   0.9999642 \n\n\n\nQuick diagnostics indicate that the resulting chains are trustworthy.\nThe effective sample size ratios are slightly above 1 and the R-hat values are very close to 1."
  },
  {
    "objectID": "files/lectures/09-1-modeling-normal.html#simulation-diagnostics-2",
    "href": "files/lectures/09-1-modeling-normal.html#simulation-diagnostics-2",
    "title": "Regression in the Bayesian Framework",
    "section": "Simulation Diagnostics",
    "text": "Simulation Diagnostics\n\nDiagnostics:\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe can see that the chains are stable, mixing quickly, and behaving much like an independent sample."
  },
  {
    "objectID": "files/lectures/09-1-modeling-normal.html#simulation-diagnostics-3",
    "href": "files/lectures/09-1-modeling-normal.html#simulation-diagnostics-3",
    "title": "Regression in the Bayesian Framework",
    "section": "Simulation Diagnostics",
    "text": "Simulation Diagnostics\n\nDiagnostics:\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe density plot lets us visualize and examine the posterior models for each of our regression parameters, \\beta_0, \\beta_1, \\sigma."
  },
  {
    "objectID": "files/lectures/09-1-modeling-normal.html#interpreting-the-posterior",
    "href": "files/lectures/09-1-modeling-normal.html#interpreting-the-posterior",
    "title": "Regression in the Bayesian Framework",
    "section": "Interpreting the Posterior",
    "text": "Interpreting the Posterior\n\nOkay… what does this mean, though?\n\n\ntidy(bike_model, effects = c(\"fixed\", \"aux\"), conf.int = TRUE, conf.level = 0.80)\n\n\n  \n\n\n\n\nThus, the posterior median relationship is\n\ny = -2194.24 + 82.16x\n\nFor a 1 degree increase in temperature, we expect ridership to increase by about 82 rides, with 80% credible interval (75.7, 88.7)."
  },
  {
    "objectID": "files/lectures/09-1-modeling-normal.html#interpreting-the-posterior-1",
    "href": "files/lectures/09-1-modeling-normal.html#interpreting-the-posterior-1",
    "title": "Regression in the Bayesian Framework",
    "section": "Interpreting the Posterior",
    "text": "Interpreting the Posterior\n\nWe can look at alternatives by drawing from the simulated data in bike_model.\n\nThe add_fitted_draws() function is from the tidybayes package.\n\n\n\n\nbikes %&gt;%\n  add_fitted_draws(bike_model, n = 50) %&gt;%\n  ggplot(aes(x = temp_feel, y = rides)) +\n    geom_line(aes(y = .value, group = .draw), alpha = 0.15) + \n    geom_point(data = bikes, size = 0.05) +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  theme_bw()"
  },
  {
    "objectID": "files/lectures/09-1-modeling-normal.html#interpreting-the-posterior-2",
    "href": "files/lectures/09-1-modeling-normal.html#interpreting-the-posterior-2",
    "title": "Regression in the Bayesian Framework",
    "section": "Interpreting the Posterior",
    "text": "Interpreting the Posterior\n\nWe can look at alternatives by drawing from the simulated data in bike_model.\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe see that the plausible models are not super variable.\n\nThis means we’re more confident about the relationship we’re observing."
  },
  {
    "objectID": "files/lectures/09-1-modeling-normal.html#interpreting-the-posterior-3",
    "href": "files/lectures/09-1-modeling-normal.html#interpreting-the-posterior-3",
    "title": "Regression in the Bayesian Framework",
    "section": "Interpreting the Posterior",
    "text": "Interpreting the Posterior\n\nHow does this compare against the frequentist version of regression?"
  },
  {
    "objectID": "files/lectures/09-1-modeling-normal.html#working-example-2",
    "href": "files/lectures/09-1-modeling-normal.html#working-example-2",
    "title": "Regression in the Bayesian Framework",
    "section": "Working Example",
    "text": "Working Example\n\nWe will examine Australian weather from the weather_WU data in the bayesrules package.\n\nThis data contains 100 days of weather data for each of two Australian cities: Uluru and Wollongong.\n\n\n\ndata(weather_WU)\nhead(weather_WU)"
  },
  {
    "objectID": "files/lectures/09-1-modeling-normal.html#working-example-3",
    "href": "files/lectures/09-1-modeling-normal.html#working-example-3",
    "title": "Regression in the Bayesian Framework",
    "section": "Working Example",
    "text": "Working Example\n\nLet’s keep only the variables on afternoon temperatures (temp3pm) and a subset of possible predictors that we’d have access to in the morning:\n\n\nweather_WU &lt;- weather_WU %&gt;% \n  select(location, windspeed9am, humidity9am, pressure9am, temp9am, temp3pm)\nhead(weather_WU)"
  },
  {
    "objectID": "files/lectures/09-1-modeling-normal.html#working-example-4",
    "href": "files/lectures/09-1-modeling-normal.html#working-example-4",
    "title": "Regression in the Bayesian Framework",
    "section": "Working Example",
    "text": "Working Example\n\nWe begin our analysis with the familiar: a simple Normal regression model of temp3pm with one quantitative predictor, the morning temperature temp9am, both measured in degrees Celsius.\n\n\n\nggplot(weather_WU, aes(x = temp9am, y = temp3pm)) +\n  geom_point(size = 0.2) + \n  theme_bw()"
  },
  {
    "objectID": "files/lectures/09-1-modeling-normal.html#working-example-5",
    "href": "files/lectures/09-1-modeling-normal.html#working-example-5",
    "title": "Regression in the Bayesian Framework",
    "section": "Working Example",
    "text": "Working Example\n\nLet’s model the 3 pm temperature as a function of the 9 am temperature on a given day i.\n\nOutcome: Y_i = 3 pm temp\nPredictor: X_{i1} = 9 am temp\n\nThen, we can model it using the Bayesian normal regression model,\n\n\n\\begin{align*}\nY_i | \\beta_0, \\beta_1, \\sigma &\\overset{\\text{ind}}{\\sim} N(\\mu_i, \\sigma^2), \\text{ with } \\mu_i = \\beta_0 + \\beta_1 X_{i1} \\\\\n\\beta_{0c} &\\sim N(25, 5^2) \\\\\n\\beta_1 &\\sim N(0,3.1^2) \\\\\n\\sigma & \\sim \\text{Exp}(0.13)\n\\end{align*}\n\n\nNote that we are using the centered intercept as 0 degree mornings are rare in Australia."
  },
  {
    "objectID": "files/lectures/09-1-modeling-normal.html#working-example-6",
    "href": "files/lectures/09-1-modeling-normal.html#working-example-6",
    "title": "Regression in the Bayesian Framework",
    "section": "Working Example",
    "text": "Working Example\n\nSimulating this model,\n\n\nweather_model_1 &lt;- stan_glm(\n  temp3pm ~ temp9am, \n  data = weather_WU, family = gaussian,\n  prior_intercept = normal(25, 5),\n  prior = normal(0, 2.5, autoscale = TRUE), \n  prior_aux = exponential(1, autoscale = TRUE),\n  chains = 4, iter = 5000*2, seed = 84735)\n\n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 1.2e-05 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.12 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 1: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 1: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 1: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 1: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 1: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 1: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 1: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 1: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 1: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 1: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 1: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.069 seconds (Warm-up)\nChain 1:                0.119 seconds (Sampling)\nChain 1:                0.188 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 7e-06 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.07 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 2: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 2: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 2: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 2: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 2: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 2: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 2: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 2: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 2: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 2: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 2: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.07 seconds (Warm-up)\nChain 2:                0.129 seconds (Sampling)\nChain 2:                0.199 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 6e-06 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.06 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 3: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 3: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 3: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 3: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 3: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 3: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 3: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 3: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 3: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 3: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 3: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 0.072 seconds (Warm-up)\nChain 3:                0.127 seconds (Sampling)\nChain 3:                0.199 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 6e-06 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.06 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 4: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 4: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 4: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 4: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 4: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 4: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 4: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 4: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 4: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 4: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 4: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 0.071 seconds (Warm-up)\nChain 4:                0.126 seconds (Sampling)\nChain 4:                0.197 seconds (Total)\nChain 4:"
  },
  {
    "objectID": "files/lectures/09-1-modeling-normal.html#working-example-7",
    "href": "files/lectures/09-1-modeling-normal.html#working-example-7",
    "title": "Regression in the Bayesian Framework",
    "section": "Working Example",
    "text": "Working Example\n\nNote that we asked stan_glm() to autoscale our priors. What did it change them to?\n\n\nprior_summary(weather_model_1)\n\nPriors for model 'weather_model_1' \n------\nIntercept (after predictors centered)\n ~ normal(location = 25, scale = 5)\n\nCoefficients\n  Specified prior:\n    ~ normal(location = 0, scale = 2.5)\n  Adjusted prior:\n    ~ normal(location = 0, scale = 3.1)\n\nAuxiliary (sigma)\n  Specified prior:\n    ~ exponential(rate = 1)\n  Adjusted prior:\n    ~ exponential(rate = 0.13)\n------\nSee help('prior_summary.stanreg') for more details"
  },
  {
    "objectID": "files/lectures/09-1-modeling-normal.html#working-example-8",
    "href": "files/lectures/09-1-modeling-normal.html#working-example-8",
    "title": "Regression in the Bayesian Framework",
    "section": "Working Example",
    "text": "Working Example\n\n\nmcmc_trace(weather_model_1, size = 0.1)"
  },
  {
    "objectID": "files/lectures/09-1-modeling-normal.html#working-example-9",
    "href": "files/lectures/09-1-modeling-normal.html#working-example-9",
    "title": "Regression in the Bayesian Framework",
    "section": "Working Example",
    "text": "Working Example\n\n\nmcmc_dens_overlay(weather_model_1)"
  },
  {
    "objectID": "files/lectures/09-1-modeling-normal.html#working-example-10",
    "href": "files/lectures/09-1-modeling-normal.html#working-example-10",
    "title": "Regression in the Bayesian Framework",
    "section": "Working Example",
    "text": "Working Example\n\nposterior_interval(weather_model_1, prob = 0.80)\n\n                  10%      90%\n(Intercept) 2.9498083 5.448752\ntemp9am     0.9802648 1.102423\nsigma       3.8739305 4.409474\n\n\n\n80% credible interval for \\beta_1: (0.98, 1.10)\n80% credible interval for \\sigma: (3.87, 4.41)"
  },
  {
    "objectID": "files/lectures/09-1-modeling-normal.html#working-example-11",
    "href": "files/lectures/09-1-modeling-normal.html#working-example-11",
    "title": "Regression in the Bayesian Framework",
    "section": "Working Example",
    "text": "Working Example\n\n\npp_check(weather_model_1)"
  },
  {
    "objectID": "files/lectures/09-1-modeling-normal.html#categorical-predictors",
    "href": "files/lectures/09-1-modeling-normal.html#categorical-predictors",
    "title": "Regression in the Bayesian Framework",
    "section": "Categorical Predictors",
    "text": "Categorical Predictors\n\nWhat if we look at the data by location?\n\n\n\nggplot(weather_WU, aes(x = temp3pm, fill = location)) + \n  geom_density(alpha = 0.5) + theme_bw()\n\n\n\n\n\n\n\n\n\n\nWe should probably look at location as a predictor…"
  },
  {
    "objectID": "files/lectures/09-1-modeling-normal.html#categorical-predictors-1",
    "href": "files/lectures/09-1-modeling-normal.html#categorical-predictors-1",
    "title": "Regression in the Bayesian Framework",
    "section": "Categorical Predictors",
    "text": "Categorical Predictors\n\nLet’s let X_{i2} be an indicator for the location,\n\n\nX_{i2} =\n\\begin{cases}\n1 & \\text{Wollongong} \\\\\n0 & \\text{otherwise (i.e., Uluru).}\n\\end{cases}\n\n\nWe are treating “not-Wollongong” as our reference group – in this case, it is Uluru.\n\n\n\\begin{array}{rl}\n\\text{data:} & Y_i \\mid \\beta_0, \\beta_1, \\sigma \\overset{\\text{ind}}{\\sim} N(\\mu_i, \\sigma^2) \\quad \\text{with} \\quad \\mu_i = \\beta_0 + \\beta_1 X_{i2} \\\\\n\\text{priors:} & \\beta_{0c} \\sim N(25, 5^2) \\\\\n& \\beta_1 \\sim N(0, 38^2) \\\\\n& \\sigma \\sim \\text{Exp}(0.13).\n\\end{array}"
  },
  {
    "objectID": "files/lectures/09-1-modeling-normal.html#categorical-predictors-2",
    "href": "files/lectures/09-1-modeling-normal.html#categorical-predictors-2",
    "title": "Regression in the Bayesian Framework",
    "section": "Categorical Predictors",
    "text": "Categorical Predictors\n\nLet’s think about our model.\n\ny = \\beta_0 + \\beta_1 x_2\n\nWhat do our coefficients mean?\n\n\\beta_0 is the typical 3 pm temperature in Uluru (x_2=0).\n\\beta_1 is the typical difference in 3 pm temperature in Wollongong (x_2=1) as compared to Uluru (x_2=0).\n\\sigma represents the standard deviation in 3 pm temperatures in Wollongong and Uluru."
  },
  {
    "objectID": "files/lectures/09-1-modeling-normal.html#categorical-predictors-3",
    "href": "files/lectures/09-1-modeling-normal.html#categorical-predictors-3",
    "title": "Regression in the Bayesian Framework",
    "section": "Categorical Predictors",
    "text": "Categorical Predictors\n\nLet’s think about our model.\n\ny = \\beta_0 + \\beta_1 x_2\n\n\\beta_1 is the typical difference in 3 pm temperature in Wollongong (x_2=1) as compared to Uluru (x_2=0).\nWhen we use a binary predictor, this results in two models, effectively.\n\nOne when x_1=0 (Uluru) and one when x_1=1 (Wollongong).\n\n\n\n\\begin{align*}\ny = \\beta_0 + \\beta_1 x_2 \\to \\text{(U)} \\ y &= \\beta_0 \\\\\n\\text{(W)} \\ y&=  \\beta_0+\\beta_1\n\\end{align*}"
  },
  {
    "objectID": "files/lectures/09-1-modeling-normal.html#categorical-predictors-4",
    "href": "files/lectures/09-1-modeling-normal.html#categorical-predictors-4",
    "title": "Regression in the Bayesian Framework",
    "section": "Categorical Predictors",
    "text": "Categorical Predictors\n\nLet’s simulate our posterior using weakly informative priors,\n\n\nweather_model_2 &lt;- stan_glm(\n  temp3pm ~ location,\n  data = weather_WU, family = gaussian,\n  prior_intercept = normal(25, 5),\n  prior = normal(0, 2.5, autoscale = TRUE), \n  prior_aux = exponential(1, autoscale = TRUE),\n  chains = 4, iter = 5000*2, seed = 84735)"
  },
  {
    "objectID": "files/lectures/09-1-modeling-normal.html#categorical-predictors-5",
    "href": "files/lectures/09-1-modeling-normal.html#categorical-predictors-5",
    "title": "Regression in the Bayesian Framework",
    "section": "Categorical Predictors",
    "text": "Categorical Predictors\n\n\nmcmc_trace(weather_model_2, size = 0.1)"
  },
  {
    "objectID": "files/lectures/09-1-modeling-normal.html#categorical-predictors-6",
    "href": "files/lectures/09-1-modeling-normal.html#categorical-predictors-6",
    "title": "Regression in the Bayesian Framework",
    "section": "Categorical Predictors",
    "text": "Categorical Predictors\n\n\nmcmc_dens_overlay(weather_model_2)"
  },
  {
    "objectID": "files/lectures/09-1-modeling-normal.html#categorical-predictors-7",
    "href": "files/lectures/09-1-modeling-normal.html#categorical-predictors-7",
    "title": "Regression in the Bayesian Framework",
    "section": "Categorical Predictors",
    "text": "Categorical Predictors\n\nLooking at the posterior summary,\n\n\ntidy(weather_model_2, effects = c(\"fixed\", \"aux\"),\n     conf.int = TRUE, conf.level = 0.80) %&gt;% \n  select(-std.error)"
  },
  {
    "objectID": "files/lectures/09-1-modeling-normal.html#categorical-predictors-8",
    "href": "files/lectures/09-1-modeling-normal.html#categorical-predictors-8",
    "title": "Regression in the Bayesian Framework",
    "section": "Categorical Predictors",
    "text": "Categorical Predictors\n\nWe can also look at the temperatures by location,"
  },
  {
    "objectID": "files/lectures/09-1-modeling-normal.html#multiple-predictors",
    "href": "files/lectures/09-1-modeling-normal.html#multiple-predictors",
    "title": "Regression in the Bayesian Framework",
    "section": "Multiple Predictors",
    "text": "Multiple Predictors\n\nWhat if we want to include multiple predictors?\n\nNotice in the code, our model now has multiple predictors (temp9am and location).\nHere, we are simulating the prior - this will allow us to graphically examine what we are claiming with the priors.\n\n\n\nweather_model_3_prior &lt;- stan_glm(\n  temp3pm ~ temp9am + location,\n  data = weather_WU, family = gaussian, \n  prior_intercept = normal(25, 5),\n  prior = normal(0, 2.5, autoscale = TRUE), \n  prior_aux = exponential(1, autoscale = TRUE),\n  chains = 4, iter = 5000*2, seed = 84735,\n  prior_PD = TRUE)"
  },
  {
    "objectID": "files/lectures/09-1-modeling-normal.html#multiple-predictors-1",
    "href": "files/lectures/09-1-modeling-normal.html#multiple-predictors-1",
    "title": "Regression in the Bayesian Framework",
    "section": "Multiple Predictors",
    "text": "Multiple Predictors\n\n\n\n\n\n\n\n\n\n\n\n\nFrom the simulated priors,\n\nWe can look at different sets of 3 p.m. temperature data (left graph)."
  },
  {
    "objectID": "files/lectures/09-1-modeling-normal.html#multiple-predictors-2",
    "href": "files/lectures/09-1-modeling-normal.html#multiple-predictors-2",
    "title": "Regression in the Bayesian Framework",
    "section": "Multiple Predictors",
    "text": "Multiple Predictors\n\n\n\n\n\n\n\n\n\n\n\n\nFrom the simulated priors,\n\nWe can also look at our prior assumptions about the relationship between 3 p.m. and 9 a.m. temperature at each location (right graph).\n\nOur prior is vague; we’re not sure of the relationship!"
  },
  {
    "objectID": "files/lectures/09-1-modeling-normal.html#multiple-predictors-3",
    "href": "files/lectures/09-1-modeling-normal.html#multiple-predictors-3",
    "title": "Regression in the Bayesian Framework",
    "section": "Multiple Predictors",
    "text": "Multiple Predictors\n\nInstead of starting the stan_glm() syntax from scratch, we can update() the weather_model_3_prior by setting prior_PD = FALSE:\n\n\nweather_model_3 &lt;- update(weather_model_3_prior, prior_PD = FALSE)\n\n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 1.3e-05 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.13 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 1: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 1: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 1: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 1: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 1: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 1: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 1: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 1: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 1: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 1: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 1: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.082 seconds (Warm-up)\nChain 1:                0.133 seconds (Sampling)\nChain 1:                0.215 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 6e-06 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.06 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 2: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 2: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 2: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 2: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 2: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 2: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 2: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 2: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 2: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 2: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 2: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.075 seconds (Warm-up)\nChain 2:                0.141 seconds (Sampling)\nChain 2:                0.216 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 6e-06 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.06 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 3: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 3: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 3: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 3: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 3: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 3: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 3: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 3: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 3: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 3: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 3: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 0.08 seconds (Warm-up)\nChain 3:                0.141 seconds (Sampling)\nChain 3:                0.221 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 6e-06 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.06 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 4: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 4: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 4: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 4: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 4: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 4: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 4: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 4: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 4: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 4: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 4: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 0.08 seconds (Warm-up)\nChain 4:                0.131 seconds (Sampling)\nChain 4:                0.211 seconds (Total)\nChain 4:"
  },
  {
    "objectID": "files/lectures/09-1-modeling-normal.html#multiple-predictors-4",
    "href": "files/lectures/09-1-modeling-normal.html#multiple-predictors-4",
    "title": "Regression in the Bayesian Framework",
    "section": "Multiple Predictors",
    "text": "Multiple Predictors\n\nThe simulation results in 20,000 posterior plausible relationships between temperature and location.\nYou try the following code:\n\n\n\nweather_WU %&gt;%\n  add_fitted_draws(weather_model_3, n = 100) %&gt;%\n  ggplot(aes(x = temp9am, y = temp3pm, color = location)) +\n    geom_line(aes(y = .value, group = paste(location, .draw)), alpha = .1) +\n    geom_point(data = weather_WU, size = 0.1) +\n  theme_bw()"
  },
  {
    "objectID": "files/lectures/09-1-modeling-normal.html#multiple-predictors-5",
    "href": "files/lectures/09-1-modeling-normal.html#multiple-predictors-5",
    "title": "Regression in the Bayesian Framework",
    "section": "Multiple Predictors",
    "text": "Multiple Predictors\n\n\n\n\n\n\n\n\n\n\n\n\n3 p.m. temperature is positively associated with 9 a.m. temperature and tends to be higher in Uluru than in Wollongong.\nFurther, relative to the prior simulated relationships in Figure 11.9, these posterior relationships are very consistent"
  },
  {
    "objectID": "files/lectures/09-1-modeling-normal.html#multiple-predictors-6",
    "href": "files/lectures/09-1-modeling-normal.html#multiple-predictors-6",
    "title": "Regression in the Bayesian Framework",
    "section": "Multiple Predictors",
    "text": "Multiple Predictors\n\nLooking at the posterior summary statistics,\n\n\ntidy(weather_model_3, effects = c(\"fixed\", \"aux\"),\n     conf.int = TRUE, conf.level = 0.80) %&gt;% \n  select(-std.error)"
  },
  {
    "objectID": "files/lectures/09-1-modeling-normal.html#multiple-predictors-7",
    "href": "files/lectures/09-1-modeling-normal.html#multiple-predictors-7",
    "title": "Regression in the Bayesian Framework",
    "section": "Multiple Predictors",
    "text": "Multiple Predictors\n\nYou try! Run the following code to look at our posterior predictive models.\n\n\n\n# Simulate a set of predictions\nset.seed(84735)\ntemp3pm_prediction &lt;- posterior_predict(\n  weather_model_3,\n  newdata = data.frame(temp9am = c(10, 10), \n                       location = c(\"Uluru\", \"Wollongong\")))\n\n# Plot the posterior predictive models\nmcmc_areas(temp3pm_prediction) +\n  ggplot2::scale_y_discrete(labels = c(\"Uluru\", \"Wollongong\")) + \n  xlab(\"temp3pm\")"
  },
  {
    "objectID": "files/lectures/09-1-modeling-normal.html#multiple-predictors-8",
    "href": "files/lectures/09-1-modeling-normal.html#multiple-predictors-8",
    "title": "Regression in the Bayesian Framework",
    "section": "Multiple Predictors",
    "text": "Multiple Predictors\n\n\n\n\n\n\n\n\n\n\n\n\nRoughly speaking, we can anticipate 3 p.m. temperatures between 15 and 25 degrees in Uluru, and cooler temperatures between 8 and 18 in Wollongong."
  },
  {
    "objectID": "files/lectures/09-1-modeling-normal.html#wrap-up",
    "href": "files/lectures/09-1-modeling-normal.html#wrap-up",
    "title": "Regression in the Bayesian Framework",
    "section": "Wrap Up",
    "text": "Wrap Up\n\nToday, we have learned about regression (under the normal distribution) in the Bayesian framework.\nWednesday: Evaluating the Model\nProject stuff:\n\nEES students should have their data this week\n\nDr. Seals and Dr. Schmutz will review before passing to Bayesian students.\n\n\nNext week:\n\nMonday: Binary logistic and Poisson regressions.\nWednesday: How to “read” (non-statistical) research papers."
  },
  {
    "objectID": "files/lectures/09-1-modeling-normal.html#practice-homework",
    "href": "files/lectures/09-1-modeling-normal.html#practice-homework",
    "title": "Regression in the Bayesian Framework",
    "section": "Practice / Homework",
    "text": "Practice / Homework\n\nFrom the Bayes Rules! textbook:\n\n9.9, 9.10, 9.11, 9.12\n9.16, 9.17, 9.18"
  },
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "Abbreviated Syllabus",
    "section": "",
    "text": "Note: this is an abbreviated syllabus. The full syllabus is available through Classmate and/or Canvas.\n\nInstructor Information\n\nDr. Samantha Seals\nOffice: 4/344\n\n\n\nMeeting Times and Location\n\nMW 4:00 pm–5:15 pm\nPhysical classroom: 4/406\nZoom classroom: see Canvas or full syllabus for link\n\n\n\nOffice Hours\n\nMonday: 9:00 am-11:00 am\nTuesday: 2:00 pm-4:00 pm\nWednesday: 9:00 am-11:00 am\nOther times by appointment\n\n\n\nGrading and Evaluation\nThe course grade will be determined as follows:\n\nAssignments (25%): Every module will finish with an assignment to demonstrate the knowledge gained. TThe resulting .html file should be submitted to the designated Assignment on Canvas by 11:59 pm on the specified date (see Canvas).\nProject (50%): Students in this course will be collaborating with students in GEO6118 - Research Design. The final research project will be submitted along with smaller pieces throughout the semester.\nFinal Exam (25%): The final exam will be a take home final.\n\n\n\nLate Policy\nAssigments have due dates, however, the dropboxes will not close until the end of the semester. All students are automatically granted “extensions” without question.\nNote that if there is not a submission when I go to grade (after the initial deadline), I will assign a zero (0) and request that you submit the assignment when you are able to. This is only for record keeping purposes. There is no penalty for submitting late and a full grade will be given upon review of your submission.\nExtensions are not available for project pieces or the final exam."
  }
]