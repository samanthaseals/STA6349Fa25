[
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Tentative Schedule",
    "section": "",
    "text": "Week 1: August 25-29, 2025\n\n\n\n\n\n\nDr. Seals out sick :(\n\n\n\n\n\n\n\n\n\n\nWeek 2: September 1-5, 2025\n\n\n\n\n\n\nMeeting 1:\n\nLabor Day holiday - campus closed.\n\nMeeting 2:\n\nTopic(s):\n\nVery Brief Review of Probability\n\nSlides:\n\n.html: view slides here\n.qmd: see underlying code here\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 3: September 8-12, 2025\n\n\n\n\n\n\nMeeting 1:\n\nSchedule a meeting with your EES collaborator.\nWorksheet: see Canvas.\n\nMeeting 2:\n\nTopic(s):\n\nDiscrete distributions\n\nSlides:\n\n.html: view slides here\n.qmd: see underlying code here\n\n\nAssignment 1: Probability Distributions - due September 16\n\n.html: view assignment here\n.qmd: see underlying code here\n\n\n\n\n\n\n\n\n\n\n\nWeek 4: September 15-19, 2025\n\n\n\n\n\n\nMeeting 1:\n\nTopic(s):\n\nContinuous distributions\n\nSlides:\n\n.html: view slides here\n.qmd: see underlying code here\n\n\nAssignment 1: Probability Distributions - due September 21\n\n.html: view assignment here\n.qmd: see underlying code here\n\nMeeting 2:\n\nTopic(s):\n\nThinking like a Bayesian\nBayes’ Rule/Theorem\n\nSlides:\n\n.html: view slides here\n.qmd: see underlying code here\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 5: September 22-26, 2025\n\n\n\n\n\n\nMeeting 1:\n\nTopic(s):\n\nBeta-Binomial\n\nSlides:\n\n.html: view slides here\n.qmd: see underlying code here\n\n\nMeeting 2:\n\nTopic(s):\n\nBalance and Sequentiality\n\nSlides:\n\n.html: view slides here\n.qmd: see underlying code here\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 6: September 29-October 3, 2025\n\n\n\n\n\n\nMeeting 1:\n\nTopic(s):\n\nGamma-Poisson\n\nSlides:\n\n.html: view slides here\n.qmd: see underlying code here\n\n\nMeeting 2:\n\nTopic(s):\n\nNormal-Normal\n\nSlides:\n\n.html: view slides here\n.qmd: see underlying code here\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 7: October 6-10, 2025\n\n\n\n\n\n\nMeeting 1:\n\nTopic(s):\n\nApproximating the posterior\nMCMC\n\nSlides:\n\n.html: view slides here\n.qmd: see underlying code here\n\n\nMeeting 2:\n\nTopic(s):\n\nPosterior inference\nPosterior prediction\n\nSlides:\n\n.html: view slides here\n.qmd: see underlying code here\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 8: October 13-17, 2025\n\n\n\n\n\n\nMeeting 1:\n\nColumbus Day holiday - campus closed.\n\nAssignment 2: Basic Bayesian Analysis\n\n.html: view assignment here\n.qmd: see underlying code here\n\n\n\n\n\n\n\n\n\n\n\nWeek 9: October 20-24, 2025\n\n\n\n\n\n\nMeeting 1:\n\nTopic(s):\n\nRegression under the normal distribution\n\nSlides:\n\n.html:\n.qmd:\n\n\nMeeting 2:\n\nTopic(s):\n\nEvaluating the model\n\nSlides:\n\n.html:\n.qmd:\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 10: October 27-31, 2025\n\n\n\n\n\n\nMeeting 1:\n\nTopic(s):\n\nPoisson/negative binomial regressions\nLogistic regression\n\nSlides:\n\n.html:\n.qmd:\n\n\nMeeting 2:\n\nTopic(s):\n\nHow to “read” research papers outside of statistics.\n\nProject 2a: Reviewing literature (complete in class)\n\n.html:\n.qmd:\n\nProject 2b: Reviewing literature (complete at home)\n\n.html:\n.qmd:\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 11: November 3-7, 2025\n\n\n\n\n\n\nMeeting 1:\n\nAssignment 3: Regression Analysis\n\n.html:\n.qmd:\n\nCheck in with Project 2b\n\nMeeting 2:\n\nMeet with your EES collaborator on your own to discuss methodology.\nYour annotations (Project 2b) will be completed, which will help guide the discussion.\nProject 2: Discussion worksheet\n\n\n\n\n\n\n\n\n\n\n\nWeek 12: November 10-14, 2025\n\n\n\n\n\n\nMeeting 1:\n\nWork on analysis\n\nMeeting 2:\n\nWork on analysis\nProject 3: Analysis worksheet/check in\n\n\n\n\n\n\n\n\n\n\n\nWeek 13: November 17-21, 2025\n\n\n\n\n\n\nMeeting 1:\n\nTopic(s):\n\nGeneral guidelines for writing about statistical methodology.\n\nEthical considerations.\n\nGeneral guidelines for providing results.\n\nTables.\nVisualizations.\nWritten summaries.\n\n\nSlides:\n\n.html:\n.qmd:\n\nProject 4a: Methods paragraph\nProject 4b: Tables\nProject 4c: Graphs\n\nMeeting 2:\n\n2 minute presentation of your methodology/results\n\n\n\n\n\n\n\n\n\n\n\nWeek 14: November 24-28, 2025\n\n\n\n\n\n\nMeeting 1:\n\nThanksgiving holiday - campus closed.\n\nMeeting 2:\n\nThanksgiving holiday - campus closed.\n\n\n\n\n\n\n\n\n\n\n\nWeek 15: December 1-5, 2025\n\n\n\n\n\n\nMeeting 1:\n\nNo meeting; meet with your EES collaborator on your own to discuss results.\nProject 5: Worksheet for discussion\nTake home final opens.\n\nMeeting 2:\n\nNo meeting.\nFinal draft of research due.\n\n\n\n\n\n\n\n\n\n\n\nWeek 16: Exam Week!\n\n\n\n\n\n\nFinal exam due this week."
  },
  {
    "objectID": "files/lectures/06-1-gamma-poisson.html#introduction-gamma-poisson-model",
    "href": "files/lectures/06-1-gamma-poisson.html#introduction-gamma-poisson-model",
    "title": "Gamma-Poisson Model",
    "section": "Introduction: Gamma-Poisson Model",
    "text": "Introduction: Gamma-Poisson Model\n\nRecall the Beta-Binomial model,\n\ny \\sim \\text{Bin}(n, \\pi) (data distribution)\n\\pi \\sim \\text{Beta}(\\alpha, \\beta) (prior distribution)\n\\pi|y \\sim \\text{Beta}(\\alpha+y, \\beta+n-y) (posterior distribution)\n\nThe Beta-Binomial model is from a conjugate family (i.e., the posterior is from the same model family as the prior).\nNow, we will learn about the Gamma-Poisson, another conjugate family."
  },
  {
    "objectID": "files/lectures/06-1-gamma-poisson.html#example-set-up",
    "href": "files/lectures/06-1-gamma-poisson.html#example-set-up",
    "title": "Gamma-Poisson Model",
    "section": "Example Set Up",
    "text": "Example Set Up\n\nSuppose we are now interested in modeling the number of spam calls we receive.\n\nThis means that we are modeling the rate, \\lambda.\n\nWe take a guess and say that the value of \\lambda that is most likely is around 5,\n\n… but reasonably ranges between 2 and 7 calls per day.\n\nWhy can’t we use the Beta distribution as our prior distribution?\n\n\\lambda is the mean of a count \\to \\lambda \\in \\mathbb{R}^+ \\to \\lambda is not limited to [0, 1] \\to broken assumption for Beta distribution.\n\nWhy can’t we use the binomial distribution as our data distribution?\n\nY_i is a count \\to Y_i \\in \\mathbb{N}^+ \\to Y_i is not limited to \\{0, 1\\} \\to broken assumption for Binomial distribution."
  },
  {
    "objectID": "files/lectures/06-1-gamma-poisson.html#poisson-data-model",
    "href": "files/lectures/06-1-gamma-poisson.html#poisson-data-model",
    "title": "Gamma-Poisson Model",
    "section": "Poisson Data Model",
    "text": "Poisson Data Model\n\nWe will use the Poisson distribution to model the number of spam calls\n\nY \\in \\{0, 1, 2, ...\\}\n\nY is the number of independent events that occur in a fixed amount of time or space.\n\\lambda &gt; 0 is the rate at which these events occur.\nMathematically,\n\n Y | \\lambda \\sim \\text{Pois}(\\lambda),\n\nwith pmf,\n\nf(y|\\lambda) = \\frac{\\lambda^y e^{-\\lambda}}{y!}, \\ \\ \\ y \\in \\{0,1, 2, ... \\}"
  },
  {
    "objectID": "files/lectures/06-1-gamma-poisson.html#gamma-prior",
    "href": "files/lectures/06-1-gamma-poisson.html#gamma-prior",
    "title": "Gamma-Poisson Model",
    "section": "Gamma Prior",
    "text": "Gamma Prior\n\nIf \\lambda is a continuous random variable that can take on any positive value (\\lambda &gt; 0), then the variability may be modeled with the Gamma distribution with\n\nshape hyperparameter s&gt;0\nrate hyperparameter r&gt;0.\n\nThus,\n\n\\lambda \\sim \\text{Gamma}(s, r)\n\nand the Gamma pdf is given by\n\nf(\\lambda) = \\frac{r^s}{\\Gamma(s)} \\lambda^{s-1} e^{-r\\lambda}"
  },
  {
    "objectID": "files/lectures/06-1-gamma-poisson.html#gamma-prior-shapes",
    "href": "files/lectures/06-1-gamma-poisson.html#gamma-prior-shapes",
    "title": "Gamma-Poisson Model",
    "section": "Gamma Prior: Shapes",
    "text": "Gamma Prior: Shapes\n\nLet’s explore the shape of the Gamma:\n\n\n\nplot_gamma(1, 1) + theme_bw() + ggtitle(\"Gamma(1, 1)\")"
  },
  {
    "objectID": "files/lectures/06-1-gamma-poisson.html#gamma-prior-shapes-1",
    "href": "files/lectures/06-1-gamma-poisson.html#gamma-prior-shapes-1",
    "title": "Gamma-Poisson Model",
    "section": "Gamma Prior: Shapes",
    "text": "Gamma Prior: Shapes\n\nLet’s explore the shape of the Gamma:\n\n\n\nplot_gamma(2, 1) + theme_bw() + ggtitle(\"Gamma(2, 1)\")"
  },
  {
    "objectID": "files/lectures/06-1-gamma-poisson.html#gamma-prior-shapes-2",
    "href": "files/lectures/06-1-gamma-poisson.html#gamma-prior-shapes-2",
    "title": "Gamma-Poisson Model",
    "section": "Gamma Prior: Shapes",
    "text": "Gamma Prior: Shapes\n\nLet’s explore the shape of the Gamma:\n\n\n\nplot_gamma(10, 1) + theme_bw() + ggtitle(\"Gamma(10, 1)\")"
  },
  {
    "objectID": "files/lectures/06-1-gamma-poisson.html#gamma-prior-shapes-3",
    "href": "files/lectures/06-1-gamma-poisson.html#gamma-prior-shapes-3",
    "title": "Gamma-Poisson Model",
    "section": "Gamma Prior: Shapes",
    "text": "Gamma Prior: Shapes\n\nLet’s explore the shape of the Gamma:\n\n\n\nplot_gamma(10, 10) + theme_bw() + ggtitle(\"Gamma(10, 10)\")"
  },
  {
    "objectID": "files/lectures/06-1-gamma-poisson.html#gamma-prior-shapes-4",
    "href": "files/lectures/06-1-gamma-poisson.html#gamma-prior-shapes-4",
    "title": "Gamma-Poisson Model",
    "section": "Gamma Prior: Shapes",
    "text": "Gamma Prior: Shapes\n\nWhat happens when we increase the \\alpha parameter?"
  },
  {
    "objectID": "files/lectures/06-1-gamma-poisson.html#gamma-prior-shapes-5",
    "href": "files/lectures/06-1-gamma-poisson.html#gamma-prior-shapes-5",
    "title": "Gamma-Poisson Model",
    "section": "Gamma Prior: Shapes",
    "text": "Gamma Prior: Shapes\n\nWhat happens when we increase the \\beta parameter?"
  },
  {
    "objectID": "files/lectures/06-1-gamma-poisson.html#gamma-prior-shapes-6",
    "href": "files/lectures/06-1-gamma-poisson.html#gamma-prior-shapes-6",
    "title": "Gamma-Poisson Model",
    "section": "Gamma Prior: Shapes",
    "text": "Gamma Prior: Shapes\n\nPutting these on the same scale,"
  },
  {
    "objectID": "files/lectures/06-1-gamma-poisson.html#tuning-the-gamma-prior",
    "href": "files/lectures/06-1-gamma-poisson.html#tuning-the-gamma-prior",
    "title": "Gamma-Poisson Model",
    "section": "Tuning the Gamma Prior",
    "text": "Tuning the Gamma Prior\n\nLet’s now tune our prior.\nWe are assuming \\lambda \\approx 5, somewhere between 2 and 7.\nWe know the mean of the gamma distribution,\n\nE(\\lambda) = \\frac{s}{r} \\approx 5 \\to 5r \\approx s\n\nYour turn! Use the plot_gamma() function to figure out what value of s and r we need."
  },
  {
    "objectID": "files/lectures/06-1-gamma-poisson.html#tuning-the-gamma-prior-1",
    "href": "files/lectures/06-1-gamma-poisson.html#tuning-the-gamma-prior-1",
    "title": "Gamma-Poisson Model",
    "section": "Tuning the Gamma Prior",
    "text": "Tuning the Gamma Prior\n\nLooking at different values:"
  },
  {
    "objectID": "files/lectures/06-1-gamma-poisson.html#poisson-data-model-1",
    "href": "files/lectures/06-1-gamma-poisson.html#poisson-data-model-1",
    "title": "Gamma-Poisson Model",
    "section": "Poisson Data Model",
    "text": "Poisson Data Model\n\nWe will be taking samples from different days.\n\nWe assume that the daily number of calls may differ from day to day. On each day i,\n\n\nY_i|\\lambda \\sim \\text{Pois}(\\lambda)\n\nThis has a unique pmf for each day (i),\n\nf(y_i|\\lambda) = \\frac{\\lambda^{y_i} e^{-\\lambda}}{y_i!}\n\nBut really, we are interested in the joint information in our sample of n observations.\n\nThe joint pmf gives us this information."
  },
  {
    "objectID": "files/lectures/06-1-gamma-poisson.html#poisson-data-model-2",
    "href": "files/lectures/06-1-gamma-poisson.html#poisson-data-model-2",
    "title": "Gamma-Poisson Model",
    "section": "Poisson Data Model",
    "text": "Poisson Data Model\n\nThe joint pmf for the Poisson,\n\n\n\\begin{align*}\nf\\left(\\overset{\\to}{y_i}|\\lambda\\right) &= \\prod_{i=1}^n f(y_i|\\lambda) \\\\\n&= f(y_1|\\lambda) \\times f(y_2|\\lambda) \\times ... \\times f(y_n|\\lambda) \\\\\n&= \\frac{\\lambda^{y_1}e^{-\\lambda}}{y_1!} \\times \\frac{\\lambda^{y_2}e^{-\\lambda}}{y_2!} \\times ... \\times \\frac{\\lambda^{y_n}e^{-\\lambda}}{y_n!} \\\\\n&= \\frac{\\left( \\lambda^{y_1} \\lambda^{y_2} \\cdot \\cdot \\cdot \\ \\lambda^{y_n}  \\right) \\left( e^{-\\lambda} e^{-\\lambda} \\cdot \\cdot \\cdot e^{-\\lambda}\\right)}{y_1! y_2! \\cdot \\cdot \\cdot y_n!} \\\\\n&= \\frac{\\lambda^{\\sum y_i}e^{-n\\lambda}}{\\prod_{i=1}^n y_i !}\n\\end{align*}"
  },
  {
    "objectID": "files/lectures/06-1-gamma-poisson.html#poisson-data-model-3",
    "href": "files/lectures/06-1-gamma-poisson.html#poisson-data-model-3",
    "title": "Gamma-Poisson Model",
    "section": "Poisson Data Model",
    "text": "Poisson Data Model\n\nIf the joint pmf for the Poisson is\n\nf\\left(\\overset{\\to}{y_i}|\\lambda\\right) = \\frac{\\lambda^{\\sum y_i}e^{-n\\lambda}}{\\prod_{i=1}^n y_i !}\n\nthen the likelihood function for \\lambda &gt; 0 is\n\n\n\\begin{align*}\nL\\left(\\lambda|\\overset{\\to}{y_i}\\right) &= \\frac{\\lambda^{\\sum y_i}e^{-n\\lambda}}{\\prod_{i=1}^n y_i !} \\\\\n& \\propto \\lambda^{\\sum y_i} e^{-n\\lambda}\n\\end{align*}\n\n\nPease see page 102 in the textbook for full derivations."
  },
  {
    "objectID": "files/lectures/06-1-gamma-poisson.html#gamma-poisson-conjugacy",
    "href": "files/lectures/06-1-gamma-poisson.html#gamma-poisson-conjugacy",
    "title": "Gamma-Poisson Model",
    "section": "Gamma-Poisson Conjugacy",
    "text": "Gamma-Poisson Conjugacy\n\nLet \\lambda &gt; 0 be an unknown rate parameter and (Y_1, Y_2, ... , Y_n) be an independent sample from the Poisson distribution.\nThe Gamma-Poisson Bayesian model is as follows:\n\n\n\\begin{align*}\nY_i | \\lambda &\\overset{ind}\\sim \\text{Pois}(\\lambda) \\\\\n\\lambda &\\sim \\text{Gamma}(s, r) \\\\\n\\lambda | \\overset{\\to}y &\\sim \\text{Gamma}\\left( s + \\sum y_i, r + n \\right)\n\\end{align*}\n\n\nThe proof can be seen in section 5.2.4 of the textbook."
  },
  {
    "objectID": "files/lectures/06-1-gamma-poisson.html#gamma-poisson-conjugacy-1",
    "href": "files/lectures/06-1-gamma-poisson.html#gamma-poisson-conjugacy-1",
    "title": "Gamma-Poisson Model",
    "section": "Gamma-Poisson Conjugacy",
    "text": "Gamma-Poisson Conjugacy\n\nSuppose we use Gamma(10, 2) as the prior for \\lambda, the daily rate of calls.\nOn four separate days in the second week of August (i.e., independent days), we received \\overset{\\to}y = (6, 2, 2, 1) calls.\nWe will use the plot_poisson_likelihood() function:\n\n\nplot_poisson_likelihood(y = c(6, 2, 2, 1), lambda_upper_bound = 10)\n\n\nNotes:\n\nlambda_upper_bound limits the x axis – recall that \\lambda \\in (0, \\infty)!\nlambda_upper_bound’s default value is 10."
  },
  {
    "objectID": "files/lectures/06-1-gamma-poisson.html#gamma-poisson-conjugacy-2",
    "href": "files/lectures/06-1-gamma-poisson.html#gamma-poisson-conjugacy-2",
    "title": "Gamma-Poisson Model",
    "section": "Gamma-Poisson Conjugacy",
    "text": "Gamma-Poisson Conjugacy\n\n\n\n\n\n\n\n\n\n\n\n\nWe can see that the average is around 2.75.\n\n\nmean(c(6, 2, 2, 1))\n\n[1] 2.75"
  },
  {
    "objectID": "files/lectures/06-1-gamma-poisson.html#gamma-poisson-conjugacy-3",
    "href": "files/lectures/06-1-gamma-poisson.html#gamma-poisson-conjugacy-3",
    "title": "Gamma-Poisson Model",
    "section": "Gamma-Poisson Conjugacy",
    "text": "Gamma-Poisson Conjugacy\n\nWe know our prior distribution is Gamma(10, 2) and the data distribution is Poi(2.75).\nThus, the posterior is as follows,\n\n\n\\begin{align*}\n\\lambda | \\overset{\\to}y &\\sim \\text{Gamma}\\left( s + \\sum y_i, r + n \\right) \\\\\n&\\sim \\text{Gamma}\\left(10 + 11, 2 + 4 \\right) \\\\\n&\\sim \\text{Gamma}\\left(21, 6 \\right)\n\\end{align*}"
  },
  {
    "objectID": "files/lectures/06-1-gamma-poisson.html#the-gamma-posterior-model",
    "href": "files/lectures/06-1-gamma-poisson.html#the-gamma-posterior-model",
    "title": "Gamma-Poisson Model",
    "section": "The Gamma Posterior Model",
    "text": "The Gamma Posterior Model\n\nLooking at just the prior and the data distributions,\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe prior expects more spam calls than what we observed."
  },
  {
    "objectID": "files/lectures/06-1-gamma-poisson.html#the-gamma-posterior-model-1",
    "href": "files/lectures/06-1-gamma-poisson.html#the-gamma-posterior-model-1",
    "title": "Gamma-Poisson Model",
    "section": "The Gamma Posterior Model",
    "text": "The Gamma Posterior Model\n\nNow including the posterior,\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe shape of the posterior has a Gamma(s, r) model.\n\nThe shape and rate parameters (s and r) have been updated."
  },
  {
    "objectID": "files/lectures/06-1-gamma-poisson.html#the-gamma-posterior-model-2",
    "href": "files/lectures/06-1-gamma-poisson.html#the-gamma-posterior-model-2",
    "title": "Gamma-Poisson Model",
    "section": "The Gamma Posterior Model",
    "text": "The Gamma Posterior Model\n\nThe plot_gamma_poisson() function:\n\n\nplot_gamma_poisson(shape = prior_s, rate = prior_r, \n                   sum_y = sum_of_obs, n = sample_size, \n                   posterior = TRUE or FALSE) + \n  theme_bw()"
  },
  {
    "objectID": "files/lectures/06-1-gamma-poisson.html#the-gamma-posterior-model-3",
    "href": "files/lectures/06-1-gamma-poisson.html#the-gamma-posterior-model-3",
    "title": "Gamma-Poisson Model",
    "section": "The Gamma Posterior Model",
    "text": "The Gamma Posterior Model\n\nYour turn! What is different if we had used one of the other priors?\nRecall, we considered\n\nGamma(5, 1)\nGamma(10, 2)\nGamma(15, 3)\nGamma(20, 4)"
  },
  {
    "objectID": "files/lectures/06-1-gamma-poisson.html#the-gamma-posterior-model-4",
    "href": "files/lectures/06-1-gamma-poisson.html#the-gamma-posterior-model-4",
    "title": "Gamma-Poisson Model",
    "section": "The Gamma Posterior Model",
    "text": "The Gamma Posterior Model\n\nYour turn! What is different if we had used Gamma(15, 3) as our prior?"
  },
  {
    "objectID": "files/lectures/06-1-gamma-poisson.html#the-gamma-posterior-model-5",
    "href": "files/lectures/06-1-gamma-poisson.html#the-gamma-posterior-model-5",
    "title": "Gamma-Poisson Model",
    "section": "The Gamma Posterior Model",
    "text": "The Gamma Posterior Model\n\nWe can use the summarize_gamma_poisson() function to summarize the distribution,\n\n\nsummarize_gamma_poisson(shape = 10, rate = 2, sum_y = 11, n = 4)"
  },
  {
    "objectID": "files/lectures/06-1-gamma-poisson.html#wrap-up-gamma-poisson-model",
    "href": "files/lectures/06-1-gamma-poisson.html#wrap-up-gamma-poisson-model",
    "title": "Gamma-Poisson Model",
    "section": "Wrap Up: Gamma-Poisson Model",
    "text": "Wrap Up: Gamma-Poisson Model\n\nWe have built the Gamma-Poisson model for \\lambda, an unknown rate.\n\n\n\\begin{equation*}\n\\begin{aligned}\nY|\\lambda &\\sim \\text{Poi}(\\lambda) \\\\\n\\lambda &\\sim \\text{Gamma}(s, r) &\n\\end{aligned}\n\\Rightarrow\n\\begin{aligned}\n&& \\lambda | y &\\sim \\text{Gamma}(s + \\sum y_i, r + n) \\\\\n\\end{aligned}\n\\end{equation*}\n\n\nThe prior model, f(\\lambda), is given by Gamma(s, r).\nThe data model, f(Y|\\lambda), is given by Poi(\\lambda).\nThe posterior model is a Gamma distribution with updated parameters s+\\sum y_i and r + n."
  },
  {
    "objectID": "files/lectures/06-1-gamma-poisson.html#homework-practice",
    "href": "files/lectures/06-1-gamma-poisson.html#homework-practice",
    "title": "Gamma-Poisson Model",
    "section": "Homework / Practice",
    "text": "Homework / Practice\n\nFrom the Bayes Rules! textbook:\n\n5.3\n5.5\n5.6"
  },
  {
    "objectID": "files/lectures/01-1-probability-theory.html#introduction",
    "href": "files/lectures/01-1-probability-theory.html#introduction",
    "title": "Overview of Probability Theory",
    "section": "Introduction",
    "text": "Introduction\n\nToday we will review very basic probability rules to help us build towards analysis under the Bayeisan framework.\nWe will discuss\n\nBasic terminology\nBasic properties\nTypes of probabilities\nTypes of events"
  },
  {
    "objectID": "files/lectures/01-1-probability-theory.html#terminology-for-probability",
    "href": "files/lectures/01-1-probability-theory.html#terminology-for-probability",
    "title": "Overview of Probability Theory",
    "section": "Terminology for Probability",
    "text": "Terminology for Probability\n\nExperiment: A process that results in one and only one of many possible observations.\nSimple outcomes: The possible results of our experiment.\nSample space: Collection of possible outcomes of the experiment.\nEvent: A collection of one or more of the outcomes of the experiment.\nExample: rolling a die once.\n\nOutcome: the result of the die.\nSample space: {1, 2, 3, 4, 5, 6}\nEvents: rolling an odd; rolling a multiple of 3; rolling a 3 or better."
  },
  {
    "objectID": "files/lectures/01-1-probability-theory.html#properties-of-probability",
    "href": "files/lectures/01-1-probability-theory.html#properties-of-probability",
    "title": "Overview of Probability Theory",
    "section": "Properties of Probability",
    "text": "Properties of Probability\n\nThere are two properties of probability that we must keep at the forefront of our mind.\nFirst, probability always falls between 0 and 1. Mathematically,\n\n0 \\le P[E_i] \\le 1\n\nWhat does p=0 imply?\nWhat does p=0.5 imply?\nWhat does p=1 imply?"
  },
  {
    "objectID": "files/lectures/01-1-probability-theory.html#properties-of-probability-1",
    "href": "files/lectures/01-1-probability-theory.html#properties-of-probability-1",
    "title": "Overview of Probability Theory",
    "section": "Properties of Probability",
    "text": "Properties of Probability\n\nThere are two properties of probability that we must keep at the forefront of our mind.\nSecond, the sum of all simple events for an experiment is always 1. Mathematically,\n\n\\sum_{i=1}^n P[E_i] = P[E_1] + ... + P[E_n] = 1\n\nIf there are 2 events and we know P[E_1]=0.7, what is P[E_2]? \nIf there are 4 events and we know P[E_1]=P[E_2]=0.1, P[E_3]=0.6, what is P[E_4]?"
  },
  {
    "objectID": "files/lectures/01-1-probability-theory.html#assigning-probabilities",
    "href": "files/lectures/01-1-probability-theory.html#assigning-probabilities",
    "title": "Overview of Probability Theory",
    "section": "Assigning Probabilities",
    "text": "Assigning Probabilities\n\nHow do we assign probabilities?\n\nSubjective probability\nClassicial probability rule\nRelative frequency"
  },
  {
    "objectID": "files/lectures/01-1-probability-theory.html#subjective-probability",
    "href": "files/lectures/01-1-probability-theory.html#subjective-probability",
    "title": "Overview of Probability Theory",
    "section": "Subjective Probability",
    "text": "Subjective Probability\n\nSubjective probability is the probability assigned to an event based on subjective judgement, experience, information, and belief.\nExamples:\n\nP[UWF wins national championship]\nP[tomato plant eaten by hornworms]\nP[A in this course]"
  },
  {
    "objectID": "files/lectures/01-1-probability-theory.html#classical-probability",
    "href": "files/lectures/01-1-probability-theory.html#classical-probability",
    "title": "Overview of Probability Theory",
    "section": "Classical Probability",
    "text": "Classical Probability\n\nLet A be an event for an experiment with equally likely outcomes,\n\nP[A] = \\frac{\\text{Number of outcomes favorable to $A$}}{\\text{Total number of outcomes for the experiment}}\n\nExamples:\n\nP[2 heads on 3 coin tosses]\nP[at least 2 heads on 4 coin tosses]\nP[even when rolling die]"
  },
  {
    "objectID": "files/lectures/01-1-probability-theory.html#relative-frequency",
    "href": "files/lectures/01-1-probability-theory.html#relative-frequency",
    "title": "Overview of Probability Theory",
    "section": "Relative Frequency",
    "text": "Relative Frequency\n\nIf an experiment is repeated n times and an event A is observed f times, then\n\nP[A] = \\frac{f}{n} = \\frac{\\text{Frequency of $A$}}{\\text{Sample size}}\n\nExample:\n\nP[car is a lemon] given 10/500 sampled cars from a factory are lemons.\nP[person is a homeowner] given 730/1000 sampled individuals own a home."
  },
  {
    "objectID": "files/lectures/01-1-probability-theory.html#contingency-tables",
    "href": "files/lectures/01-1-probability-theory.html#contingency-tables",
    "title": "Overview of Probability Theory",
    "section": "Contingency Tables",
    "text": "Contingency Tables\n\nSuppose 100 employees at Target were asked whether they are in favor of or against extending store hours during the holiday season.\n\n\n\n\n\n\n\n\nDepartment\n\n\nIn Favor\n\n\nAgainst\n\n\nTotal\n\n\n\n\nElectronics\n\n\n12\n\n\n8\n\n\n20\n\n\n\n\nClothing\n\n\n18\n\n\n12\n\n\n30\n\n\n\n\nGrocery\n\n\n25\n\n\n15\n\n\n40\n\n\n\n\nCustomer Service\n\n\n5\n\n\n5\n\n\n10\n\n\n\n\nTotal\n\n\n60\n\n\n40\n\n\n100"
  },
  {
    "objectID": "files/lectures/01-1-probability-theory.html#marginal-probability",
    "href": "files/lectures/01-1-probability-theory.html#marginal-probability",
    "title": "Overview of Probability Theory",
    "section": "Marginal Probability",
    "text": "Marginal Probability\n\nA marginal probability is the probability of a single event occurring without considering any other variables.\n\nIn a contingency table, marginal probabilities are found outside the body of the table.\n\nIt tells us the likelihood of one category happening overall, regardless of how it combines (or interacts) with other categories.\nIn our Target example:\n\nWhat is the probability that a randomly selected employee is in favor?\nWhat is the probability that randomly selected employee works in the Grocery department?"
  },
  {
    "objectID": "files/lectures/01-1-probability-theory.html#marginal-probability-1",
    "href": "files/lectures/01-1-probability-theory.html#marginal-probability-1",
    "title": "Overview of Probability Theory",
    "section": "Marginal Probability",
    "text": "Marginal Probability\n\nRecall,\n\n\n\n\n\n\n\n\nDepartment\n\n\nIn Favor\n\n\nAgainst\n\n\nTotal\n\n\n\n\nElectronics\n\n\n12\n\n\n8\n\n\n20\n\n\n\n\nClothing\n\n\n18\n\n\n12\n\n\n30\n\n\n\n\nGrocery\n\n\n25\n\n\n15\n\n\n40\n\n\n\n\nCustomer Service\n\n\n5\n\n\n5\n\n\n10\n\n\n\n\nTotal\n\n\n60\n\n\n40\n\n\n100\n\n\n\n\n\n\n\nWhat is the probability that a randomly selected employee is in favor?\nWhat is the probability that randomly selected employee works in the Grocery department?"
  },
  {
    "objectID": "files/lectures/01-1-probability-theory.html#joint-probability",
    "href": "files/lectures/01-1-probability-theory.html#joint-probability",
    "title": "Overview of Probability Theory",
    "section": "Joint Probability",
    "text": "Joint Probability\n\nThe joint probability is the probability that two events happen at the same time.\n\nIn a contingency table, joint probabilities are found inside the body of the table.\n\nIt tells us the likelihood that a randomly selected observation falls into both categories simultaneously.\nIn our Target example:\n\nWhat is the probability that an employee is in the Grocery department and in favor of extended hours?\nWhat is the probability that an employee is in Electronics and against extended hours?"
  },
  {
    "objectID": "files/lectures/01-1-probability-theory.html#joint-probability-1",
    "href": "files/lectures/01-1-probability-theory.html#joint-probability-1",
    "title": "Overview of Probability Theory",
    "section": "Joint Probability",
    "text": "Joint Probability\n\nRecall,\n\n\n\n\n\n\n\n\nDepartment\n\n\nIn Favor\n\n\nAgainst\n\n\nTotal\n\n\n\n\nElectronics\n\n\n12\n\n\n8\n\n\n20\n\n\n\n\nClothing\n\n\n18\n\n\n12\n\n\n30\n\n\n\n\nGrocery\n\n\n25\n\n\n15\n\n\n40\n\n\n\n\nCustomer Service\n\n\n5\n\n\n5\n\n\n10\n\n\n\n\nTotal\n\n\n60\n\n\n40\n\n\n100\n\n\n\n\n\n\n\nWhat is the probability that an employee is in the Grocery department and in favor of extended hours?\nWhat is the probability that an employee is in Electronics and against extended hours?"
  },
  {
    "objectID": "files/lectures/01-1-probability-theory.html#conditional-probability",
    "href": "files/lectures/01-1-probability-theory.html#conditional-probability",
    "title": "Overview of Probability Theory",
    "section": "Conditional Probability",
    "text": "Conditional Probability\n\nConditional probability is the probability that one event occurs given that we already know another event has occurred.\n\n“What is the probability of Event A if we know Event B is true?”\nIn a contingency table, conditional probabilities are found by limiting yourself to a specific row or column of the table, then finding the corresponding probability.\n\nIn our Target example,\n\nWhat is the probability that an employee is in favor of extended hours given that they work in the Grocery department?\nWhat is the probability that an employee works in the Electronics department given that they are against extended hours?"
  },
  {
    "objectID": "files/lectures/01-1-probability-theory.html#conditional-probability-1",
    "href": "files/lectures/01-1-probability-theory.html#conditional-probability-1",
    "title": "Overview of Probability Theory",
    "section": "Conditional Probability",
    "text": "Conditional Probability\n\nRecall,\n\n\n\n\n\n\n\n\nDepartment\n\n\nIn Favor\n\n\nAgainst\n\n\nTotal\n\n\n\n\nElectronics\n\n\n12\n\n\n8\n\n\n20\n\n\n\n\nClothing\n\n\n18\n\n\n12\n\n\n30\n\n\n\n\nGrocery\n\n\n25\n\n\n15\n\n\n40\n\n\n\n\nCustomer Service\n\n\n5\n\n\n5\n\n\n10\n\n\n\n\nTotal\n\n\n60\n\n\n40\n\n\n100\n\n\n\n\n\n\n\nWhat is the probability that an employee is in favor of extended hours given that they work in the Grocery department?\nWhat is the probability that an employee works in the Electronics department given that they are against extended hours?"
  },
  {
    "objectID": "files/lectures/01-1-probability-theory.html#mutually-exclusive-events",
    "href": "files/lectures/01-1-probability-theory.html#mutually-exclusive-events",
    "title": "Overview of Probability Theory",
    "section": "Mutually Exclusive Events",
    "text": "Mutually Exclusive Events\n\nEvents that cannot occur together are mutually exclusive or disjoint.\nConsider rolling a single die:\n\nA = an even number = {2, 4, 6}\nB = an odd number = {1, 3, 5}\nC = a number less than 5 = {1, 2, 3, 4}\n\nAre events A and B mutually exclusive?\nAre events A and C mutually exclusive?"
  },
  {
    "objectID": "files/lectures/01-1-probability-theory.html#mutually-exclusive-events-1",
    "href": "files/lectures/01-1-probability-theory.html#mutually-exclusive-events-1",
    "title": "Overview of Probability Theory",
    "section": "Mutually Exclusive Events",
    "text": "Mutually Exclusive Events\n\nConsider rolling a single die:\n\nA = an even number = {2, 4, 6}\nB = an odd number = {1, 3, 5}\nC = a number less than 5 = {1, 2, 3, 4}\n\nAre events A and B mutually exclusive?"
  },
  {
    "objectID": "files/lectures/01-1-probability-theory.html#mutually-exclusive-events-2",
    "href": "files/lectures/01-1-probability-theory.html#mutually-exclusive-events-2",
    "title": "Overview of Probability Theory",
    "section": "Mutually Exclusive Events",
    "text": "Mutually Exclusive Events\n\nConsider rolling a single die:\n\nA = an even number = {2, 4, 6}\nB = an odd number = {1, 3, 5}\nC = a number less than 5 = {1, 2, 3, 4}\n\nAre events A and C mutually exclusive?"
  },
  {
    "objectID": "files/lectures/01-1-probability-theory.html#independent-events",
    "href": "files/lectures/01-1-probability-theory.html#independent-events",
    "title": "Overview of Probability Theory",
    "section": "Independent Events",
    "text": "Independent Events\n\nTwo events are said to be independent if the occurrence of one event does not affect the probability of the occurrence of the other event.\nMathematically,\n\n P[A|B] = P[A] \\text{ or } P[B|A] = P[B]\n\nIn our Target example, is department independent of being in favor of extended hours?\n\nWe are being asked to examine P[department|favor] or P[favor|department]."
  },
  {
    "objectID": "files/lectures/01-1-probability-theory.html#independent-events-1",
    "href": "files/lectures/01-1-probability-theory.html#independent-events-1",
    "title": "Overview of Probability Theory",
    "section": "Independent Events",
    "text": "Independent Events\n\nRecall,\n\n\n\n\n\n\n\n\nDepartment\n\n\nIn Favor\n\n\nAgainst\n\n\nTotal\n\n\n\n\nElectronics\n\n\n12\n\n\n8\n\n\n20\n\n\n\n\nClothing\n\n\n18\n\n\n12\n\n\n30\n\n\n\n\nGrocery\n\n\n25\n\n\n15\n\n\n40\n\n\n\n\nCustomer Service\n\n\n5\n\n\n5\n\n\n10\n\n\n\n\nTotal\n\n\n60\n\n\n40\n\n\n100\n\n\n\n\n\n\n\nWe need to examine P[department|favor] or P[favor|department]. We say that they are independent if\n\nP[department|favor] = P[department]\nP[favor|department] = P[favor]"
  },
  {
    "objectID": "files/lectures/01-1-probability-theory.html#complementary-events",
    "href": "files/lectures/01-1-probability-theory.html#complementary-events",
    "title": "Overview of Probability Theory",
    "section": "Complementary Events",
    "text": "Complementary Events\n\nThe complement of A is the event that includes all the outcomes that are not in A.\n\n\n\n\n\nMathematically,\n\n\n\\begin{align*}\nP[A] &+ P[A^c] = 1 \\\\\nP[A] &= 1 - P[A^c] \\\\\nP[A^c] &= 1 - P[A]\n\\end{align*}"
  },
  {
    "objectID": "files/lectures/01-1-probability-theory.html#complementary-events-1",
    "href": "files/lectures/01-1-probability-theory.html#complementary-events-1",
    "title": "Overview of Probability Theory",
    "section": "Complementary Events",
    "text": "Complementary Events\n\nRecall,\n\n\n\n\n\n\n\n\nDepartment\n\n\nIn Favor\n\n\nAgainst\n\n\nTotal\n\n\n\n\nElectronics\n\n\n12\n\n\n8\n\n\n20\n\n\n\n\nClothing\n\n\n18\n\n\n12\n\n\n30\n\n\n\n\nGrocery\n\n\n25\n\n\n15\n\n\n40\n\n\n\n\nCustomer Service\n\n\n5\n\n\n5\n\n\n10\n\n\n\n\nTotal\n\n\n60\n\n\n40\n\n\n100\n\n\n\n\n\n\n\nFind P[Electronicsc].\nFind P[Electronicsc | Against]."
  },
  {
    "objectID": "files/lectures/01-1-probability-theory.html#wrap-up",
    "href": "files/lectures/01-1-probability-theory.html#wrap-up",
    "title": "Overview of Probability Theory",
    "section": "Wrap Up",
    "text": "Wrap Up\n\nToday we have reviewed probability basics.\n\nNote that this is no replacement for Advanced Probability :)\n\nWe just need to understand general concepts to move foward.\nNext week:\n\nMonday: no class meeting – instead, you will meet with your EES collaborator (at an agreed upon time/date).\n\nPairing document with contact information is linked on Discord. We now have a “EES project” channel.\n\nWednesday: probability distributions"
  },
  {
    "objectID": "files/lectures/04-2-thinking-bayesian.html#introduction",
    "href": "files/lectures/04-2-thinking-bayesian.html#introduction",
    "title": "Thinking Like a Bayesian",
    "section": "Introduction",
    "text": "Introduction\n\nBefore today:\n\nRefresher on probability theory\nWhat does each distribution do?\n\nbeta \\to outcomes limited to [0, 1]\nbinomial \\to binary outcomes\ngamma \\to continuous & positive outcomes; skewed right\nnormal \\to continuous outcomes; mound-shaped & symmetric distribution\nPoisson \\to count outcomes; skewed right\nuniform \\to each outcome has equal probability; rectangular distribution\n\n\nToday: building up Bayesian analysis concepts"
  },
  {
    "objectID": "files/lectures/04-2-thinking-bayesian.html#thinking-like-a-bayesian",
    "href": "files/lectures/04-2-thinking-bayesian.html#thinking-like-a-bayesian",
    "title": "Thinking Like a Bayesian",
    "section": "Thinking Like a Bayesian",
    "text": "Thinking Like a Bayesian\n\nBayesian analysis involves updating beliefs based on observed data."
  },
  {
    "objectID": "files/lectures/04-2-thinking-bayesian.html#thinking-like-a-bayesian-1",
    "href": "files/lectures/04-2-thinking-bayesian.html#thinking-like-a-bayesian-1",
    "title": "Thinking Like a Bayesian",
    "section": "Thinking Like a Bayesian",
    "text": "Thinking Like a Bayesian\n\nThis is the natural Bayesian knowledge-building process of:\n\nacknowledging your preconceptions (prior distribution),\nusing data (data distribution) to update your knowledge (posterior distribution), and\nrepeating (posterior distribution \\to new prior distribution)"
  },
  {
    "objectID": "files/lectures/04-2-thinking-bayesian.html#thinking-like-a-bayesian-2",
    "href": "files/lectures/04-2-thinking-bayesian.html#thinking-like-a-bayesian-2",
    "title": "Thinking Like a Bayesian",
    "section": "Thinking Like a Bayesian",
    "text": "Thinking Like a Bayesian\n\nBayesian and frequentist analyses share a common goal: to learn from data about the world around us.\n\nBoth Bayesian and frequentist analyses use data to fit models, make predictions, and evaluate hypotheses.\nWhen working with the same data, they will typically produce a similar set of conclusions.\n\nStatisticians typically identify as either a “Bayesian” or “frequentist” …\n\n🚫 We are not going to “take sides.”\n✅ We will see these as tools in our toolbox."
  },
  {
    "objectID": "files/lectures/04-2-thinking-bayesian.html#thinking-like-a-bayesian-3",
    "href": "files/lectures/04-2-thinking-bayesian.html#thinking-like-a-bayesian-3",
    "title": "Thinking Like a Bayesian",
    "section": "Thinking Like a Bayesian",
    "text": "Thinking Like a Bayesian\n\nBayesian probability: the relative plausibility of an event.\n\nConsiders prior belief."
  },
  {
    "objectID": "files/lectures/04-2-thinking-bayesian.html#thinking-like-a-bayesian-4",
    "href": "files/lectures/04-2-thinking-bayesian.html#thinking-like-a-bayesian-4",
    "title": "Thinking Like a Bayesian",
    "section": "Thinking Like a Bayesian",
    "text": "Thinking Like a Bayesian\n\nFrequentist probability: the long-run relative frequency of a repeatable event.\n\nDoes not consider prior belief."
  },
  {
    "objectID": "files/lectures/04-2-thinking-bayesian.html#thinking-like-a-bayesian-5",
    "href": "files/lectures/04-2-thinking-bayesian.html#thinking-like-a-bayesian-5",
    "title": "Thinking Like a Bayesian",
    "section": "Thinking Like a Bayesian",
    "text": "Thinking Like a Bayesian\n\nThe Bayesian framework depends upon prior information, data, and the balance between them.\n\nThe balance between the prior information and data is determined by the relative strength of each\n\n\n\n\nWhen we have little data, our posterior can rely more on prior knowledge.\nAs we collect more data, the prior can lose its influence."
  },
  {
    "objectID": "files/lectures/04-2-thinking-bayesian.html#thinking-like-a-bayesian-6",
    "href": "files/lectures/04-2-thinking-bayesian.html#thinking-like-a-bayesian-6",
    "title": "Thinking Like a Bayesian",
    "section": "Thinking Like a Bayesian",
    "text": "Thinking Like a Bayesian\n\nWe can also use this approach to combine analysis results."
  },
  {
    "objectID": "files/lectures/04-2-thinking-bayesian.html#thinking-like-a-bayesian-7",
    "href": "files/lectures/04-2-thinking-bayesian.html#thinking-like-a-bayesian-7",
    "title": "Thinking Like a Bayesian",
    "section": "Thinking Like a Bayesian",
    "text": "Thinking Like a Bayesian\n\nWe will use an example to work through Bayesian logic.\nThe Collins Dictionary named “fake news” the 2017 term of the year.\n\nFake, misleading, and biased news has proliferated along with online news and social media platforms which allow users to post articles with little quality control.\n\nWe want to flag articles as “real” or “fake.”\nWe’ll examine a sample of 150 articles which were posted on Facebook and fact checked by five BuzzFeed journalists (Shu et al. 2017)."
  },
  {
    "objectID": "files/lectures/04-2-thinking-bayesian.html#thinking-like-a-bayesian-8",
    "href": "files/lectures/04-2-thinking-bayesian.html#thinking-like-a-bayesian-8",
    "title": "Thinking Like a Bayesian",
    "section": "Thinking Like a Bayesian",
    "text": "Thinking Like a Bayesian\n\nInformation about each article is stored in the fake_news dataset in the bayesrules package.\n\n\nfake_news &lt;- bayesrules::fake_news\nprint(colnames(fake_news))\n\n [1] \"title\"                   \"text\"                   \n [3] \"url\"                     \"authors\"                \n [5] \"type\"                    \"title_words\"            \n [7] \"text_words\"              \"title_char\"             \n [9] \"text_char\"               \"title_caps\"             \n[11] \"text_caps\"               \"title_caps_percent\"     \n[13] \"text_caps_percent\"       \"title_excl\"             \n[15] \"text_excl\"               \"title_excl_percent\"     \n[17] \"text_excl_percent\"       \"title_has_excl\"         \n[19] \"anger\"                   \"anticipation\"           \n[21] \"disgust\"                 \"fear\"                   \n[23] \"joy\"                     \"sadness\"                \n[25] \"surprise\"                \"trust\"                  \n[27] \"negative\"                \"positive\"               \n[29] \"text_syllables\"          \"text_syllables_per_word\""
  },
  {
    "objectID": "files/lectures/04-2-thinking-bayesian.html#thinking-like-a-bayesian-9",
    "href": "files/lectures/04-2-thinking-bayesian.html#thinking-like-a-bayesian-9",
    "title": "Thinking Like a Bayesian",
    "section": "Thinking Like a Bayesian",
    "text": "Thinking Like a Bayesian\n\nWe could build a simple news filter which uses the following rule: since most articles are real, we should read and believe all articles.\n\nWhile this filter would solve the problem of disregarding real articles, we would read lots of fake news.\nIt also only takes into account the overall rates of, not the typical features of, real and fake news."
  },
  {
    "objectID": "files/lectures/04-2-thinking-bayesian.html#thinking-like-a-bayesian-10",
    "href": "files/lectures/04-2-thinking-bayesian.html#thinking-like-a-bayesian-10",
    "title": "Thinking Like a Bayesian",
    "section": "Thinking Like a Bayesian",
    "text": "Thinking Like a Bayesian\n\nSuppose that the most recent article posted to a social media platform is titled: The president has a funny secret!\n\nSome features of this title probably set off some red flags.\nFor example, the usage of an exclamation point might seem like an odd choice for a real news article.\n\nIn the dataset, what is the split of real and fake articles?"
  },
  {
    "objectID": "files/lectures/04-2-thinking-bayesian.html#thinking-like-a-bayesian-11",
    "href": "files/lectures/04-2-thinking-bayesian.html#thinking-like-a-bayesian-11",
    "title": "Thinking Like a Bayesian",
    "section": "Thinking Like a Bayesian",
    "text": "Thinking Like a Bayesian\n\nIn the dataset, what is the split of real and fake articles?\n\n\n\n\n  \n\n\n\n\nOur data backs up our instinct on the article,\n\n\n\n\n  \n\n\n\n\nIn this dataset, 26.67% (16 of 60) of fake news titles but only 2.22% (2 of 90) of real news titles use an exclamation point."
  },
  {
    "objectID": "files/lectures/04-2-thinking-bayesian.html#thinking-like-a-bayesian-12",
    "href": "files/lectures/04-2-thinking-bayesian.html#thinking-like-a-bayesian-12",
    "title": "Thinking Like a Bayesian",
    "section": "Thinking Like a Bayesian",
    "text": "Thinking Like a Bayesian\n\nWe now have two pieces of contradictory information.\n\nOur prior information suggested that incoming articles are most likely real.\nHowever, the exclamation point data is more consistent with fake news.\n\n\n\n\nThinking like Bayesians, we know that balancing both pieces of information is important in developing a posterior understanding of whether the article is fake."
  },
  {
    "objectID": "files/lectures/04-2-thinking-bayesian.html#building-a-bayesian-model",
    "href": "files/lectures/04-2-thinking-bayesian.html#building-a-bayesian-model",
    "title": "Thinking Like a Bayesian",
    "section": "Building a Bayesian Model",
    "text": "Building a Bayesian Model\n\nOur fake news analysis studies two variables:\n\nan article’s fake vs real status and\nits use of exclamation points.\n\nWe can represent the randomness in these variables using probability models.\nWe will now build:\n\na prior probability model for our prior understanding of whether the most recent article is fake;\na model for interpreting the exclamation point data; and, eventually,\na posterior probability model which summarizes the posterior plausibility that the article is fake."
  },
  {
    "objectID": "files/lectures/04-2-thinking-bayesian.html#building-a-bayesian-model-1",
    "href": "files/lectures/04-2-thinking-bayesian.html#building-a-bayesian-model-1",
    "title": "Thinking Like a Bayesian",
    "section": "Building a Bayesian Model",
    "text": "Building a Bayesian Model\n\nLet’s now formalize our prior understanding of whether the new article is fake.\nBased on our fake_news data, we saw that 40% of articles are fake and 60% are real.\n\nBefore reading the new article, there’s a 0.4 prior probability that it’s fake and a 0.6 prior probability it’s not.\n\n\nP\\left[B\\right] = 0.40 \\text{ and } P\\left[B^c\\right] = 0.60\n\nRemember that a valid probability model must:\n\naccount for all possible events (all articles must be fake or real);\nit assigns prior probabilities to each event; and\nthe probabilities sum to one."
  },
  {
    "objectID": "files/lectures/04-2-thinking-bayesian.html#building-a-bayesian-model-2",
    "href": "files/lectures/04-2-thinking-bayesian.html#building-a-bayesian-model-2",
    "title": "Thinking Like a Bayesian",
    "section": "Building a Bayesian Model",
    "text": "Building a Bayesian Model\n\n\n\n\n\n\n\n\ntitle_has_excl\nfake\nreal\n\n\n\n\nFALSE\n73.3% (44)\n97.8% (88)\n\n\nTRUE\n26.7% (16)\n2.2% (2)\n\n\n\n\n\n\n\n\nWe have the following conditional probabilities:\n\nIf an article is fake (B), then there’s a roughly 26.67% chance it uses exclamation points in the title.\nIf an article is real (B^c), then there’s only a roughly 2.22% chance it uses exclamation points."
  },
  {
    "objectID": "files/lectures/04-2-thinking-bayesian.html#building-a-bayesian-model-3",
    "href": "files/lectures/04-2-thinking-bayesian.html#building-a-bayesian-model-3",
    "title": "Thinking Like a Bayesian",
    "section": "Building a Bayesian Model",
    "text": "Building a Bayesian Model\n\nWe have the following conditional probabilities:\n\nIf an article is fake (B), then there’s a roughly 26.67% chance it uses exclamation points in the title.\nIf an article is real (B^c), then there’s only a roughly 2.22% chance it uses exclamation points.\n\nLooking at the probabilities, we can see that 26.67% of fake articles vs. 2.22% of real articles use exclamation points.\n\nExclamation point usage is much more likely among fake news than real news.\nWe have evidence that the article is fake."
  },
  {
    "objectID": "files/lectures/04-2-thinking-bayesian.html#building-a-bayesian-model-4",
    "href": "files/lectures/04-2-thinking-bayesian.html#building-a-bayesian-model-4",
    "title": "Thinking Like a Bayesian",
    "section": "Building a Bayesian Model",
    "text": "Building a Bayesian Model\n\nNote that we know that the incoming article used exclamation points (A), but we do not actually know if the article is fake (B or B^c).\nIn this case, we compared P[A|B] and P[A|B^c] to ascertain the relative likelihood of observing A under different scenarios.\n\nL\\left[B|A\\right] = P\\left[A|B\\right] \\text{ and } L\\left[B^c|A\\right] = P\\left[A|B^c\\right]"
  },
  {
    "objectID": "files/lectures/04-2-thinking-bayesian.html#building-a-bayesian-model-5",
    "href": "files/lectures/04-2-thinking-bayesian.html#building-a-bayesian-model-5",
    "title": "Thinking Like a Bayesian",
    "section": "Building a Bayesian Model",
    "text": "Building a Bayesian Model\n\n\n\nEvent\nB\nBc\nTotal\n\n\n\n\nPrior Probability\n0.4\n0.6\n1.0\n\n\nLikelihood\n0.2667\n0.0222\n0.2889\n\n\n\nL\\left[B|A\\right] = P\\left[A|B\\right] \\text{ and } L\\left[B^c|A\\right] = P\\left[A|B^c\\right]\n\nIt is important for us to note that the likelihood function is not a probability function.\n\nThis is a framework to compare the relative comparability of our exclamation point data with B and B^c."
  },
  {
    "objectID": "files/lectures/04-2-thinking-bayesian.html#building-a-bayesian-model-6",
    "href": "files/lectures/04-2-thinking-bayesian.html#building-a-bayesian-model-6",
    "title": "Thinking Like a Bayesian",
    "section": "Building a Bayesian Model",
    "text": "Building a Bayesian Model\n\n\n\nEvent\nB (fake)\nBc (real)\nTotal\n\n\n\n\nPrior Probability\n0.4\n0.6\n1.0\n\n\nLikelihood\n0.2667\n0.0222\n0.2889\n\n\n\n\nThe prior evidence suggested the article is most likely real,\n\nP[B] = 0.4 &lt; P[B^c] = 0.6\n\nThe data, however, is more consistent with the article being fake,\n\nL[B|A] = 0.2667 &gt; L[B^c|A] = 0.0222"
  },
  {
    "objectID": "files/lectures/04-2-thinking-bayesian.html#building-a-bayesian-model-7",
    "href": "files/lectures/04-2-thinking-bayesian.html#building-a-bayesian-model-7",
    "title": "Thinking Like a Bayesian",
    "section": "Building a Bayesian Model",
    "text": "Building a Bayesian Model\n\nWe can summarize our probabilities in a table,\n\n\n\n\n\nB\nB^c\nTotal\n\n\n\n\nA\n\n\n\n\n\nA^c\n\n\n\n\n\nTotal\n0.4\n0.6\n1\n\n\n\n\nAs found earlier, P[A|B] = 0.2667 and P[A|B^c]=0.0222.\n\n\n\\begin{align*}\nP[A \\cap B] &= P[A|B] \\times P[B] \\\\\n&= 0.2667 \\times 0.4 \\\\\n&= 0.1067\n\\end{align*}"
  },
  {
    "objectID": "files/lectures/04-2-thinking-bayesian.html#building-a-bayesian-model-8",
    "href": "files/lectures/04-2-thinking-bayesian.html#building-a-bayesian-model-8",
    "title": "Thinking Like a Bayesian",
    "section": "Building a Bayesian Model",
    "text": "Building a Bayesian Model\n\nHere’s what we know,\n\n\n\n\n\nB\nB^c\nTotal\n\n\n\n\nA\n0.1067\n\n\n\n\nA^c\n\n\n\n\n\nTotal\n0.4\n0.6\n1\n\n\n\n\nWe also know P[A|B] = 0.2667 and P[A|B^c]=0.0222.\n\n\n\\begin{align*}\nP[A^c \\cap B] &= P(A^c|B) \\times P[B]  \\\\\n&= (1-P[A|B]) \\times P[B] \\\\\n&= (1-0.2667) \\times 0.4 \\\\\n&= 0.2933\n\\end{align*}"
  },
  {
    "objectID": "files/lectures/04-2-thinking-bayesian.html#building-a-bayesian-model-9",
    "href": "files/lectures/04-2-thinking-bayesian.html#building-a-bayesian-model-9",
    "title": "Thinking Like a Bayesian",
    "section": "Building a Bayesian Model",
    "text": "Building a Bayesian Model\n\nHere’s what we know,\n\n\n\n\n\nB\nB^c\nTotal\n\n\n\n\nA\n0.1067\n\n\n\n\nA^c\n0.2933\n\n\n\n\nTotal\n0.4\n0.6\n1\n\n\n\n\nWe also know P[A|B] = 0.2667 and P[A|B^c]=0.0222.\n\n\n\\begin{align*}\nP[A \\cap B^c] &= P[A|B^c] \\times P[B^c]  \\\\\n&= 0.0222 \\times 0.6 \\\\\n&= 0.0133\n\\end{align*}"
  },
  {
    "objectID": "files/lectures/04-2-thinking-bayesian.html#building-a-bayesian-model-10",
    "href": "files/lectures/04-2-thinking-bayesian.html#building-a-bayesian-model-10",
    "title": "Thinking Like a Bayesian",
    "section": "Building a Bayesian Model",
    "text": "Building a Bayesian Model\n\nHere’s what we know,\n\n\n\n\n\nB\nB^c\nTotal\n\n\n\n\nA\n0.1067\n0.0133\n\n\n\nA^c\n0.2933\n\n\n\n\nTotal\n0.4\n0.6\n1\n\n\n\n\nWe also know P[A|B] = 0.2667 and P[A|B^c]=0.0222.\n\n\n\\begin{align*}\nP[A^c \\cap B^c] &= P[A^c|B^c] \\times P[B^c]  \\\\\n&= 0.9778 \\times 0.6 \\\\\n&= 0.5867\n\\end{align*}"
  },
  {
    "objectID": "files/lectures/04-2-thinking-bayesian.html#building-a-bayesian-model-11",
    "href": "files/lectures/04-2-thinking-bayesian.html#building-a-bayesian-model-11",
    "title": "Thinking Like a Bayesian",
    "section": "Building a Bayesian Model",
    "text": "Building a Bayesian Model\n\nHere’s what we know,\n\n\n\n\n\nB\nB^c\nTotal\n\n\n\n\nA\n0.1067\n0.0133\n\n\n\nA^c\n0.2933\n0.5867\n\n\n\nTotal\n0.4\n0.6\n1\n\n\n\n\nFinally,\n\n\n\\begin{align*}\n&P[A] = 0.1067 + 0.0133 = 0.12 \\\\\n&P[A^c] = 0.2933 + 0.5867 = 0.88\n\\end{align*}"
  },
  {
    "objectID": "files/lectures/04-2-thinking-bayesian.html#building-a-bayesian-model-12",
    "href": "files/lectures/04-2-thinking-bayesian.html#building-a-bayesian-model-12",
    "title": "Thinking Like a Bayesian",
    "section": "Building a Bayesian Model",
    "text": "Building a Bayesian Model\n\nUsing rules of probability, we have completed the table.\n\n\n\n\n\nB\nB^c\nTotal\n\n\n\n\nA\n0.1067\n0.0133\n0.12\n\n\nA^c\n0.2933\n0.5867\n0.88\n\n\nTotal\n0.4\n0.6\n1"
  },
  {
    "objectID": "files/lectures/04-2-thinking-bayesian.html#building-a-bayesian-model-13",
    "href": "files/lectures/04-2-thinking-bayesian.html#building-a-bayesian-model-13",
    "title": "Thinking Like a Bayesian",
    "section": "Building a Bayesian Model",
    "text": "Building a Bayesian Model\n\nWith more information, we can answer the question: what is the probability that the latest article is fake?\nWe will use the posterior probability, P[B|A], which is found using Bayes’ Rule.\nBayes’ Rule: For events A and B,\n\nP[B|A] = \\frac{P[A \\cap B]}{P[A]} = \\frac{P[B] \\times L[B|A]}{P[A]}\n\nBut really, we can think about it like this,\n\n\\text{posterior} = \\frac{\\text{prior} \\times \\text{likelihood}}{\\text{normalizing constant}}"
  },
  {
    "objectID": "files/lectures/04-2-thinking-bayesian.html#example-set-up",
    "href": "files/lectures/04-2-thinking-bayesian.html#example-set-up",
    "title": "Thinking Like a Bayesian",
    "section": "Example Set Up",
    "text": "Example Set Up\n\nIn 1996, Gary Kasparov played a six-game chess match against the IBM supercomputer Deep Blue.\n\nOf the six games, Kasparov won three, drew two, and lost one.\nThus, Kasparov won the overall match.\n\nKasparov and Deep Blue were to meet again for a six-game match in 1997.\nLet \\pi denote Kasparov’s chances of winning any particular game in the re-match.\n\nThus, \\pi is a measure of his overall skill relative to Deep Blue.\nGiven the complexity of chess, machines, and humans, \\pi is unknown and can vary over time.\n\ni.e., \\pi is a random variable."
  },
  {
    "objectID": "files/lectures/04-2-thinking-bayesian.html#example-set-up-1",
    "href": "files/lectures/04-2-thinking-bayesian.html#example-set-up-1",
    "title": "Thinking Like a Bayesian",
    "section": "Example Set Up",
    "text": "Example Set Up\n\nOur first step is to start with a prior model. This model\n\nIdentifies what values \\pi can take,\nassigns a prior weight or probability to each, and\nthese probabilities sum to 1.\n\nBased on what we were told, the prior model for \\pi in our example,\n\n\n\n\n\\pi\n0.2\n0.5\n0.8\nTotal\n\n\n\n\nf(\\pi)\n0.10\n0.25\n0.65\n1"
  },
  {
    "objectID": "files/lectures/04-2-thinking-bayesian.html#example-set-up-2",
    "href": "files/lectures/04-2-thinking-bayesian.html#example-set-up-2",
    "title": "Thinking Like a Bayesian",
    "section": "Example Set Up",
    "text": "Example Set Up\n\nBased on what we were told, the prior model for \\pi in our example,\n\n\n\n\n\\pi\n0.2\n0.5\n0.8\nTotal\n\n\n\n\nf(\\pi)\n0.10\n0.25\n0.65\n1\n\n\n\n\nNote that this is an incredibly simple model.\n\nThe win probability can technically be any number \\in [0, 1].\nHowever, this prior assumes that \\pi has a discrete set of possibilities: 20%, 50%, or 80%."
  },
  {
    "objectID": "files/lectures/04-2-thinking-bayesian.html#example-set-up-3",
    "href": "files/lectures/04-2-thinking-bayesian.html#example-set-up-3",
    "title": "Thinking Like a Bayesian",
    "section": "Example Set Up",
    "text": "Example Set Up\n\nIn the second step of our analysis, we collect and process data which can inform our understanding of \\pi.\nHere, Y = the number of the six games in the 1997 re-match that Kasparov wins.\n\nAs chess match outcome isn’t predetermined, Y is a random variable that can take any value in \\{0, 1, 2, 3, 4, 5, 6\\}.\n\nNote that Y inherently depends upon \\pi.\n\nIf \\pi = 0.80, Y would also be high (on average).\nIf \\pi = 0.20, Y would also be low (on average).\n\nThus, we must model this dependence of Y on \\pi using a conditional probability model."
  },
  {
    "objectID": "files/lectures/04-2-thinking-bayesian.html#binomial-data-model",
    "href": "files/lectures/04-2-thinking-bayesian.html#binomial-data-model",
    "title": "Thinking Like a Bayesian",
    "section": "Binomial Data Model",
    "text": "Binomial Data Model\n\nWe must make two assumptions about the chess match:\n\nGames are independent (the outcome of one game does not influence the outcome of another).\nKasparov has an equal probability of winning any game in the match.\n\ni.e., probability of winning does not increase or decrease as the match goes on.\n\n\nWe will use a binomial model for this problem.\n\nIn our case,\n\n\nY|\\pi \\sim \\text{Bin}(6, \\pi)"
  },
  {
    "objectID": "files/lectures/04-2-thinking-bayesian.html#binomial-data-model-1",
    "href": "files/lectures/04-2-thinking-bayesian.html#binomial-data-model-1",
    "title": "Thinking Like a Bayesian",
    "section": "Binomial Data Model",
    "text": "Binomial Data Model\n\nLet’s assume \\pi = 0.8.\nThe probability that he would win all 6 games is approximately 26%.\n\nf(y=6|\\pi=0.8) = {6 \\choose 6} 0.8^6 (1-0.8)^{6-6}, \n\ndbinom(6, 6, 0.8)\n\n[1] 0.262144"
  },
  {
    "objectID": "files/lectures/04-2-thinking-bayesian.html#binomial-data-model-2",
    "href": "files/lectures/04-2-thinking-bayesian.html#binomial-data-model-2",
    "title": "Thinking Like a Bayesian",
    "section": "Binomial Data Model",
    "text": "Binomial Data Model\n\nLet’s assume \\pi = 0.8.\nThe probability that he would win none of the games is approximately 0%.\n\nf(y=0|\\pi=0.8) = {6 \\choose 0} 0.8^0 (1-0.8)^{6-0}, \n\ndbinom(0, 6, 0.8)\n\n[1] 6.4e-05"
  },
  {
    "objectID": "files/lectures/04-2-thinking-bayesian.html#binomial-data-model-3",
    "href": "files/lectures/04-2-thinking-bayesian.html#binomial-data-model-3",
    "title": "Thinking Like a Bayesian",
    "section": "Binomial Data Model",
    "text": "Binomial Data Model\n\nWe want to reproduce Figure 2.5 from the Bayes Rules! textbook (from Section 2.3.2).\n\n\n\nEach group will complete the graph for a specified value of \\pi.\n\nCampus: \\pi=0.2\nZoom 1: \\pi=0.5\nZoom 2: \\pi=0.8"
  },
  {
    "objectID": "files/lectures/04-2-thinking-bayesian.html#binomial-data-model-4",
    "href": "files/lectures/04-2-thinking-bayesian.html#binomial-data-model-4",
    "title": "Thinking Like a Bayesian",
    "section": "Binomial Data Model",
    "text": "Binomial Data Model\n\nNote that the Binomial gives us the theoretical model of the data we might observe.\n\nKasparov only won one of the six games against Deep Blue in 1997 (Y=1).\n\nNext step: how compatible this particular data is with the various possible \\pi?\n\nWhat is the likelihood of Kasparov winning Y=1 game under each possible \\pi?\n\nRecall, f(y|\\pi) = L(\\pi|Y=y). When Y=1,\n\n\n\\begin{align*}\nL(\\pi | y = 1) &= f(y=1|\\pi) \\\\\n&= {6 \\choose 1} \\pi^1 (1-\\pi)^{6-1} \\\\\n&= 6\\pi(1-\\pi)^5\n\\end{align*}\n\n\nNote that we do not expect all likelihoods to sum to 1."
  },
  {
    "objectID": "files/lectures/04-2-thinking-bayesian.html#binomial-data-model-5",
    "href": "files/lectures/04-2-thinking-bayesian.html#binomial-data-model-5",
    "title": "Thinking Like a Bayesian",
    "section": "Binomial Data Model",
    "text": "Binomial Data Model\n\nUse your results from earlier to tell me the resulting likelihood values.\n\n\n\n\n\n\n\n\n\n\n\\pi\n0.2\n0.5\n0.8\n\n\n\n\nL(\\pi|y=1)"
  },
  {
    "objectID": "files/lectures/04-2-thinking-bayesian.html#binomial-data-model-6",
    "href": "files/lectures/04-2-thinking-bayesian.html#binomial-data-model-6",
    "title": "Thinking Like a Bayesian",
    "section": "Binomial Data Model",
    "text": "Binomial Data Model\n\nUse your results from earlier to tell me the resulting likelihood values.\n\n\n\n\n\\pi\n0.2\n0.5\n0.8\n\n\n\n\nL(\\pi|y=1)\n0.3932\n0.0938\n0.0015\n\n\n\n\nAs we can see, the likelihoods do not sum to 1."
  },
  {
    "objectID": "files/lectures/04-2-thinking-bayesian.html#normalizing-constant",
    "href": "files/lectures/04-2-thinking-bayesian.html#normalizing-constant",
    "title": "Thinking Like a Bayesian",
    "section": "Normalizing Constant",
    "text": "Normalizing Constant\n\nBayes’ Rule requires three pieces of information:\n\nPrior\nLikelihood\nNormalizing constant\n\nNormalizing constant: ensures that the sum of all probabilities is equal to 1.\n\nIt can be a scalar or a function.\nEvery probability distribution that does not sum to 1 will have a normalizing constant."
  },
  {
    "objectID": "files/lectures/04-2-thinking-bayesian.html#normalizing-constant-1",
    "href": "files/lectures/04-2-thinking-bayesian.html#normalizing-constant-1",
    "title": "Thinking Like a Bayesian",
    "section": "Normalizing Constant",
    "text": "Normalizing Constant\n\nWe now must determine the total probability that Kasparov would win Y=1 games across all possible win probabilities \\pi, f(y=1).\n\n\n\\begin{align*}\nf(y=1) =& \\sum_{\\pi} L(\\pi |y=1)f(\\pi) \\\\\n=& L(\\pi=0.2|y=1)f(\\pi=0.2) + L(\\pi=0.5|y=1)f(\\pi=0.5) + \\\\\n& L(\\pi=0.8|y=1)f(\\pi=0.8)\n\\end{align*}\n\n\nWork with your group to find the normalizing constant."
  },
  {
    "objectID": "files/lectures/04-2-thinking-bayesian.html#normalizing-constant-2",
    "href": "files/lectures/04-2-thinking-bayesian.html#normalizing-constant-2",
    "title": "Thinking Like a Bayesian",
    "section": "Normalizing Constant",
    "text": "Normalizing Constant\n\nWe now must determine the total probability that Kasparov would win Y=1 games across all possible win probabilities \\pi, f(y=1).\n\n\n\\begin{align*}\nf(y=1) =& \\sum_{\\pi} L(\\pi |y=1)f(\\pi) \\\\\n=& L(\\pi=0.2|y=1)f(\\pi=0.2) + L(\\pi=0.5|y=1)f(\\pi=0.5) + \\\\\n& L(\\pi=0.8|y=1)f(\\pi=0.8) \\\\\n\\approx& 0.3932 \\cdot 0.10 + 0.0938 \\cdot 0.25 + 0.0015 \\cdot 0.65 \\\\\n\\approx& 0.0637\n\\end{align*}\n\n\nAcross all possible values of \\pi, there is about a 6% chance that Kasparov would have won only one game."
  },
  {
    "objectID": "files/lectures/04-2-thinking-bayesian.html#posterior-probability-model",
    "href": "files/lectures/04-2-thinking-bayesian.html#posterior-probability-model",
    "title": "Thinking Like a Bayesian",
    "section": "Posterior Probability Model",
    "text": "Posterior Probability Model\n\nNow recall,\n\n\\text{posterior} = \\frac{\\text{prior} \\times \\text{likelihood}}{\\text{normalizing constant}}\n\nIn our example, where y = 1,\n\nf(\\pi | y=1) = \\frac{f(\\pi) L(\\pi | y = 1)}{f(y=1)} \\  \\text{for} \\ \\pi \\in \\{ 0.2, 0.5, 0.8\\}\n\nWork with your group to find the posterior probabilities.\n\nYou will have one posterior probability for each value of \\pi."
  },
  {
    "objectID": "files/lectures/04-2-thinking-bayesian.html#posterior-probability-model-1",
    "href": "files/lectures/04-2-thinking-bayesian.html#posterior-probability-model-1",
    "title": "Thinking Like a Bayesian",
    "section": "Posterior Probability Model",
    "text": "Posterior Probability Model\n\nNote!! We do not have to calculate the normalizing constant!\nWe can note that f(Y=y) = 1/c.\nThen, we say that\n\n\n\\begin{align*}\nf(\\pi | y) &= \\frac{f(\\pi) L(\\pi|y)}{f(y)} \\\\\n& \\propto  f(\\pi) L(\\pi|y) \\\\\n\\\\\n\\text{posterior} &\\propto \\text{prior} \\cdot \\text{likelihood}\n\\end{align*}"
  },
  {
    "objectID": "files/lectures/04-2-thinking-bayesian.html#wrap-up",
    "href": "files/lectures/04-2-thinking-bayesian.html#wrap-up",
    "title": "Thinking Like a Bayesian",
    "section": "Wrap Up",
    "text": "Wrap Up\n\nToday we learned how to, in general, approach Bayesian analysis.\nNext week, we will formalize what we observed today and learn about the conjugate families.\n\nBeta-binomial\nGamma-Poisson\nNormal Normal"
  },
  {
    "objectID": "files/lectures/04-2-thinking-bayesian.html#homework-practice",
    "href": "files/lectures/04-2-thinking-bayesian.html#homework-practice",
    "title": "Thinking Like a Bayesian",
    "section": "Homework / Practice",
    "text": "Homework / Practice\n\nFrom the Bayes Rules! textbook:\n\n1.3\n1.4\n1.8\n2.4\n2.9"
  },
  {
    "objectID": "files/lectures/07-1-approx-posterior.html#introduction",
    "href": "files/lectures/07-1-approx-posterior.html#introduction",
    "title": "Approximating the Posterior",
    "section": "Introduction",
    "text": "Introduction\n\nWe have learned how to think like a Bayesian:\n\nPrior distribution\nData distribution\nPosterior distribution\n\nWe have learned three conjugate families:\n\nBeta-Binomial (binary outcomes)\nGamma-Poisson (count outcomes)\nNormal-Normal (continuous outcomes)\n\nOnce we have a posterior model, we must be able to apply the results.\n\nPosterior estimation\nHypothesis testing\nPrediction"
  },
  {
    "objectID": "files/lectures/07-1-approx-posterior.html#introduction-1",
    "href": "files/lectures/07-1-approx-posterior.html#introduction-1",
    "title": "Approximating the Posterior",
    "section": "Introduction",
    "text": "Introduction\n\nRecall, we have the posterior pdf,\n\nf(\\theta|y) = \\frac{f(\\theta) L(\\theta|y)}{f(y)} \\propto f(\\theta)L(\\theta|y)\n\nNow, in the denominator, we need to remember,\n\nf(y) = \\int_{\\theta_1} \\int_{\\theta_2} \\cdot \\cdot \\cdot \\int_{\\theta_k} f(\\theta) L(\\theta|y) d\\theta_k \\cdot \\cdot \\cdot d\\theta_2 d\\theta_1\n\nBecause this is … not fun … we will approximate the posterior via simulation."
  },
  {
    "objectID": "files/lectures/07-1-approx-posterior.html#introduction-2",
    "href": "files/lectures/07-1-approx-posterior.html#introduction-2",
    "title": "Approximating the Posterior",
    "section": "Introduction",
    "text": "Introduction\n\nWe are going to explore two simulation techniques:\n\ngrid approximation\nMarkov chain Monte Carlo (MCMC)\n\nEither method will produce a sample of N values for \\theta.\n\n\\left \\{ \\theta^{(1)}, \\theta^{(2)}, ..., \\theta^{(N)} \\right \\}\n\nThese \\theta_i will have properties that reflect those of the posterior model for \\theta.\nTo help us, we will apply these simulation techniques to Beta-Binomial and Gamma-Poisson models.\n\nNote that these models do not require simulation! We know their posteriors!\nThat’s why we are starting there – we can link the concepts to what we know. :)"
  },
  {
    "objectID": "files/lectures/07-1-approx-posterior.html#introduction-3",
    "href": "files/lectures/07-1-approx-posterior.html#introduction-3",
    "title": "Approximating the Posterior",
    "section": "Introduction",
    "text": "Introduction\n\nNote: we will use the following packages that may be new to you:\n\njanitor\nrstan\nbayesplot\n\nIf you are using the server provided by HMCSE, they have been installed for you.\nIf you are working at home, please check to see if you have the libraries, then install if you do not.\n\ninstall.packages(c(\"janitor\", \"rstan\", \"bayesplot\"))"
  },
  {
    "objectID": "files/lectures/07-1-approx-posterior.html#grid-approximation",
    "href": "files/lectures/07-1-approx-posterior.html#grid-approximation",
    "title": "Approximating the Posterior",
    "section": "Grid Approximation",
    "text": "Grid Approximation\n\n\n\n\nSuppose there is an image that you can’t view in its entirety.\nWe can see snippets along a grid that sweeps from left to right across the image.\nThe finer the grid, the clearer the image; if the grid is fine enough, the result is a good approximation."
  },
  {
    "objectID": "files/lectures/07-1-approx-posterior.html#grid-approximation-1",
    "href": "files/lectures/07-1-approx-posterior.html#grid-approximation-1",
    "title": "Approximating the Posterior",
    "section": "Grid Approximation",
    "text": "Grid Approximation\n\n\n\n\nThis is the general idea behind Bayesian grid approximation.\nOur target image is the posterior pdf, f(\\theta|y).\n\nIt is not necessary to observe all possible f(\\theta|y) \\ \\forall \\theta for us to understand its structure.\nInstead, we evaluate f(\\theta|y) at a finite, discrete grid of possible \\theta values.\nThen, we take random samples from this discretized pdf to approximate the full posterior pdf."
  },
  {
    "objectID": "files/lectures/07-1-approx-posterior.html#grid-approximation-2",
    "href": "files/lectures/07-1-approx-posterior.html#grid-approximation-2",
    "title": "Approximating the Posterior",
    "section": "Grid Approximation",
    "text": "Grid Approximation\n\nGrid approximation produces a sample of N independent \\theta values,\n\n\\left\\{ \\theta^{(1)}, \\theta^{(2)}, ..., \\theta^{(N)} \\right\\},\nfrom a discretized approximation of the posterior pdf, f(\\theta|y)."
  },
  {
    "objectID": "files/lectures/07-1-approx-posterior.html#grid-approximation-3",
    "href": "files/lectures/07-1-approx-posterior.html#grid-approximation-3",
    "title": "Approximating the Posterior",
    "section": "Grid Approximation",
    "text": "Grid Approximation\n\nAlgorithm:\n\nDefine a discrete grid of possible \\theta values.\nEvaluate the prior pdf, f(\\theta), and the likelihood function, L(\\theta|y) at each \\theta grid value.\nObtain a discrete approximation of the posterior pdf, f(\\theta|y) by:\n\nCalculating the product f(\\theta) L(\\theta|y) at each \\theta grid value,\nNormalize the products from (a) to sum to 1 across all \\theta.\n\nRandomly sample N \\theta grid values with respect to their corresponding normalized posterior probabilities."
  },
  {
    "objectID": "files/lectures/07-1-approx-posterior.html#grid-approximation---example",
    "href": "files/lectures/07-1-approx-posterior.html#grid-approximation---example",
    "title": "Approximating the Posterior",
    "section": "Grid Approximation - Example",
    "text": "Grid Approximation - Example\n\nWe will use the following Beta-Binomial model to learn how to do grid approximation:\n\n\n\\begin{align*}\nY|\\pi &\\sim \\text{Bin}(10, \\pi) \\\\\n\\pi &\\sim \\text{Beta}(2, 2)\n\\end{align*}\n\n\nNote that\n\nY is the number of successes in 10 independent trials.\nEvery trial has probability of success, \\pi.\nOur prior understanding about \\pi is captured by a \\text{Beta}(2,2) model."
  },
  {
    "objectID": "files/lectures/07-1-approx-posterior.html#grid-approximation---example-1",
    "href": "files/lectures/07-1-approx-posterior.html#grid-approximation---example-1",
    "title": "Approximating the Posterior",
    "section": "Grid Approximation - Example",
    "text": "Grid Approximation - Example\n\nIf we observe Y = 9 successes, we know that the updated posterior model for \\pi is \\text{Beta}(11, 3).\n\nY + \\alpha = 9+2\nn - Y + \\beta = 10-9+2\n\nThus,\n\n\n\\begin{align*}\nY|\\pi &\\sim \\text{Bin}(10, \\pi) \\\\\n\\pi &\\sim \\text{Beta}(2, 2) \\\\\n\\pi | Y &\\sim \\text{Beta}(11, 3)\n\\end{align*}"
  },
  {
    "objectID": "files/lectures/07-1-approx-posterior.html#grid-approximation-4",
    "href": "files/lectures/07-1-approx-posterior.html#grid-approximation-4",
    "title": "Approximating the Posterior",
    "section": "Grid Approximation",
    "text": "Grid Approximation\n\nInstead of using the posterior we know, let’s approximate it using grid approximation.\nFirst step: define a discrete grid of possible \\theta values.\n\nSo, let’s consider \\pi \\in \\{0, 0.2, 0.4, 0.6, 0.8, 1\\}.\n\n\n\nlibrary(tidyverse)\ngrid_data &lt;- tibble(pi_grid = seq(from = 0, to = 1, length = 6))"
  },
  {
    "objectID": "files/lectures/07-1-approx-posterior.html#grid-approximation-5",
    "href": "files/lectures/07-1-approx-posterior.html#grid-approximation-5",
    "title": "Approximating the Posterior",
    "section": "Grid Approximation",
    "text": "Grid Approximation\n\nInstead of using the posterior we know, let’s approximate it using grid approximation.\nFirst step: define a discrete grid of possible \\theta values.\n\nSo, let’s consider \\pi \\in \\{0, 0.2, 0.4, 0.6, 0.8, 1\\}.\n\n\n\nlibrary(tidyverse)\ngrid_data &lt;- tibble(pi_grid = seq(from = 0, to = 1, length = 6))"
  },
  {
    "objectID": "files/lectures/07-1-approx-posterior.html#grid-approximation-6",
    "href": "files/lectures/07-1-approx-posterior.html#grid-approximation-6",
    "title": "Approximating the Posterior",
    "section": "Grid Approximation",
    "text": "Grid Approximation\n\nInstead of using the posterior we know, let’s approximate it using grid approximation.\nSecond step: evaluate the prior pdf, f(\\theta), and the likelihood function, L(\\theta|y) at each \\theta grid value.\n\nWe will use dbeta() and dbinom() to evaluate the \\text{Beta}(2,2) prior and \\text{Bin}(10, \\pi) likelihood with Y=9 at each \\pi in pi_grid.\n\n\n\ngrid_data &lt;- grid_data %&gt;%\n  mutate(prior = dbeta(pi_grid, 2, 2),\n         likelihood = dbinom(9, 10, pi_grid))"
  },
  {
    "objectID": "files/lectures/07-1-approx-posterior.html#grid-approximation-7",
    "href": "files/lectures/07-1-approx-posterior.html#grid-approximation-7",
    "title": "Approximating the Posterior",
    "section": "Grid Approximation",
    "text": "Grid Approximation\n\nInstead of using the posterior we know, let’s approximate it using grid approximation.\nSecond step: evaluate the prior pdf, f(\\theta), and the likelihood function, L(\\theta|y) at each \\theta grid value.\n\nWe will use dbeta() and dbinom() to evaluate the \\text{Beta}(2,2) prior and \\text{Bin}(10, \\pi) likelihood with Y=9 at each \\pi in pi_grid.\n\n\n\ngrid_data &lt;- grid_data %&gt;%\n  mutate(prior = dbeta(pi_grid, 2, 2),\n         likelihood = dbinom(9, 10, pi_grid))"
  },
  {
    "objectID": "files/lectures/07-1-approx-posterior.html#grid-approximation-8",
    "href": "files/lectures/07-1-approx-posterior.html#grid-approximation-8",
    "title": "Approximating the Posterior",
    "section": "Grid Approximation",
    "text": "Grid Approximation\n\nInstead of using the posterior we know, let’s approximate it using grid approximation.\nThird step: obtain a discrete approximation of the posterior pdf, f(\\theta|y) by calculating the product f(\\theta) L(\\theta|y) at each \\theta grid value and normalizing the products to sum to 1 across all \\theta.\n\n\ngrid_data &lt;- grid_data %&gt;%\n  mutate(unnormalized = likelihood*prior,\n         posterior = unnormalized / sum(unnormalized))"
  },
  {
    "objectID": "files/lectures/07-1-approx-posterior.html#grid-approximation-9",
    "href": "files/lectures/07-1-approx-posterior.html#grid-approximation-9",
    "title": "Approximating the Posterior",
    "section": "Grid Approximation",
    "text": "Grid Approximation\n\nInstead of using the posterior we know, let’s approximate it using grid approximation.\nThird step: obtain a discrete approximation of the posterior pdf, f(\\theta|y) by calculating the product f(\\theta) L(\\theta|y) at each \\theta grid value and normalizing the products to sum to 1 across all \\theta.\n\n\ngrid_data &lt;- grid_data %&gt;%\n  mutate(unnormalized = likelihood*prior,\n         posterior = unnormalized / sum(unnormalized))\n\n\nWe can verify,\n\n\ngrid_data %&gt;%\n  summarize(sum(unnormalized), sum(posterior))"
  },
  {
    "objectID": "files/lectures/07-1-approx-posterior.html#grid-approximation-10",
    "href": "files/lectures/07-1-approx-posterior.html#grid-approximation-10",
    "title": "Approximating the Posterior",
    "section": "Grid Approximation",
    "text": "Grid Approximation\n\nInstead of using the posterior we know, let’s approximate it using grid approximation.\nThird step: obtain a discrete approximation of the posterior pdf, f(\\theta|y) by calculating the product f(\\theta) L(\\theta|y) at each \\theta grid value and normalizing the products to sum to 1 across all \\theta.\n\n\ngrid_data &lt;- grid_data %&gt;%\n  mutate(unnormalized = likelihood*prior,\n         posterior = unnormalized / sum(unnormalized))"
  },
  {
    "objectID": "files/lectures/07-1-approx-posterior.html#grid-approximation-11",
    "href": "files/lectures/07-1-approx-posterior.html#grid-approximation-11",
    "title": "Approximating the Posterior",
    "section": "Grid Approximation",
    "text": "Grid Approximation\n\nInstead of using the posterior we know, let’s approximate it using grid approximation.\nWe now have a glimpse into the actual posterior pdf.\n\nWe can plot it to see what it looks like,"
  },
  {
    "objectID": "files/lectures/07-1-approx-posterior.html#grid-approximation-12",
    "href": "files/lectures/07-1-approx-posterior.html#grid-approximation-12",
    "title": "Approximating the Posterior",
    "section": "Grid Approximation",
    "text": "Grid Approximation\n\nAs we increase the number of possible \\theta values, the better we can “see” the resulting posterior.\nWhat happens if we try the following: n=50, n=100, n=500, n=1000?"
  },
  {
    "objectID": "files/lectures/07-1-approx-posterior.html#grid-approximation-13",
    "href": "files/lectures/07-1-approx-posterior.html#grid-approximation-13",
    "title": "Approximating the Posterior",
    "section": "Grid Approximation",
    "text": "Grid Approximation\n\nAs we increase the number of possible \\theta values, the better we can “see” the resulting posterior.\nWhat happens if we try the following: n=50, n=100, n=500, n=1000?"
  },
  {
    "objectID": "files/lectures/07-1-approx-posterior.html#markov-chain-monte-carlo-mcmc",
    "href": "files/lectures/07-1-approx-posterior.html#markov-chain-monte-carlo-mcmc",
    "title": "Approximating the Posterior",
    "section": "Markov chain Monte Carlo (MCMC)",
    "text": "Markov chain Monte Carlo (MCMC)\n\nMarkov chain Monte Carlo (MCMC) is an application of Markov chains to simulate probability models.\nMCMC samples are not taken directly from the posterior pdf, f(\\theta | y)… and they are not independent.\n\nEach subsequent value depends on the previous value.\n\nSuppose we have an N-length MCMC sample, \\left\\{ \\theta^{(1)}, \\theta^{(2)}, \\theta^{(3)}, ..., \\theta^{(N)} \\right\\}\n\n\\theta^{(2)} is drawn from a model that depends on \\theta^{(1)}.\n\\theta^{(3)} is drawn from a model that depends on \\theta^{(2)}.\netc."
  },
  {
    "objectID": "files/lectures/07-1-approx-posterior.html#markov-chain-monte-carlo-mcmc-1",
    "href": "files/lectures/07-1-approx-posterior.html#markov-chain-monte-carlo-mcmc-1",
    "title": "Approximating the Posterior",
    "section": "Markov chain Monte Carlo (MCMC)",
    "text": "Markov chain Monte Carlo (MCMC)\n\nThe (i+1)st chain value, \\theta^{(i+1)} is drawn from a model that depends on data y and the previous chain value, \\theta^{(i)}.\n\nf\\left( \\theta^{(i+1)} | \\theta^{(i)}, y \\right)\n\nIt is important for us to note that the pdf from which a Markov chain is simulated is not equivalent to the posterior pdf!\n\nf\\left( \\theta^{(i+1)} | \\theta^{(i)}, y  \\right) \\ne f\\left(\\theta^{(i+1)}|y \\right)"
  },
  {
    "objectID": "files/lectures/07-1-approx-posterior.html#using-rstan",
    "href": "files/lectures/07-1-approx-posterior.html#using-rstan",
    "title": "Approximating the Posterior",
    "section": "Using rstan",
    "text": "Using rstan\n\nWe will use rstan:\n\ndefine the Bayesian model structure in rstan notation\nsimulate the posterior\n\nAgain, we will use the Beta-Binomial model from earlier.\n\ndata: in our example, Y is the observed number of successes in 10 trials.\n\nWe need to tell rstan that Y is an integer between 0 and 10.\n\nparameters: in our example, our model depends on \\pi.\n\nWe need to tell rstan that \\pi can be any real number between 0 and 1.\n\nmodel: in our example, we need to specify Y \\sim \\text{Bin}(10, \\pi) and \\pi \\sim \\text{Beta}(2,2)."
  },
  {
    "objectID": "files/lectures/07-1-approx-posterior.html#using-rstan-1",
    "href": "files/lectures/07-1-approx-posterior.html#using-rstan-1",
    "title": "Approximating the Posterior",
    "section": "Using rstan",
    "text": "Using rstan\n\n# STEP 1: DEFINE the model\nbb_model &lt;- \"\n  data {\n    int&lt;lower = 0, upper = 10&gt; Y;\n  }\n  parameters {\n    real&lt;lower = 0, upper = 1&gt; pi;\n  }\n  model {\n    Y ~ binomial(10, pi);\n    pi ~ beta(2, 2);\n  }\n\""
  },
  {
    "objectID": "files/lectures/07-1-approx-posterior.html#using-rstan-2",
    "href": "files/lectures/07-1-approx-posterior.html#using-rstan-2",
    "title": "Approximating the Posterior",
    "section": "Using rstan",
    "text": "Using rstan\n\nThen, when we go to simulate, we first put in the model information\n\nmodel code: the character string defining the model (in our case, bb_model).\ndata: a list of the observed data.\n\nIn this example, we are using Y = 9 - a single data point.\n\n\nThen, we put in the Markov chain information,\n\nchains: how many parallel Markov chains to run.\n\nThis will be the number of distinct \\theta values we want.\n\niter: desired number of iterations, or length of Markov chain.\n\nHalf are thrown out as “burn in” samples.\n\n“burn in”? Think: pancakes!\n\n\nseed: used to set the seed of the RNG."
  },
  {
    "objectID": "files/lectures/07-1-approx-posterior.html#using-rstan-3",
    "href": "files/lectures/07-1-approx-posterior.html#using-rstan-3",
    "title": "Approximating the Posterior",
    "section": "Using rstan",
    "text": "Using rstan\n\n# STEP 2: simulate the posterior\nbb_sim &lt;- stan(model_code = bb_model, data = list(Y = 9), \n               chains = 4, iter = 5000*2, seed = 84735)\n\n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 4e-06 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.04 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 1: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 1: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 1: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 1: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 1: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 1: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 1: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 1: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 1: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 1: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 1: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.012 seconds (Warm-up)\nChain 1:                0.014 seconds (Sampling)\nChain 1:                0.026 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 1e-06 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.01 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 2: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 2: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 2: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 2: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 2: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 2: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 2: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 2: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 2: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 2: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 2: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.013 seconds (Warm-up)\nChain 2:                0.014 seconds (Sampling)\nChain 2:                0.027 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 1e-06 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.01 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 3: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 3: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 3: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 3: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 3: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 3: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 3: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 3: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 3: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 3: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 3: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 0.013 seconds (Warm-up)\nChain 3:                0.012 seconds (Sampling)\nChain 3:                0.025 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 0 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 4: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 4: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 4: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 4: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 4: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 4: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 4: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 4: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 4: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 4: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 4: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 0.013 seconds (Warm-up)\nChain 4:                0.014 seconds (Sampling)\nChain 4:                0.027 seconds (Total)\nChain 4:"
  },
  {
    "objectID": "files/lectures/07-1-approx-posterior.html#using-rstan-4",
    "href": "files/lectures/07-1-approx-posterior.html#using-rstan-4",
    "title": "Approximating the Posterior",
    "section": "Using rstan",
    "text": "Using rstan\n\nNow, we need to extract the values,\n\n\nas.array(bb_sim, pars = \"pi\") %&gt;% head(4)\n\n, , parameters = pi\n\n          chains\niterations   chain:1   chain:2   chain:3   chain:4\n      [1,] 0.8278040 0.7523560 0.6386998 0.9627572\n      [2,] 0.9087546 0.8922386 0.6254761 0.9413518\n      [3,] 0.6143859 0.8682356 0.7222360 0.9489624\n      [4,] 0.8462156 0.8792812 0.8100192 0.9413812\n\n\n\nRemember, these are not a random sample from the posterior!\nThey are also not independent!\nEach chain forms a dependent 5,000 length Markov chain of \\left\\{ \\pi^{(1)}, \\pi^{(2)}, ..., \\pi^{(5000)}\\right\\}\n\nEach chain will move along the sample space of plausible values for \\pi."
  },
  {
    "objectID": "files/lectures/07-1-approx-posterior.html#using-rstan-5",
    "href": "files/lectures/07-1-approx-posterior.html#using-rstan-5",
    "title": "Approximating the Posterior",
    "section": "Using rstan",
    "text": "Using rstan\n\nWe will look at the trace plot (using mcmc_trace() from bayesplot package) to see what the values did longitudinally.\n\n\n\nmcmc_trace(bb_sim, pars = \"pi\", size = 0.1)"
  },
  {
    "objectID": "files/lectures/07-1-approx-posterior.html#using-rstan-6",
    "href": "files/lectures/07-1-approx-posterior.html#using-rstan-6",
    "title": "Approximating the Posterior",
    "section": "Using rstan",
    "text": "Using rstan\n\nWe can also look at the mcmc_hist() and mcmc_dens() functions,"
  },
  {
    "objectID": "files/lectures/07-1-approx-posterior.html#diagnostics",
    "href": "files/lectures/07-1-approx-posterior.html#diagnostics",
    "title": "Approximating the Posterior",
    "section": "Diagnostics",
    "text": "Diagnostics\n\nSimulations are not perfect…\n\nWhat does a good Markov chain look like?\nHow can we tell if the Markov chain sample produces a reasonable approximation of the posterior?\nHow big should our Markov chain sample size be?\n\nUnfortunately there is no one answer here.\n\nYou will learn through experience, much like other nuanced areas of statistics."
  },
  {
    "objectID": "files/lectures/07-1-approx-posterior.html#diagnostics-1",
    "href": "files/lectures/07-1-approx-posterior.html#diagnostics-1",
    "title": "Approximating the Posterior",
    "section": "Diagnostics",
    "text": "Diagnostics\n\nLet’s now discuss diagnostic tools.\n\nTrace plots\nParallel chains\nEffective sample size\nAutocorrelation\n\\hat{R}"
  },
  {
    "objectID": "files/lectures/07-1-approx-posterior.html#trace-plots",
    "href": "files/lectures/07-1-approx-posterior.html#trace-plots",
    "title": "Approximating the Posterior",
    "section": "Trace Plots",
    "text": "Trace Plots\n\n\n\n\nChain A has not stabilized after 5000 iterations.\n\nIt has not “found” or does not know how to explore the range of posterior plausible \\pi values.\nThe downward trend also hints against independent noise."
  },
  {
    "objectID": "files/lectures/07-1-approx-posterior.html#trace-plots-1",
    "href": "files/lectures/07-1-approx-posterior.html#trace-plots-1",
    "title": "Approximating the Posterior",
    "section": "Trace Plots",
    "text": "Trace Plots\n\n\n\n\nWe say that Chain A is mixing slowly.\n\nThe more Markov chains behave like fast mixing (noisy) independent samples, the smaller the error in the resulting posterior approximation."
  },
  {
    "objectID": "files/lectures/07-1-approx-posterior.html#trace-plots-2",
    "href": "files/lectures/07-1-approx-posterior.html#trace-plots-2",
    "title": "Approximating the Posterior",
    "section": "Trace Plots",
    "text": "Trace Plots\n\n\n\n\nChain B is not great, either – it gets stuck when looking at a smaller value of \\pi."
  },
  {
    "objectID": "files/lectures/07-1-approx-posterior.html#trace-plots-3",
    "href": "files/lectures/07-1-approx-posterior.html#trace-plots-3",
    "title": "Approximating the Posterior",
    "section": "Trace Plots",
    "text": "Trace Plots\n\nRealistically, we are only going to do simulations when we can’t specify the posterior and must approximate\n\ni.e., we won’t be able to compare the simulation results to the “true” results.\n\nIf we see bad trace plots:\n\nCheck the model (… or your code). Are the assumed prior and data models appropriate?\nRun the chain for more iterations. Sometimes we just need a longer run to iron out issues."
  },
  {
    "objectID": "files/lectures/07-1-approx-posterior.html#parallel-chains",
    "href": "files/lectures/07-1-approx-posterior.html#parallel-chains",
    "title": "Approximating the Posterior",
    "section": "Parallel Chains",
    "text": "Parallel Chains\n\nLet’s now consider a smaller simulation, where n=50 (recall, overall n=100, but half is for burn-in).\nRun the following code:\n\n\nbb_sim_short &lt;- stan(model_code = bb_model, data = list(Y = 9), \n                     chains = 4, iter = 50*2, seed = 84735)"
  },
  {
    "objectID": "files/lectures/07-1-approx-posterior.html#parallel-chains-1",
    "href": "files/lectures/07-1-approx-posterior.html#parallel-chains-1",
    "title": "Approximating the Posterior",
    "section": "Parallel Chains",
    "text": "Parallel Chains\n\nLet’s now consider a smaller simulation, where n=50 (recall, overall n=100, but half is for burn-in).\nRun the following code:\n\n\nbb_sim_short &lt;- stan(model_code = bb_model, data = list(Y = 9), \n                     chains = 4, iter = 50*2, seed = 84735)\n\n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 1e-06 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.01 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: WARNING: There aren't enough warmup iterations to fit the\nChain 1:          three stages of adaptation as currently configured.\nChain 1:          Reducing each adaptation stage to 15%/75%/10% of\nChain 1:          the given number of warmup iterations:\nChain 1:            init_buffer = 7\nChain 1:            adapt_window = 38\nChain 1:            term_buffer = 5\nChain 1: \nChain 1: Iteration:  1 / 100 [  1%]  (Warmup)\nChain 1: Iteration: 10 / 100 [ 10%]  (Warmup)\nChain 1: Iteration: 20 / 100 [ 20%]  (Warmup)\nChain 1: Iteration: 30 / 100 [ 30%]  (Warmup)\nChain 1: Iteration: 40 / 100 [ 40%]  (Warmup)\nChain 1: Iteration: 50 / 100 [ 50%]  (Warmup)\nChain 1: Iteration: 51 / 100 [ 51%]  (Sampling)\nChain 1: Iteration: 60 / 100 [ 60%]  (Sampling)\nChain 1: Iteration: 70 / 100 [ 70%]  (Sampling)\nChain 1: Iteration: 80 / 100 [ 80%]  (Sampling)\nChain 1: Iteration: 90 / 100 [ 90%]  (Sampling)\nChain 1: Iteration: 100 / 100 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0 seconds (Warm-up)\nChain 1:                0 seconds (Sampling)\nChain 1:                0 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 0 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: WARNING: There aren't enough warmup iterations to fit the\nChain 2:          three stages of adaptation as currently configured.\nChain 2:          Reducing each adaptation stage to 15%/75%/10% of\nChain 2:          the given number of warmup iterations:\nChain 2:            init_buffer = 7\nChain 2:            adapt_window = 38\nChain 2:            term_buffer = 5\nChain 2: \nChain 2: Iteration:  1 / 100 [  1%]  (Warmup)\nChain 2: Iteration: 10 / 100 [ 10%]  (Warmup)\nChain 2: Iteration: 20 / 100 [ 20%]  (Warmup)\nChain 2: Iteration: 30 / 100 [ 30%]  (Warmup)\nChain 2: Iteration: 40 / 100 [ 40%]  (Warmup)\nChain 2: Iteration: 50 / 100 [ 50%]  (Warmup)\nChain 2: Iteration: 51 / 100 [ 51%]  (Sampling)\nChain 2: Iteration: 60 / 100 [ 60%]  (Sampling)\nChain 2: Iteration: 70 / 100 [ 70%]  (Sampling)\nChain 2: Iteration: 80 / 100 [ 80%]  (Sampling)\nChain 2: Iteration: 90 / 100 [ 90%]  (Sampling)\nChain 2: Iteration: 100 / 100 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0 seconds (Warm-up)\nChain 2:                0 seconds (Sampling)\nChain 2:                0 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 0 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: WARNING: There aren't enough warmup iterations to fit the\nChain 3:          three stages of adaptation as currently configured.\nChain 3:          Reducing each adaptation stage to 15%/75%/10% of\nChain 3:          the given number of warmup iterations:\nChain 3:            init_buffer = 7\nChain 3:            adapt_window = 38\nChain 3:            term_buffer = 5\nChain 3: \nChain 3: Iteration:  1 / 100 [  1%]  (Warmup)\nChain 3: Iteration: 10 / 100 [ 10%]  (Warmup)\nChain 3: Iteration: 20 / 100 [ 20%]  (Warmup)\nChain 3: Iteration: 30 / 100 [ 30%]  (Warmup)\nChain 3: Iteration: 40 / 100 [ 40%]  (Warmup)\nChain 3: Iteration: 50 / 100 [ 50%]  (Warmup)\nChain 3: Iteration: 51 / 100 [ 51%]  (Sampling)\nChain 3: Iteration: 60 / 100 [ 60%]  (Sampling)\nChain 3: Iteration: 70 / 100 [ 70%]  (Sampling)\nChain 3: Iteration: 80 / 100 [ 80%]  (Sampling)\nChain 3: Iteration: 90 / 100 [ 90%]  (Sampling)\nChain 3: Iteration: 100 / 100 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 0 seconds (Warm-up)\nChain 3:                0 seconds (Sampling)\nChain 3:                0 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 0 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: WARNING: There aren't enough warmup iterations to fit the\nChain 4:          three stages of adaptation as currently configured.\nChain 4:          Reducing each adaptation stage to 15%/75%/10% of\nChain 4:          the given number of warmup iterations:\nChain 4:            init_buffer = 7\nChain 4:            adapt_window = 38\nChain 4:            term_buffer = 5\nChain 4: \nChain 4: Iteration:  1 / 100 [  1%]  (Warmup)\nChain 4: Iteration: 10 / 100 [ 10%]  (Warmup)\nChain 4: Iteration: 20 / 100 [ 20%]  (Warmup)\nChain 4: Iteration: 30 / 100 [ 30%]  (Warmup)\nChain 4: Iteration: 40 / 100 [ 40%]  (Warmup)\nChain 4: Iteration: 50 / 100 [ 50%]  (Warmup)\nChain 4: Iteration: 51 / 100 [ 51%]  (Sampling)\nChain 4: Iteration: 60 / 100 [ 60%]  (Sampling)\nChain 4: Iteration: 70 / 100 [ 70%]  (Sampling)\nChain 4: Iteration: 80 / 100 [ 80%]  (Sampling)\nChain 4: Iteration: 90 / 100 [ 90%]  (Sampling)\nChain 4: Iteration: 100 / 100 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 0 seconds (Warm-up)\nChain 4:                0 seconds (Sampling)\nChain 4:                0 seconds (Total)\nChain 4:"
  },
  {
    "objectID": "files/lectures/07-1-approx-posterior.html#parallel-chains-2",
    "href": "files/lectures/07-1-approx-posterior.html#parallel-chains-2",
    "title": "Approximating the Posterior",
    "section": "Parallel Chains",
    "text": "Parallel Chains\n\nLet’s now generate the trace and density plots for the smaller simulation:\n\n\nggarrange(mcmc_trace(bb_sim_short, pars = \"pi\") + ggtitle(\"trace\") + theme_bw() + theme(legend.position=\"none\"),\n          mcmc_dens_overlay(bb_sim_short, pars = \"pi\") + ylab(\"density\") + ggtitle(\"density\") + theme_bw(),\n          ncol=2, nrow=1)"
  },
  {
    "objectID": "files/lectures/07-1-approx-posterior.html#parallel-chains-3",
    "href": "files/lectures/07-1-approx-posterior.html#parallel-chains-3",
    "title": "Approximating the Posterior",
    "section": "Parallel Chains",
    "text": "Parallel Chains"
  },
  {
    "objectID": "files/lectures/07-1-approx-posterior.html#parallel-chains-4",
    "href": "files/lectures/07-1-approx-posterior.html#parallel-chains-4",
    "title": "Approximating the Posterior",
    "section": "Parallel Chains",
    "text": "Parallel Chains\n\nNow you try 10,000 iterations. Update the following code:\n\n\nbb_sim_full &lt;- stan(model_code = bb_model, \n                    data = list(Y = 9), \n                    chains = 4, \n                    iter = 50*2, \n                    seed = 84735)"
  },
  {
    "objectID": "files/lectures/07-1-approx-posterior.html#parallel-chains-5",
    "href": "files/lectures/07-1-approx-posterior.html#parallel-chains-5",
    "title": "Approximating the Posterior",
    "section": "Parallel Chains",
    "text": "Parallel Chains\n\nCreate the trace and density plots. Recall the code,\n\n\nggarrange(\n  mcmc_trace(bb_sim, pars = \"pi\") + theme_bw() + theme(legend.position=\"none\") + ggtitle(\"trace\"),\n  mcmc_dens_overlay(bb_sim, pars = \"pi\") +  ylab(\"density\") + ggtitle(\"density\") + theme_bw(),\n  ncol = 2, nrow = 1)"
  },
  {
    "objectID": "files/lectures/07-1-approx-posterior.html#parallel-chains-6",
    "href": "files/lectures/07-1-approx-posterior.html#parallel-chains-6",
    "title": "Approximating the Posterior",
    "section": "Parallel Chains",
    "text": "Parallel Chains\n\nCreate the trace and density plots."
  },
  {
    "objectID": "files/lectures/07-1-approx-posterior.html#effective-sample-size",
    "href": "files/lectures/07-1-approx-posterior.html#effective-sample-size",
    "title": "Approximating the Posterior",
    "section": "Effective Sample Size",
    "text": "Effective Sample Size\n\nThe more a dependent Markov chain behaves like an independent sample, the smaller the error in the resulting posterior approximation.\n\nPlots are great, but numerical assessment can provide more nuanced information.\n\nEffective sample size (N_{\\text{eff}}): the number of independent sample values it would take to produce an equivalently accurate posterior approximation.\nEffective sample size ratio:\n\n\\frac{N_{\\text{eff}}}{N}\n\nGenerally, we look for the effective sample size, N_{\\text{eff}}, to be greater than 10% of the actual sample size, N."
  },
  {
    "objectID": "files/lectures/07-1-approx-posterior.html#effective-sample-size-1",
    "href": "files/lectures/07-1-approx-posterior.html#effective-sample-size-1",
    "title": "Approximating the Posterior",
    "section": "Effective Sample Size",
    "text": "Effective Sample Size\n\nWe will use the neff_ratio() function to find this ratio.\nIn our example data,\n\n\n# Calculate the effective sample size ratio - N = 50\nneff_ratio(bb_sim, pars = c(\"pi\"))\n\n[1] 0.3462257\n\n# Calculate the effective sample size ratio - N = 10000\nneff_ratio(bb_sim_short, pars = c(\"pi\"))\n\n[1] 0.4950171\n\n\n\nBecause the N_{\\text{eff}} is over 10%, we are not concerned and can proceed."
  },
  {
    "objectID": "files/lectures/07-1-approx-posterior.html#autocorrelation",
    "href": "files/lectures/07-1-approx-posterior.html#autocorrelation",
    "title": "Approximating the Posterior",
    "section": "Autocorrelation",
    "text": "Autocorrelation\n\nAutocorrelation allows us to evaluate if our Markov chain sufficiently mimics the behavior of an independent sample.\nAutocorrelation:\n\nLag 1 autocorrelation measures the correlation between pairs of Markov chain values that are one “step” apart (e.g., \\pi_i and \\pi_{(i-1)}; e.g., \\pi_4 and \\pi_3).\nLag 2 autocorrelation measures the correlation between pairs of Markov chain values that are two “steps apart (e.g., \\pi_i and \\pi_{(i-2)}; e.g., \\pi_4 and \\pi_2).\nLag k autocorrelation measures the correlation between pairs of Markov chain values that are k “steps” apart (e.g., \\pi_i and \\pi_{(i-k)}; e.g., \\pi_4 and \\pi_{(4-k)})."
  },
  {
    "objectID": "files/lectures/07-1-approx-posterior.html#autocorrelation-1",
    "href": "files/lectures/07-1-approx-posterior.html#autocorrelation-1",
    "title": "Approximating the Posterior",
    "section": "Autocorrelation",
    "text": "Autocorrelation\n\nAutocorrelation allows us to evaluate if our Markov chain sufficiently mimics the behavior of an independent sample.\nStrong autocorrelation or dependence is a bad thing.\n\nIt goes hand in hand with small effective sample size ratios.\nThese provide a warning sign that our resulting posterior approximations might be unreliable."
  },
  {
    "objectID": "files/lectures/07-1-approx-posterior.html#autocorrelation-2",
    "href": "files/lectures/07-1-approx-posterior.html#autocorrelation-2",
    "title": "Approximating the Posterior",
    "section": "Autocorrelation",
    "text": "Autocorrelation\n\n\n\n\nNo obvious patterns in the trace plot; dependence is relatively weak.\nAutocorrelation plot quickly drops off and is effectively 0 by lag 5.\nConfirmation that our Markov chain is mixing quickly.\n\ni.e., quickly moving around the range of posterior plausible \\pi values\ni.e., at least mimicking an independent sample."
  },
  {
    "objectID": "files/lectures/07-1-approx-posterior.html#autocorrelation-3",
    "href": "files/lectures/07-1-approx-posterior.html#autocorrelation-3",
    "title": "Approximating the Posterior",
    "section": "Autocorrelation",
    "text": "Autocorrelation\n\n\n\n\nThis is an “unhealthy” Markov chain.\nTrace plot shows strong trends \\to autocorrelation in the Markov chain values.\nSlow decrease in autocorrelation plot indicates that the dependence between chain values does not quickly fade away.\n\nAt lag 20, the autocorrelation is still \\sim 90%."
  },
  {
    "objectID": "files/lectures/07-1-approx-posterior.html#fast-vs.-slow-mixing-markov-chains",
    "href": "files/lectures/07-1-approx-posterior.html#fast-vs.-slow-mixing-markov-chains",
    "title": "Approximating the Posterior",
    "section": "Fast vs. Slow Mixing Markov Chains",
    "text": "Fast vs. Slow Mixing Markov Chains\n\nFast mixing chains:\n\nThe chains move “quickly” around the range of posterior plausible values\nThe autocorrelation among the chain values drops off quickly.\nThe effective sample size ratio is reasonably large.\n\nSlow mixing chains:\n\nThe chains move “slowly” around the range of posterior plauslbe values.\nThe autocorrelation among the chainv alues drops off very slowly.\nThe effective sample size ratio is small.\n\nWhat do we do if we have a slow mixing chain?\n\nIncrease the chain size :)\nThin the Markov chain :|"
  },
  {
    "objectID": "files/lectures/07-1-approx-posterior.html#thinning-markov-chains",
    "href": "files/lectures/07-1-approx-posterior.html#thinning-markov-chains",
    "title": "Approximating the Posterior",
    "section": "Thinning Markov Chains",
    "text": "Thinning Markov Chains\n\nLet’s thin our original results, bb_sim, to every tenth value using the thin argument in stan().\n\n\nthinned_sim &lt;- stan(model_code = bb_model, data = list(Y = 9), \n                    chains = 4, iter = 5000*2, seed = 84735, thin = 10)\n\nmcmc_trace(thinned_sim, pars = \"pi\")\nmcmc_acf(thinned_sim, pars = \"pi\")"
  },
  {
    "objectID": "files/lectures/07-1-approx-posterior.html#thinning-markov-chains-1",
    "href": "files/lectures/07-1-approx-posterior.html#thinning-markov-chains-1",
    "title": "Approximating the Posterior",
    "section": "Thinning Markov Chains",
    "text": "Thinning Markov Chains\n\nLet’s thin our original results, bb_sim, to every tenth value using the thin argument in stan().\n\n\nthinned_sim &lt;- stan(model_code = bb_model, data = list(Y = 9), \n                    chains = 4, iter = 5000*2, seed = 84735, thin = 10)\n\n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 1e-06 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.01 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 1: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 1: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 1: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 1: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 1: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 1: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 1: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 1: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 1: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 1: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 1: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.01 seconds (Warm-up)\nChain 1:                0.011 seconds (Sampling)\nChain 1:                0.021 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 1e-06 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.01 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 2: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 2: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 2: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 2: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 2: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 2: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 2: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 2: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 2: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 2: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 2: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.01 seconds (Warm-up)\nChain 2:                0.011 seconds (Sampling)\nChain 2:                0.021 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 0 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 3: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 3: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 3: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 3: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 3: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 3: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 3: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 3: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 3: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 3: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 3: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 0.01 seconds (Warm-up)\nChain 3:                0.009 seconds (Sampling)\nChain 3:                0.019 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 0 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 4: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 4: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 4: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 4: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 4: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 4: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 4: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 4: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 4: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 4: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 4: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 0.01 seconds (Warm-up)\nChain 4:                0.011 seconds (Sampling)\nChain 4:                0.021 seconds (Total)\nChain 4:"
  },
  {
    "objectID": "files/lectures/07-1-approx-posterior.html#thinning-markov-chains-2",
    "href": "files/lectures/07-1-approx-posterior.html#thinning-markov-chains-2",
    "title": "Approximating the Posterior",
    "section": "Thinning Markov Chains",
    "text": "Thinning Markov Chains\n\nLet’s thin our original results, bb_sim, to every tenth value using the thin argument in stan()."
  },
  {
    "objectID": "files/lectures/07-1-approx-posterior.html#thinning-markov-chains-3",
    "href": "files/lectures/07-1-approx-posterior.html#thinning-markov-chains-3",
    "title": "Approximating the Posterior",
    "section": "Thinning Markov Chains",
    "text": "Thinning Markov Chains\n\nWarning!\n\nThe benefits of reduced autocorrelation do not necessarily outweigh the loss of chain values.\ni.e., 5,000 Markov chain values with stronger autocorrelation may be a better posterior approximation than 500 chain values with weaker autocorrelation.\n\nThe effectiveness depends on the algorithm used to construct the Markov chain.\n\nFolks advise against thinning unless you need memory space on your computer."
  },
  {
    "objectID": "files/lectures/07-1-approx-posterior.html#hatr",
    "href": "files/lectures/07-1-approx-posterior.html#hatr",
    "title": "Approximating the Posterior",
    "section": "\\hat{R}",
    "text": "\\hat{R}\n\\hat{R} \\approx \\sqrt{\\frac{\\text{var}_{\\text{combined}}}{\\text{var}_{\\text{within}}}}\n\nwhere\n\n\\text{var}_{\\text{combined}} is the variability in \\theta across all chains combined.\n\\text{var}_{\\text{within}} is the typical variability within any individual chain.\n\n\\hat{R} compares the variability in sampled \\theta values across all chains combined with the variability within each individual change.\n\nIdeally, \\hat{R} \\approx 1, showing stability across chains.\n\\hat{R} &gt; 1 indicates instability with the variability in the combined chains larger than that of the variability within the chains.\n\\hat{R} &gt; 1.05 raises red flags about the stability of the simulation."
  },
  {
    "objectID": "files/lectures/07-1-approx-posterior.html#hatr-1",
    "href": "files/lectures/07-1-approx-posterior.html#hatr-1",
    "title": "Approximating the Posterior",
    "section": "\\hat{R}",
    "text": "\\hat{R}\n\nWe can use the rhat() function from the bayesplot package to find \\hat{R}.\n\n\nrhat(bb_sim, pars = \"pi\")\n\n[1] 1.000245\n\n\n\nWe can see that our simulation is stable.\nIf we were to find \\hat{R} for the other (obviously bad) simulation, it would be 5.35 😱"
  },
  {
    "objectID": "files/lectures/07-1-approx-posterior.html#homework",
    "href": "files/lectures/07-1-approx-posterior.html#homework",
    "title": "Approximating the Posterior",
    "section": "Homework",
    "text": "Homework\n\n6.5\n6.6\n6.7\n6.13\n6.14\n6.15\n6.17"
  },
  {
    "objectID": "files/lectures/05-2-balance-squentiality.html#introduction",
    "href": "files/lectures/05-2-balance-squentiality.html#introduction",
    "title": "Balance and Sequentiality in Bayesian Analysis",
    "section": "Introduction",
    "text": "Introduction\n\nOn Monday, we talked about the Beta-Binomial model for binary outcomes with an unknown probability of success, \\pi.\nWe will now discuss sequentality in Bayesian analyses.\nWorking example:\n\nIn Alison Bechdel’s 1985 comic strip The Rule, a character states that they only see a movie if it satisfies the following three rules (Bechdel 1986):\n\nthe movie has to have at least two women in it;\nthese two women talk to each other; and\nthey talk about something besides a man.\n\nThinking of movies you’ve watched, what percentage of all recent movies do you think pass the Bechdel test? Is it closer to 10%, 50%, 80%, or 100%?"
  },
  {
    "objectID": "files/lectures/05-2-balance-squentiality.html#introduction-example",
    "href": "files/lectures/05-2-balance-squentiality.html#introduction-example",
    "title": "Balance and Sequentiality in Bayesian Analysis",
    "section": "Introduction: Example",
    "text": "Introduction: Example\n\nLet \\pi, a random value between 0 and 1, denote the unknown proportion of recent movies that pass the Bechdel test.\nThree friends (feminist, clueless, and optimist) have some prior ideas about \\pi.\n\nReflecting upon movies that he has seen in the past, the feminist understands that the majority lack strong women characters.\nThe clueless doesn’t really recall the movies they’ve seen, and so are unsure whether passing the Bechdel test is common or uncommon.\nLastly, the optimist thinks that the Bechdel test is a really low bar for the representation of women in film, and thus assumes almost all movies pass the test."
  },
  {
    "objectID": "files/lectures/05-2-balance-squentiality.html#introduction-example-1",
    "href": "files/lectures/05-2-balance-squentiality.html#introduction-example-1",
    "title": "Balance and Sequentiality in Bayesian Analysis",
    "section": "Introduction: Example",
    "text": "Introduction: Example\n\nGraph the following priors:\n\n\nplot_beta(alpha = 1, beta = 1)\nplot_beta(alpha = 5, beta = 11)\nplot_beta(alpha = 14, beta = 1)\n\n\nWhich prior belongs to each friend?\n\nReflecting upon movies that he has seen in the past, the feminist understands that the majority lack strong women characters.\nThe clueless doesn’t really recall the movies they’ve seen, and so are unsure whether passing the Bechdel test is common or uncommon.\nLastly, the optimist thinks that the Bechdel test is a really low bar for the representation of women in film, and thus assumes almost all movies pass the test."
  },
  {
    "objectID": "files/lectures/05-2-balance-squentiality.html#introduction-example-2",
    "href": "files/lectures/05-2-balance-squentiality.html#introduction-example-2",
    "title": "Balance and Sequentiality in Bayesian Analysis",
    "section": "Introduction: Example",
    "text": "Introduction: Example\n\nThe clueless doesn’t really recall the movies they’ve seen, and so are unsure whether passing the Bechdel test is common or uncommon."
  },
  {
    "objectID": "files/lectures/05-2-balance-squentiality.html#introduction-example-3",
    "href": "files/lectures/05-2-balance-squentiality.html#introduction-example-3",
    "title": "Balance and Sequentiality in Bayesian Analysis",
    "section": "Introduction: Example",
    "text": "Introduction: Example\n\nReflecting upon movies that he has seen in the past, the feminist understands that the majority lack strong women characters."
  },
  {
    "objectID": "files/lectures/05-2-balance-squentiality.html#introduction-example-4",
    "href": "files/lectures/05-2-balance-squentiality.html#introduction-example-4",
    "title": "Balance and Sequentiality in Bayesian Analysis",
    "section": "Introduction: Example",
    "text": "Introduction: Example\n\nLastly, the optimist thinks that the Bechdel test is a really low bar for the representation of women in film, and thus assumes almost all movies pass the test."
  },
  {
    "objectID": "files/lectures/05-2-balance-squentiality.html#introduction-example-5",
    "href": "files/lectures/05-2-balance-squentiality.html#introduction-example-5",
    "title": "Balance and Sequentiality in Bayesian Analysis",
    "section": "Introduction: Example",
    "text": "Introduction: Example\n\nThe analysts agree to review a sample of n recent movies and record Y, the number that pass the Bechdel test.\n\nBecause the outcome is yes/no, the binomial distribution is appropriate for the data distribution.\nWe aren’t sure what the population proportion, \\pi, is, so we will not restrict it to a fixed value.\n\nBecause we know \\pi \\in [0, 1], the beta distribution is appropriate for the prior distribution.\n\n\n\n\n\\begin{align*}\nY|\\pi &\\sim \\text{Bin}(n, \\pi) \\\\\n\\pi &\\sim \\text{Beta}(\\alpha, \\beta)\n\\end{align*}"
  },
  {
    "objectID": "files/lectures/05-2-balance-squentiality.html#introduction-example-6",
    "href": "files/lectures/05-2-balance-squentiality.html#introduction-example-6",
    "title": "Balance and Sequentiality in Bayesian Analysis",
    "section": "Introduction: Example",
    "text": "Introduction: Example\n\nBecause we know \\pi \\in [0, 1], the beta distribution is appropriate for the prior distribution.\n\n\n\\begin{align*}\nY|\\pi &\\sim \\text{Bin}(n, \\pi) \\\\\n\\pi &\\sim \\text{Beta}(\\alpha, \\beta)\n\\end{align*}\n\n\nFrom the previous chapter, we know that this results in the following posterior distribution\n\n\n\\pi | (Y=y) \\sim \\text{Beta}(\\alpha+y, \\beta+n-y)"
  },
  {
    "objectID": "files/lectures/05-2-balance-squentiality.html#introduction-example-7",
    "href": "files/lectures/05-2-balance-squentiality.html#introduction-example-7",
    "title": "Balance and Sequentiality in Bayesian Analysis",
    "section": "Introduction: Example",
    "text": "Introduction: Example\n\nWait!!\n\nEveryone gets their own prior?\n… is there a “correct” prior?\n…… is the Bayesian world always this subjective?"
  },
  {
    "objectID": "files/lectures/05-2-balance-squentiality.html#introduction-example-8",
    "href": "files/lectures/05-2-balance-squentiality.html#introduction-example-8",
    "title": "Balance and Sequentiality in Bayesian Analysis",
    "section": "Introduction: Example",
    "text": "Introduction: Example\n\nMore clearly defined questions that we can actually answer:\n\nTo what extent might different priors lead the analysts to three different posterior conclusions about the Bechdel test?\n\nHow might this depend upon the sample size and outcomes of the movie data they collect?\n\nTo what extent will the analysts’ posterior understandings evolve as they collect more and more data?\nWill they ever come to agreement about the representation of women in film?!"
  },
  {
    "objectID": "files/lectures/05-2-balance-squentiality.html#different-priors-to-different-posteriors",
    "href": "files/lectures/05-2-balance-squentiality.html#different-priors-to-different-posteriors",
    "title": "Balance and Sequentiality in Bayesian Analysis",
    "section": "Different Priors \\to Different Posteriors",
    "text": "Different Priors \\to Different Posteriors\n\n\nThe differing prior means show disagreement about whether \\pi is closer to 0 or 1.\nThe differing levels of prior variability show that the analysts have different degrees of certainty in their prior information."
  },
  {
    "objectID": "files/lectures/05-2-balance-squentiality.html#different-priors-to-different-posteriors-1",
    "href": "files/lectures/05-2-balance-squentiality.html#different-priors-to-different-posteriors-1",
    "title": "Balance and Sequentiality in Bayesian Analysis",
    "section": "Different Priors \\to Different Posteriors",
    "text": "Different Priors \\to Different Posteriors\n\n\nInformative prior: reflects specific information about the unknown variable with high certainty, i.e., low variability."
  },
  {
    "objectID": "files/lectures/05-2-balance-squentiality.html#different-priors-to-different-posteriors-2",
    "href": "files/lectures/05-2-balance-squentiality.html#different-priors-to-different-posteriors-2",
    "title": "Balance and Sequentiality in Bayesian Analysis",
    "section": "Different Priors \\to Different Posteriors",
    "text": "Different Priors \\to Different Posteriors\n\n\nVague or diffuse prior: reflects little specific information about the unknown variable.\n\nA flat prior, which assigns equal prior plausibility to all possible values of the variable, is a special case.\nThis is effectively saying “🤷.”"
  },
  {
    "objectID": "files/lectures/05-2-balance-squentiality.html#different-priors-to-different-posteriors-3",
    "href": "files/lectures/05-2-balance-squentiality.html#different-priors-to-different-posteriors-3",
    "title": "Balance and Sequentiality in Bayesian Analysis",
    "section": "Different Priors \\to Different Posteriors",
    "text": "Different Priors \\to Different Posteriors\n\nOkay, great - we have different priors.\n\nHow do the different priors affect the posterior?\n\nWe have data from FiveThirtyEight, reporting results of the Bechdel test."
  },
  {
    "objectID": "files/lectures/05-2-balance-squentiality.html#different-priors-to-different-posteriors-4",
    "href": "files/lectures/05-2-balance-squentiality.html#different-priors-to-different-posteriors-4",
    "title": "Balance and Sequentiality in Bayesian Analysis",
    "section": "Different Priors \\to Different Posteriors",
    "text": "Different Priors \\to Different Posteriors\n\nSo how many pass the test in this sample?\n\n\nbechdel20 %&gt;% tabyl(binary) %&gt;% adorn_totals(\"row\")"
  },
  {
    "objectID": "files/lectures/05-2-balance-squentiality.html#different-priors-to-different-posteriors-5",
    "href": "files/lectures/05-2-balance-squentiality.html#different-priors-to-different-posteriors-5",
    "title": "Balance and Sequentiality in Bayesian Analysis",
    "section": "Different Priors \\to Different Posteriors",
    "text": "Different Priors \\to Different Posteriors\n\nLet’s look at the graphs of just the prior and likelihood.\n\n\nplot_beta_binomial(alpha = 5, beta = 11, y = 9, n = 20, posterior = FALSE) + theme_bw()\nplot_beta_binomial(alpha = 1, beta = 1, y = 9, n = 20, posterior = FALSE) + theme_bw()\nplot_beta_binomial(alpha = 14, beta = 1, y = 9, n = 20, posterior = FALSE) + theme_bw()\n\n\nQuestions to think about:\n\nWhose posterior do you anticipate will look the most like the scaled likelihood?\nWhose do you anticipate will look the least like the scaled likelihood?"
  },
  {
    "objectID": "files/lectures/05-2-balance-squentiality.html#different-priors-to-different-posteriors-6",
    "href": "files/lectures/05-2-balance-squentiality.html#different-priors-to-different-posteriors-6",
    "title": "Balance and Sequentiality in Bayesian Analysis",
    "section": "Different Priors \\to Different Posteriors",
    "text": "Different Priors \\to Different Posteriors\n\nLet’s look at the graphs of just the prior and likelihood."
  },
  {
    "objectID": "files/lectures/05-2-balance-squentiality.html#different-priors-to-different-posteriors-7",
    "href": "files/lectures/05-2-balance-squentiality.html#different-priors-to-different-posteriors-7",
    "title": "Balance and Sequentiality in Bayesian Analysis",
    "section": "Different Priors \\to Different Posteriors",
    "text": "Different Priors \\to Different Posteriors\n\nLet’s look at the graphs of just the prior and likelihood."
  },
  {
    "objectID": "files/lectures/05-2-balance-squentiality.html#different-priors-to-different-posteriors-8",
    "href": "files/lectures/05-2-balance-squentiality.html#different-priors-to-different-posteriors-8",
    "title": "Balance and Sequentiality in Bayesian Analysis",
    "section": "Different Priors \\to Different Posteriors",
    "text": "Different Priors \\to Different Posteriors\n\nLet’s look at the graphs of just the prior and likelihood."
  },
  {
    "objectID": "files/lectures/05-2-balance-squentiality.html#different-priors-to-different-posteriors-9",
    "href": "files/lectures/05-2-balance-squentiality.html#different-priors-to-different-posteriors-9",
    "title": "Balance and Sequentiality in Bayesian Analysis",
    "section": "Different Priors \\to Different Posteriors",
    "text": "Different Priors \\to Different Posteriors\n\nFind the posterior distributions. (i.e., What are the updated parameters?)\n\n\n\n\n\n\nAnalyst\n\n\nPrior\n\n\nPosterior\n\n\n\n\n\n\nthe feminist\n\n\nBeta(5, 11)\n\n\nBeta(14, 22)\n\n\n\n\nthe clueless\n\n\nBeta(1, 1)\n\n\nBeta(10, 12)\n\n\n\n\nthe optimist\n\n\nBeta(14, 1)\n\n\nBeta(23, 12)\n\n\n\n\n\n\nLet’s now explore what the posteriors look like.\n\n\nplot_beta_binomial(alpha = 5, beta = 11, y = 9, n = 20) + theme_bw()\nplot_beta_binomial(alpha = 1, beta = 1, y = 9, n = 20) + theme_bw()\nplot_beta_binomial(alpha = 14, beta = 1, y = 9, n = 20) + theme_bw()"
  },
  {
    "objectID": "files/lectures/05-2-balance-squentiality.html#different-priors-to-different-posteriors-10",
    "href": "files/lectures/05-2-balance-squentiality.html#different-priors-to-different-posteriors-10",
    "title": "Balance and Sequentiality in Bayesian Analysis",
    "section": "Different Priors \\to Different Posteriors",
    "text": "Different Priors \\to Different Posteriors\n\nLet’s now explore what the posteriors look like."
  },
  {
    "objectID": "files/lectures/05-2-balance-squentiality.html#different-priors-to-different-posteriors-11",
    "href": "files/lectures/05-2-balance-squentiality.html#different-priors-to-different-posteriors-11",
    "title": "Balance and Sequentiality in Bayesian Analysis",
    "section": "Different Priors \\to Different Posteriors",
    "text": "Different Priors \\to Different Posteriors\n\nLet’s now explore what the posteriors look like."
  },
  {
    "objectID": "files/lectures/05-2-balance-squentiality.html#different-priors-to-different-posteriors-12",
    "href": "files/lectures/05-2-balance-squentiality.html#different-priors-to-different-posteriors-12",
    "title": "Balance and Sequentiality in Bayesian Analysis",
    "section": "Different Priors \\to Different Posteriors",
    "text": "Different Priors \\to Different Posteriors\n\nLet’s now explore what the posteriors look like."
  },
  {
    "objectID": "files/lectures/05-2-balance-squentiality.html#different-priors-to-different-posteriors-13",
    "href": "files/lectures/05-2-balance-squentiality.html#different-priors-to-different-posteriors-13",
    "title": "Balance and Sequentiality in Bayesian Analysis",
    "section": "Different Priors \\to Different Posteriors",
    "text": "Different Priors \\to Different Posteriors\n\nIn addition to priors affecting our posterior distributions… the data also affects it.\nLet’s now consider three new analysts: they all share the optimistic Beta(14, 1) for \\pi, however, they have access to different data.\n\nMorteza reviews n = 13 movies from the year 1991, among which Y=6 (about 46%) pass the Bechdel.\nNadide reviews n = 63 movies from the year 2001, among which Y=29 (about 46%) pass the Bechdel.\nUrsula reviews n = 99 movies from the year 2013, among which Y=46 (about 46%) pass the Bechdel.\n\nHow will the different data affect the posterior distributions?"
  },
  {
    "objectID": "files/lectures/05-2-balance-squentiality.html#different-priors-to-different-posteriors-14",
    "href": "files/lectures/05-2-balance-squentiality.html#different-priors-to-different-posteriors-14",
    "title": "Balance and Sequentiality in Bayesian Analysis",
    "section": "Different Priors \\to Different Posteriors",
    "text": "Different Priors \\to Different Posteriors\n\nHow will the different data affect the posterior distributions?\n\n\nplot_beta_binomial(alpha = 14, beta = 1, y = 6, n = 13) + theme_bw()\nplot_beta_binomial(alpha = 14, beta = 1, y = 29, n = 63) + theme_bw()\nplot_beta_binomial(alpha = 14, beta = 1, y = 46, n = 99) + theme_bw()\n\n\nWhich posterior is the most in sync with their data?\nWhich posterior is the least in sync with their data?"
  },
  {
    "objectID": "files/lectures/05-2-balance-squentiality.html#different-priors-to-different-posteriors-15",
    "href": "files/lectures/05-2-balance-squentiality.html#different-priors-to-different-posteriors-15",
    "title": "Balance and Sequentiality in Bayesian Analysis",
    "section": "Different Priors \\to Different Posteriors",
    "text": "Different Priors \\to Different Posteriors\n\nHow will the different data affect the posterior distributions?"
  },
  {
    "objectID": "files/lectures/05-2-balance-squentiality.html#different-priors-to-different-posteriors-16",
    "href": "files/lectures/05-2-balance-squentiality.html#different-priors-to-different-posteriors-16",
    "title": "Balance and Sequentiality in Bayesian Analysis",
    "section": "Different Priors \\to Different Posteriors",
    "text": "Different Priors \\to Different Posteriors\n\nHow will the different data affect the posterior distributions?"
  },
  {
    "objectID": "files/lectures/05-2-balance-squentiality.html#different-priors-to-different-posteriors-17",
    "href": "files/lectures/05-2-balance-squentiality.html#different-priors-to-different-posteriors-17",
    "title": "Balance and Sequentiality in Bayesian Analysis",
    "section": "Different Priors \\to Different Posteriors",
    "text": "Different Priors \\to Different Posteriors\n\nHow will the different data affect the posterior distributions?"
  },
  {
    "objectID": "files/lectures/05-2-balance-squentiality.html#different-priors-to-different-posteriors-18",
    "href": "files/lectures/05-2-balance-squentiality.html#different-priors-to-different-posteriors-18",
    "title": "Balance and Sequentiality in Bayesian Analysis",
    "section": "Different Priors \\to Different Posteriors",
    "text": "Different Priors \\to Different Posteriors\n\nFind the posterior distributions. (i.e., What are the updated parameters?)\n\nRecall that all use the Beta(14, 1) prior.\n\n\n\n\n\n\n\n\nAnalyst\n\n\n\n\nData\n\n\n\n\nPosterior\n\n\n\n\n\n\n\nMorteza\n\n\nY=6 of n=13\n\n\nBeta(20, 8)\n\n\n\n\nNadide\n\n\nY=29 of n=63\n\n\nBeta(45, 35)\n\n\n\n\nUrsula\n\n\nY=46 of n=99\n\n\nBeta(60, 54)\n\n\n\n\n\n\nLet’s also explore what the posteriors look like.\n\n\nplot_beta_binomial(alpha = 14, beta = 1, y = 6, n = 13) + theme_bw() \nplot_beta_binomial(alpha = 14, beta = 1, y = 29, n = 63) + theme_bw()\nplot_beta_binomial(alpha = 14, beta = 1, y = 46, n = 99) + theme_bw()"
  },
  {
    "objectID": "files/lectures/05-2-balance-squentiality.html#different-priors-to-different-posteriors-19",
    "href": "files/lectures/05-2-balance-squentiality.html#different-priors-to-different-posteriors-19",
    "title": "Balance and Sequentiality in Bayesian Analysis",
    "section": "Different Priors \\to Different Posteriors",
    "text": "Different Priors \\to Different Posteriors\n\nLet’s explore what the posteriors look like."
  },
  {
    "objectID": "files/lectures/05-2-balance-squentiality.html#different-priors-to-different-posteriors-20",
    "href": "files/lectures/05-2-balance-squentiality.html#different-priors-to-different-posteriors-20",
    "title": "Balance and Sequentiality in Bayesian Analysis",
    "section": "Different Priors \\to Different Posteriors",
    "text": "Different Priors \\to Different Posteriors\n\nLet’s explore what the posteriors look like."
  },
  {
    "objectID": "files/lectures/05-2-balance-squentiality.html#different-priors-to-different-posteriors-21",
    "href": "files/lectures/05-2-balance-squentiality.html#different-priors-to-different-posteriors-21",
    "title": "Balance and Sequentiality in Bayesian Analysis",
    "section": "Different Priors \\to Different Posteriors",
    "text": "Different Priors \\to Different Posteriors\n\nLet’s explore what the posteriors look like."
  },
  {
    "objectID": "files/lectures/05-2-balance-squentiality.html#different-priors-to-different-posteriors-22",
    "href": "files/lectures/05-2-balance-squentiality.html#different-priors-to-different-posteriors-22",
    "title": "Balance and Sequentiality in Bayesian Analysis",
    "section": "Different Priors \\to Different Posteriors",
    "text": "Different Priors \\to Different Posteriors\n\nWhat did we observe?\n\nAs n \\to \\infty, variance in the likelihood \\to 0.\n\nIn Morteza’s small sample of 13 movies, the likelihood function is wide.\nIn Ursula’s larger sample size of 99 movies, the likelihood function is narrower.\n\nWe see that the narrower the likelihood, the more influence the data holds over the posterior."
  },
  {
    "objectID": "files/lectures/05-2-balance-squentiality.html#striking-a-balance",
    "href": "files/lectures/05-2-balance-squentiality.html#striking-a-balance",
    "title": "Balance and Sequentiality in Bayesian Analysis",
    "section": "Striking a Balance",
    "text": "Striking a Balance\n\n\nOverall message: no matter the strength of and discrepancies among their prior understanding of \\pi, analysts will come to a common posterior understanding in light of strong data."
  },
  {
    "objectID": "files/lectures/05-2-balance-squentiality.html#striking-a-balance-1",
    "href": "files/lectures/05-2-balance-squentiality.html#striking-a-balance-1",
    "title": "Balance and Sequentiality in Bayesian Analysis",
    "section": "Striking a Balance",
    "text": "Striking a Balance\n\nThe posterior can either favor the data or the prior.\n\nThe rate at which the posterior balance tips in favor of the data depends upon the prior.\n\nLeft to right on the graph, the sample size increases from n=13 to n=99 movies, while preserving the proportion that pass (\\approx 0.46).\n\nThe likelihood’s insistence and the data’s influence over the posterior increase with sample size.\nThis also means that the influence of our prior understanding diminishes as we gather new data.\n\nTop to bottom on the graph, priors move from informative (Beta(14,1)) to vague (Beta(1,1)).\n\nNaturally, the more informative the prior, the greater its influence on the posterior."
  },
  {
    "objectID": "files/lectures/05-2-balance-squentiality.html#introduction-sequentiality",
    "href": "files/lectures/05-2-balance-squentiality.html#introduction-sequentiality",
    "title": "Balance and Sequentiality in Bayesian Analysis",
    "section": "Introduction: Sequentiality",
    "text": "Introduction: Sequentiality\n\nLet’s now turn our thinking to - okay, we’ve updated our beliefs… but now we have new data!\nThe evolution in our posterior understanding happens incrementally, as we accumulate new data.\n\nScientists’ understanding of climate change has evolved over the span of decades as they gain new information.\nPresidential candidates’ understanding of their chances of winning an election evolve over months as new poll results become available."
  },
  {
    "objectID": "files/lectures/05-2-balance-squentiality.html#introduction-sequentiality-1",
    "href": "files/lectures/05-2-balance-squentiality.html#introduction-sequentiality-1",
    "title": "Balance and Sequentiality in Bayesian Analysis",
    "section": "Introduction: Sequentiality",
    "text": "Introduction: Sequentiality\n\nLet’s revisit Milgram’s behavioral study of obedience from Chapter 3. Recall, \\pi represents the proportion of people that will obey authority, even if it means bringing harm to others.\nPrior to Milgram’s experiments, our fictional psychologist expected that few people would obey authority in the face of harming another: \\pi \\sim \\text{Beta}(1,10).\nNow, suppose that the psychologist collected the data incrementally, day by day, over a three-day period.\nFind the following posterior distributions, each building off the last:\n\nDay 0: \\text{Beta}(1,10).\nDay 1: Y=1 out of n=10.\nDay 2: Y=17 out of n=20.\nDay 3: Y=8 out of n=10."
  },
  {
    "objectID": "files/lectures/05-2-balance-squentiality.html#introduction-sequentiality-2",
    "href": "files/lectures/05-2-balance-squentiality.html#introduction-sequentiality-2",
    "title": "Balance and Sequentiality in Bayesian Analysis",
    "section": "Introduction: Sequentiality",
    "text": "Introduction: Sequentiality\n\nFind the following posterior distributions, each building off the last:\n\nDay 0: \\text{Beta}(1,10).\nDay 1: Y=1 out of n=10: \\text{Beta}(1,10) \\to \\text{Beta}(2, 19).\nDay 2: Y=17 out of n=20: \\text{Beta}(2, 19) \\to \\text{Beta}(19, 22).\nDay 3: Y=8 out of n=10: \\text{Beta}(19, 22) \\to \\text{Beta}(27, 24).\n\nRecall from Chapter 3, our posterior was \\text{Beta}(27,24)!"
  },
  {
    "objectID": "files/lectures/05-2-balance-squentiality.html#sequential-bayesian-analysis-or-bayesian-learning",
    "href": "files/lectures/05-2-balance-squentiality.html#sequential-bayesian-analysis-or-bayesian-learning",
    "title": "Balance and Sequentiality in Bayesian Analysis",
    "section": "Sequential Bayesian Analysis or Bayesian Learning",
    "text": "Sequential Bayesian Analysis or Bayesian Learning\n\nIn a sequential Bayesian analysis, a posterior model is updated incrementally as more data come in.\n\nWith each new piece of data, the previous posterior model reflecting our understanding prior to observing this data becomes the new prior model.\n\nThis is why we love Bayesian!\n\nWe evolve our thinking as new data come in.\n\nThese types of sequential analyses also uphold two fundamental properties:\n\nThe final posterior model is data order invariant,\n\nThe final posterior only depends upon the cumulative data."
  },
  {
    "objectID": "files/lectures/05-2-balance-squentiality.html#sequential-bayesian-analysis-or-bayesian-learning-1",
    "href": "files/lectures/05-2-balance-squentiality.html#sequential-bayesian-analysis-or-bayesian-learning-1",
    "title": "Balance and Sequentiality in Bayesian Analysis",
    "section": "Sequential Bayesian Analysis or Bayesian Learning",
    "text": "Sequential Bayesian Analysis or Bayesian Learning\n\nIn order:\n\nDay 0: \\text{Beta}(1,10).\nDay 1: Y=1 out of n=10: \\text{Beta}(1,10) \\to \\text{Beta}(2, 19).\nDay 2: Y=17 out of n=20: \\text{Beta}(2, 19) \\to \\text{Beta}(19, 22).\nDay 3: Y=8 out of n=10: \\text{Beta}(19, 22) \\to \\text{Beta}(27, 24).\n\nOut of order:\n\nDay 0: \\text{Beta}(1,10).\nDay 3: Y=8 out of n=10: \\text{Beta}(1,10) \\to \\text{Beta}(9, 12).\nDay 2: Y=17 out of n=20: \\text{Beta}(9, 12) \\to \\text{Beta}(26, 15).\nDay 1: Y=1 out of n=10: \\text{Beta}(26, 15) \\to \\text{Beta}(27, 24)."
  },
  {
    "objectID": "files/lectures/05-2-balance-squentiality.html#sequential-bayesian-analysis-or-bayesian-learning-2",
    "href": "files/lectures/05-2-balance-squentiality.html#sequential-bayesian-analysis-or-bayesian-learning-2",
    "title": "Balance and Sequentiality in Bayesian Analysis",
    "section": "Sequential Bayesian Analysis or Bayesian Learning",
    "text": "Sequential Bayesian Analysis or Bayesian Learning"
  },
  {
    "objectID": "files/lectures/05-2-balance-squentiality.html#example-mario-kart",
    "href": "files/lectures/05-2-balance-squentiality.html#example-mario-kart",
    "title": "Balance and Sequentiality in Bayesian Analysis",
    "section": "Example: Mario Kart",
    "text": "Example: Mario Kart\n\nIn Mario Kart 8 Deluxe, item boxes give different items depending on race position. To reduce the “position bias,” only item boxes opened while the racer was in mid-pack (positions 4–10) were recorded.\nYou want to estimate the probability that an item box yields a Red Shell. When playing the Special Cup, only 31 red shells were seen in 114 boxes opened by mid-pack racers.\nFind the posterior distribution under two priors:\n\nFlat/uninformative prior, Beta(1,1).\nBeta(\\alpha, \\beta) of your choosing.\n\nHow different are the posterior distributions?"
  },
  {
    "objectID": "files/lectures/05-2-balance-squentiality.html#example-mario-kart-1",
    "href": "files/lectures/05-2-balance-squentiality.html#example-mario-kart-1",
    "title": "Balance and Sequentiality in Bayesian Analysis",
    "section": "Example: Mario Kart",
    "text": "Example: Mario Kart\n\nSuppose that we know the individual data points for the Special Cup:\n\nCloudtop Cruise, of 28 boxes, 6 had a red shell.\nBone-Dry Dunes, of 27 boxes, 8 had a red shell.\nBowser’s Castle, of 29 boxes, 12 had a red shell.\nRainbow Road, of 30 boxes, 5 had a red shell.\n\nProve sequentiality to yourself.\n\nAnalyze the data race-by-race in this order: Cloudtop Cruise, Bone-Dry Dunes, Bowser’s Castle, and Rainbow Road.\nAnalyze the data race-by-race in this order: Rainbow Road, Bowser’s Castle, Cloudtop Cruise, and Bone-Dry Dunes."
  },
  {
    "objectID": "files/lectures/05-2-balance-squentiality.html#wrap-up",
    "href": "files/lectures/05-2-balance-squentiality.html#wrap-up",
    "title": "Balance and Sequentiality in Bayesian Analysis",
    "section": "Wrap Up",
    "text": "Wrap Up\n\nToday we have discussed balance and sequentiality.\nRemember that order of data inclusion does not matter – we will end up with the same posterior.\nWe have seen that prior specification “matters” but there will not be a large difference in the posterior distribution when priors are more similar to one another.\nNext week:\n\nGamma-Poisson\nNormal-Normal\nWhat to do with the posterior distribution."
  },
  {
    "objectID": "files/lectures/05-2-balance-squentiality.html#homework-practice",
    "href": "files/lectures/05-2-balance-squentiality.html#homework-practice",
    "title": "Balance and Sequentiality in Bayesian Analysis",
    "section": "Homework / Practice",
    "text": "Homework / Practice\n\nFrom the Bayes Rules! textbook:\n\n4.3\n4.4\n4.6\n4.9\n4.15\n4.16\n4.17\n4.18\n4.19"
  },
  {
    "objectID": "files/assignments/assignment2.html",
    "href": "files/assignments/assignment2.html",
    "title": "Assignment 2: Bayesian Inference",
    "section": "",
    "text": "All data for this project can be found here: Google Sheets\n1. Belle has taken over the castle’s grand library and is determined to restore its warm, welcoming glow. The enchanted lanterns are meant to light automatically each evening, but ever since the enchantress’s curse, not all of them behave reliably.\nFrom past experience, Lumière estimates that somewhere between 50% and 70% of the lanterns light properly on any given night. When he is pressed for a single (point) estimate, he says that about 60% of the lanterns light properly. However, Belle suspects that things are improving as the castle’s magic grows stronger… perhaps the true proportion of working lanterns is now higher?\nTo test this, she records data for a week evenings. Each night, she notes how many of the 10 enchanted lanterns successfully light when the sun sets.\n1a. What modeling approach is appropriate for this scenario? Explain why.\nInsert your answer here.\n1b. Find an appropriate prior given the information provided. Justify your choice.\nInsert your answer here.\n1c. Find the posterior distribution in light of the observed data.\nInsert your answer here.\n1d. Construct the appropriate hypothesis test (in the Bayesian framework) to determine if the proportion of lanterns that successfully light has increased.\n\nHypotheses:\n\nH_0:\nH_1:\n\nDecision information:\n\nPrior probabilities:\n\nPrior odds:\n\nPosterior probabilities:\n\nPosterior odds:\n\nBayes Factor:\n95% credible interval:\n\nConclusion and interpretation:\n\n2. As the castle’s magic returns, Cogsworth has been busy ensuring that every enchanted clock in the west wing ticks properly. Each evening, he listens carefully for the number of clocks that chime on time during the first hour after sunset.\nBefore Belle’s arrival, the castle staff believed that the average number of properly chiming clocks per hour was around 5, but they are not too confident about that estimate. However, as the curse weakens, Cogsworth suspects the clocks are performing better than before. To check, he records data over seven evenings.\n2a. What modeling approach is appropriate for this scenario? Explain why.\nInsert your answer here.\n2b. Find an appropriate prior given the information provided. Justify your choice.\nInsert your answer here.\n2c. Find the posterior distribution in light of the observed data.\nInsert your answer here.\n2d. Construct the appropriate hypothesis test (in the Bayesian framework) to determine if the number of clocks that properly chime has increased.\n\nHypotheses:\n\nH_0:\nH_1:\n\nDecision information:\n\nPrior probabilities:\n\nPrior odds:\n\nPosterior probabilities:\n\nPosterior odds:\n\nBayes Factor:\n95% credible interval:\n\nConclusion and interpretation:\n\n3. Mrs. Potts prides herself on brewing tea at just the right temperature: not too hot to startle a guest, not too cool to lose its charm. She believes that when the castle’s magic was weaker, the average serving temperature of her enchanted teapot’s pours was about 85°C, with a standard deviation of about 3°C.\nNow that the curse has nearly lifted, Mrs. Potts suspects her tea is coming out hotter on average. To investigate, she measures the temperature (in °C) of her pours over seven evenings.\n3a. What modeling approach is appropriate for this scenario? Explain why.\nInsert your answer here.\n3b. Find an appropriate prior given the information provided. Justify your choice.\nInsert your answer here.\n3c. Find the posterior distribution in light of the observed data.\nInsert your answer here.\n3d. Construct the appropriate hypothesis test (in the Bayesian framework) to determine if the tea is hotter when poured.\n\nHypotheses:\n\nH_0:\nH_1:\n\nDecision information:\n\nPrior probabilities:\n\nPrior odds:\n\nPosterior probabilities:\n\nPosterior odds:\n\nBayes Factor:\n95% credible interval:\n\nConclusion and interpretation:"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "STA6349 - Applied Bayesian Analysis - Fall 2025",
    "section": "",
    "text": "Welcome to Applied Bayesian Analysis for Fall 2025!\nThe initial development of this course was supported by the 2023 BATS program, funded by NSF IUSE: EHR program with award numbers 2215879, 2215920, and 2215709."
  },
  {
    "objectID": "files/assignments/assignment1.html",
    "href": "files/assignments/assignment1.html",
    "title": "Assignment 1: Probability Distributions",
    "section": "",
    "text": "Note: do not read too much into the following situations. The purpose is to get comfortable identifying distributions and finding corresponding probabilities.\nSituation 1: Walter is known for his intense personality during bowling league nights. Whether it’s a foot foul, a scoring dispute, or just general frustration with life, he frequently loses his temper and yells loudly at his teammates or opponents. Based on observations over several league nights, it has been determined that Walter loses his temper at an average rate of 4 outbursts per hour. His outbursts are unpredictable and can happen at any point during the game, but they seem to occur independently of each other.\na. What is the random variable of interest? (What is being measured?)\nReplace with your answer\nb. What distribution is appropriate and why?\nReplace with your answer\nc. What are the parameters of the distribution?\nReplace with your answer\nd. What is the probability that Walter will not have an outburst at all?\nReplace with your answer (i.e., don’t just show me R output here :))\ne. What is the probability that Walter has more than 10 outbursts during league night?\nReplace with your answer (i.e., don’t just show me R output here :))\nScenario 2: Donny often struggles to find parking near the bowling alley, especially on busy league nights. Sometimes he gets lucky and finds a spot fairly quickly, but other times he ends up circling the block several times before he can finally park. His parking times are generally right-skewed as it is much more common for it to take a shorter amount of time… but every now and then it can take quite a while. Over time, his friends have noticed a pattern: it usually takes him a few minutes, but there is a decent chance it could take significantly longer.\na. What is the random variable of interest? (What is being measured?)\nReplace with your answer\nb. What distribution is appropriate and why?\nReplace with your answer\nc. What are the parameters of the distribution? (Hint: for this situation, they are 3 and 4 (in that order)).\nReplace with your answer\nd. What is the probability that Donny will take more than 15 minutes to find a spot?\nReplace with your answer (i.e., don’t just show me R output here :))\ne. What is the probability that Donny will find a parking space quickly – i.e., that he parks within 5 minutes of arriving?\nReplace with your answer (i.e., don’t just show me R output here :))\nSituation 3: Maude is famous for her avant-garde dance performances that leave audiences both puzzled and mesmerized. Her performances at the local art venue are known for their spontaneity and lack of strict timing. Observers have noted that the duration of her performances can last anywhere between 15 and 30 minutes, with any length within this interval being equally likely. There’s no predicting exactly how long Maude will choose to perform on any given night… sometimes it’s a brief expression and sometimes it stretches to the edge of the audience’s patience.\na. What is the random variable of interest? (What is being measured?)\nReplace with your answer\nb. What distribution is appropriate and why?\nReplace with your answer\nc. What are the parameters of the distribution?\nReplace with your answer\nd. What is the probability that Maude’s performance will be on the shorter end – i.e., it lasts somewhere between 15 and 16 minutes?\nReplace with your answer (i.e., don’t just show me R output here :))\ne. What is the probability that Maude’s performance will be on the longer end – i.e., it lasts somewhere between 25 and 30 minutes?\nReplace with your answer (i.e., don’t just show me R output here :))\nScenario 4. The Dude always orders a White Russian when he’s at the bowling alley. However, the bowling alley bar is not always well-stocked. Each time The Dude places an order, there is only a 60% chance that the bartender has all the necessary ingredients on hand to make the drink (they are frequently out of cream). Throughout one particularly long evening of bowling and philosophical debates, The Dude orders 8 White Russians. He hopes that most of them will be successfully made.\na. What is the random variable of interest? (What is being measured?)\nReplace with your answer\nb. What distribution is appropriate and why?\nReplace with your answer\nc. What are the parameters of the distribution?\nReplace with your answer\nd. What is the probability that at least 5 drinks will be successfully made?\nReplace with your answer (i.e., don’t just show me R output here :))\ne. What is the probability that all 8 drinks are successfully made?\nReplace with your answer (i.e., don’t just show me R output here :))\nScenario 5: The Dude’s bowling games tend to last around 45 minutes, give or take. While the exact length can vary from game to game, it’s rare for his games to be extremely short or unusually long. Most of the time, his game lengths cluster fairly tightly around that 45-minute mark, though occasionally he’ll have a quicker match or one that drags on a little longer if the vibe calls for it, to the tune of \\pm 5 minutes.\na. What is the random variable of interest? (What is being measured?)\nReplace with your answer\nb. What distribution is appropriate and why?\nReplace with your answer\nc. What are the parameters of the distribution?\nReplace with your answer\nd. What is the probability that the game will last exactly 45 minutes?\nReplace with your answer (i.e., don’t just show me R output here :))\ne. What is the probability that the game will really drag on – that it lasts somewhere between 50 and 60 minutes?\nReplace with your answer (i.e., don’t just show me R output here :))\nScenario 6: After many laid-back bowling sessions (and several White Russians), The Dude has started keeping track of how often he lands a strike. He knows he’s not perfect, but he’s developed a pretty good feel for his overall strike success rate. It seems like his strike percentage tends to hang out somewhere between 60% and 80% most nights, though there’s still some uncertainty. Sometimes he’s on fire, sometimes he’s a little off, but overall he’s confident his long-term strike rate leans more toward success than failure.\na. What is the random variable of interest? (What is being measured?)\nReplace with your answer\nb. What distribution is appropriate and why?\nReplace with your answer\nc. What are the parameters of the distribution? (Hint: for this example, they are 7 and 3 (in that order)).\nReplace with your answer\nd. What is the probability that The Dude’s strike success rate is less than 50%?\nReplace with your answer (i.e., don’t just show me R output here :))\ne. What is the probability that The Dude’s strike success rate is above 80%?\nReplace with your answer (i.e., don’t just show me R output here :))"
  },
  {
    "objectID": "files/lectures/05-1-beta-binomial.html#introduction-beta-binomial-model",
    "href": "files/lectures/05-1-beta-binomial.html#introduction-beta-binomial-model",
    "title": "Beta-Binomial Model",
    "section": "Introduction: Beta-Binomial Model",
    "text": "Introduction: Beta-Binomial Model\n\nLast week, we learned how to think like a Bayesian.\n\nToday, we will formalize the model we muddled through last time.\n\nThis is called the Beta-Binomial model.\n\nThe Beta distribution is the prior.\nThe Binomial distribution is the data distribution (or the likeihood).\nThe posterior also follows a Beta distribution.\n\nConjugate family: When the prior and posterior are the same named distribution, but different parameters."
  },
  {
    "objectID": "files/lectures/05-1-beta-binomial.html#example-set-up",
    "href": "files/lectures/05-1-beta-binomial.html#example-set-up",
    "title": "Beta-Binomial Model",
    "section": "Example Set Up",
    "text": "Example Set Up\n\nConsider the following scenario.\n\n“Michelle” has decided to run for president and you’re her campaign manager for the state of Florida.\nAs such, you’ve conducted 30 different polls throughout the election season.\nThough Michelle’s support has hovered around 45%, she polled at around 35% in the dreariest days and around 55% in the best days on the campaign trail."
  },
  {
    "objectID": "files/lectures/05-1-beta-binomial.html#example-set-up-1",
    "href": "files/lectures/05-1-beta-binomial.html#example-set-up-1",
    "title": "Beta-Binomial Model",
    "section": "Example Set Up",
    "text": "Example Set Up\n\nPast polls provide prior information about \\pi, the proportion of Floridians that currently support Michelle.\n\nIn fact, we can reorganize this information into a formal prior probability model of \\pi.\n\nIn a previous problem, we assumed that \\pi could only be 0.2, 0.5, or 0.8, the corresponding chances of which were defined by a discrete probability model.\n\nHowever, in the reality of Michelle’s election support, \\pi \\in [0, 1].\n\nWe can reflect this reality and conduct a Bayesian analysis by constructing a continuous prior probability model of \\pi."
  },
  {
    "objectID": "files/lectures/05-1-beta-binomial.html#example-set-up-2",
    "href": "files/lectures/05-1-beta-binomial.html#example-set-up-2",
    "title": "Beta-Binomial Model",
    "section": "Example Set Up",
    "text": "Example Set Up\n\n\n\n\nA reasonable prior is represented by the curve on the right.\n\nNotice that this curve preserves the overall information and variability in the past polls, i.e., Michelle’s support, \\pi can be anywhere between 0 and 1, but is most likely around 0.45."
  },
  {
    "objectID": "files/lectures/05-1-beta-binomial.html#example-set-up-3",
    "href": "files/lectures/05-1-beta-binomial.html#example-set-up-3",
    "title": "Beta-Binomial Model",
    "section": "Example Set Up",
    "text": "Example Set Up\n\nIncorporating this more nuanced, continuous view of Michelle’s support, \\pi, will require some new tools.\n\nNo matter if our parameter \\pi is continuous or discrete, the posterior model of \\pi will combine insights from the prior and data.\n\\pi isn’t the only variable of interest that lives on [0,1].\n\nMaybe we’re interested in modeling the proportion of people that use public transit, the proportion of trains that are delayed, the proportion of people that prefer cats to dogs, etc.\n\nThe Beta-Binomial model provides the tools we need to study the proportion of interest, \\pi, in each of these settings."
  },
  {
    "objectID": "files/lectures/05-1-beta-binomial.html#beta-prior",
    "href": "files/lectures/05-1-beta-binomial.html#beta-prior",
    "title": "Beta-Binomial Model",
    "section": "Beta Prior",
    "text": "Beta Prior\n\n\n\n\nIn building the Bayesian election model of Michelle’s election support among Floridians, \\pi, we begin with the prior.\n\nOur continuous prior probability model of \\pi is specified by the probability density function (pdf).\n\nWhat values can \\pi take and which are more plausible than others?"
  },
  {
    "objectID": "files/lectures/05-1-beta-binomial.html#beta-prior-1",
    "href": "files/lectures/05-1-beta-binomial.html#beta-prior-1",
    "title": "Beta-Binomial Model",
    "section": "Beta Prior",
    "text": "Beta Prior\n\nLet \\pi be a random variable, where \\pi \\in [0, 1].\nThe variability in \\pi may be captured by a Beta model with shape hyperparameters \\alpha &gt; 0 and \\beta &gt; 0,\n\nhyperparameter: a parameter used in a prior model.\n\n\n\\pi \\sim \\text{Beta}(\\alpha, \\beta),"
  },
  {
    "objectID": "files/lectures/05-1-beta-binomial.html#beta-prior-shapes",
    "href": "files/lectures/05-1-beta-binomial.html#beta-prior-shapes",
    "title": "Beta-Binomial Model",
    "section": "Beta Prior: Shapes",
    "text": "Beta Prior: Shapes\n\nLet’s explore the shape of the Beta:\n\n\n\nplot_beta(1, 5) + theme_bw() + ggtitle(\"Beta(1, 5)\")"
  },
  {
    "objectID": "files/lectures/05-1-beta-binomial.html#beta-prior-shapes-1",
    "href": "files/lectures/05-1-beta-binomial.html#beta-prior-shapes-1",
    "title": "Beta-Binomial Model",
    "section": "Beta Prior: Shapes",
    "text": "Beta Prior: Shapes\n\nLet’s explore the shape of the Beta:\n\n\n\nplot_beta(1, 2) + theme_bw() + ggtitle(\"Beta(1, 2)\")"
  },
  {
    "objectID": "files/lectures/05-1-beta-binomial.html#beta-prior-shapes-2",
    "href": "files/lectures/05-1-beta-binomial.html#beta-prior-shapes-2",
    "title": "Beta-Binomial Model",
    "section": "Beta Prior: Shapes",
    "text": "Beta Prior: Shapes\n\nLet’s explore the shape of the Beta:\n\n\n\nplot_beta(3, 7) + theme_bw() + ggtitle(\"Beta(3, 7)\")"
  },
  {
    "objectID": "files/lectures/05-1-beta-binomial.html#beta-prior-shapes-3",
    "href": "files/lectures/05-1-beta-binomial.html#beta-prior-shapes-3",
    "title": "Beta-Binomial Model",
    "section": "Beta Prior: Shapes",
    "text": "Beta Prior: Shapes\n\nLet’s explore the shape of the Beta:\n\n\n\nplot_beta(1, 1) + theme_bw() + ggtitle(\"Beta(1, 1)\")"
  },
  {
    "objectID": "files/lectures/05-1-beta-binomial.html#beta-prior-shapes-4",
    "href": "files/lectures/05-1-beta-binomial.html#beta-prior-shapes-4",
    "title": "Beta-Binomial Model",
    "section": "Beta Prior: Shapes",
    "text": "Beta Prior: Shapes\n\nYour turn!\nHow would you describe the typical behavior of a:\n\nBeta(\\alpha, \\beta) variable, \\pi, when \\alpha=\\beta?\nBeta(\\alpha, \\beta) variable, \\pi, when \\alpha&gt;\\beta?\nBeta(\\alpha, \\beta) variable, \\pi, when \\alpha&lt;\\beta?\n\nFor which model is there greater variability in the plausible values of \\pi, Beta(20, 20) or Beta(5, 5)?"
  },
  {
    "objectID": "files/lectures/05-1-beta-binomial.html#beta-prior-shapes-5",
    "href": "files/lectures/05-1-beta-binomial.html#beta-prior-shapes-5",
    "title": "Beta-Binomial Model",
    "section": "Beta Prior: Shapes",
    "text": "Beta Prior: Shapes\n\nHow would you describe the typical behavior of a Beta(\\alpha, \\beta) variable, \\pi, when \\alpha=\\beta?"
  },
  {
    "objectID": "files/lectures/05-1-beta-binomial.html#beta-prior-shapes-6",
    "href": "files/lectures/05-1-beta-binomial.html#beta-prior-shapes-6",
    "title": "Beta-Binomial Model",
    "section": "Beta Prior: Shapes",
    "text": "Beta Prior: Shapes\n\nHow would you describe the typical behavior of a Beta(\\alpha, \\beta) variable, \\pi, when \\alpha&gt;\\beta?"
  },
  {
    "objectID": "files/lectures/05-1-beta-binomial.html#beta-prior-shapes-7",
    "href": "files/lectures/05-1-beta-binomial.html#beta-prior-shapes-7",
    "title": "Beta-Binomial Model",
    "section": "Beta Prior: Shapes",
    "text": "Beta Prior: Shapes\n\nHow would you describe the typical behavior of a Beta(\\alpha, \\beta) variable, \\pi, when \\alpha&lt;\\beta?"
  },
  {
    "objectID": "files/lectures/05-1-beta-binomial.html#beta-prior-shapes-8",
    "href": "files/lectures/05-1-beta-binomial.html#beta-prior-shapes-8",
    "title": "Beta-Binomial Model",
    "section": "Beta Prior: Shapes",
    "text": "Beta Prior: Shapes\n\nFor which model is there greater variability in the plausible values of \\pi, Beta(20, 20) or Beta(5, 5)?"
  },
  {
    "objectID": "files/lectures/05-1-beta-binomial.html#tuning-the-beta-prior",
    "href": "files/lectures/05-1-beta-binomial.html#tuning-the-beta-prior",
    "title": "Beta-Binomial Model",
    "section": "Tuning the Beta Prior",
    "text": "Tuning the Beta Prior\n\nWe can tune the shape hyperparameters (\\alpha and \\beta) to reflect our prior information about Michelle’s election support, \\pi.\nIn our example, we saw that she polled between 25 and 65 percentage points, with an average of 45 percentage points.\n\nWe want our Beta(\\alpha, \\beta) to have similar patterns, so we should pick \\alpha and \\beta such that \\pi is around 0.45.\n\n\n\nE[\\pi] = \\frac{\\alpha}{\\alpha+\\beta} \\approx 0.45\n\n\nUsing algebra, we can tune, and find\n\n\\alpha \\approx \\frac{9}{11} \\beta"
  },
  {
    "objectID": "files/lectures/05-1-beta-binomial.html#tuning-the-beta-prior-1",
    "href": "files/lectures/05-1-beta-binomial.html#tuning-the-beta-prior-1",
    "title": "Beta-Binomial Model",
    "section": "Tuning the Beta Prior",
    "text": "Tuning the Beta Prior\n\nYour turn!\n\nGraph the following and determine which is best for the example.\n\n\n\nplot_beta(9, 11) + theme_bw()\nplot_beta(27, 33) + theme_bw()\nplot_beta(45, 55) + theme_bw()\n\n\nRecall, this is what we are going for:"
  },
  {
    "objectID": "files/lectures/05-1-beta-binomial.html#tuning-the-beta-prior-2",
    "href": "files/lectures/05-1-beta-binomial.html#tuning-the-beta-prior-2",
    "title": "Beta-Binomial Model",
    "section": "Tuning the Beta Prior",
    "text": "Tuning the Beta Prior\n\n\nplot_beta(9, 11) + theme_bw() + ggtitle(\"Beta(9, 11)\")"
  },
  {
    "objectID": "files/lectures/05-1-beta-binomial.html#tuning-the-beta-prior-3",
    "href": "files/lectures/05-1-beta-binomial.html#tuning-the-beta-prior-3",
    "title": "Beta-Binomial Model",
    "section": "Tuning the Beta Prior",
    "text": "Tuning the Beta Prior\n\n\nplot_beta(27, 33) + theme_bw() + ggtitle(\"Beta(27, 33)\")"
  },
  {
    "objectID": "files/lectures/05-1-beta-binomial.html#tuning-the-beta-prior-4",
    "href": "files/lectures/05-1-beta-binomial.html#tuning-the-beta-prior-4",
    "title": "Beta-Binomial Model",
    "section": "Tuning the Beta Prior",
    "text": "Tuning the Beta Prior\n\n\nplot_beta(45, 55) + theme_bw() + ggtitle(\"Beta(45, 55)\")"
  },
  {
    "objectID": "files/lectures/05-1-beta-binomial.html#tuning-the-beta-prior-5",
    "href": "files/lectures/05-1-beta-binomial.html#tuning-the-beta-prior-5",
    "title": "Beta-Binomial Model",
    "section": "Tuning the Beta Prior",
    "text": "Tuning the Beta Prior\n\nNow that we have a prior, we “know” some things.\n\n\\pi \\sim \\text{Beta}(45, 55)\n\nFrom the properties of the beta distribution,\n\n\n\\begin{equation*}\n\\begin{aligned}\nE[\\pi] &= \\frac{\\alpha}{\\alpha + \\beta} & \\text{ and } & \\text{ } & \\text{ }  \\\\\n&=\\frac{45}{45+55} \\\\\n&= 0.45\n\\end{aligned}\n\\begin{aligned}\n\\text{var}[\\pi] &= \\frac{\\alpha\\beta}{(\\alpha+\\beta)^2(\\alpha+\\beta+1)} \\\\\n&= \\frac{(45)(55)}{(45+55)^2(45+55+1)} \\\\\n&= 0.0025\n\\end{aligned}\n\\end{equation*}"
  },
  {
    "objectID": "files/lectures/05-1-beta-binomial.html#binomial-data-model",
    "href": "files/lectures/05-1-beta-binomial.html#binomial-data-model",
    "title": "Beta-Binomial Model",
    "section": "Binomial Data Model",
    "text": "Binomial Data Model\n\nA new poll of n = 50 Floridians recorded Y, the number that support Michelle.\n\nThe results depend upon \\pi (as \\pi increases, Y tends to increase).\n\nTo model the dependence of Y on \\pi, we assume\n\nvoters answer the poll independently of one another;\nthe probability that any polled voter supports your candidate Michelle is \\pi\n\nThis is a binomial event, Y|\\pi \\sim \\text{Bin}(50, \\pi), with conditional pmf, f(y|\\pi) defined for y \\in \\{0, 1, ..., 50\\}\n\nf(y|\\pi) = P[Y = y|\\pi] = {50 \\choose y} \\pi^y (1-\\pi)^{50-y}"
  },
  {
    "objectID": "files/lectures/05-1-beta-binomial.html#binomial-data-model-1",
    "href": "files/lectures/05-1-beta-binomial.html#binomial-data-model-1",
    "title": "Beta-Binomial Model",
    "section": "Binomial Data Model",
    "text": "Binomial Data Model\n\nThe conditional pmf, f(y|\\pi), gives us answers to a hypothetical question:\n\nIf Michelle’s support were given some value of \\pi, then how many of the 50 polled voters (Y=y) might we expect to suppport her?\n\nLet’s look at this graphically:\n\n\nbinom_prob &lt;- tibble(n_success = 1:sample_size,\n                     prob = dbinom(n_success, size=sample_size, prob=pi_value))\n\nbinom_prob %&gt;%\n  ggplot(aes(x=n_success,y=prob))+\n  geom_col(width=0.2)+\n  labs(x= \"Number of Successes\",\n       y= \"Probability\") +\n  theme_bw()"
  },
  {
    "objectID": "files/lectures/05-1-beta-binomial.html#binomial-data-model-2",
    "href": "files/lectures/05-1-beta-binomial.html#binomial-data-model-2",
    "title": "Beta-Binomial Model",
    "section": "Binomial Data Model",
    "text": "Binomial Data Model"
  },
  {
    "objectID": "files/lectures/05-1-beta-binomial.html#binomial-data-model-3",
    "href": "files/lectures/05-1-beta-binomial.html#binomial-data-model-3",
    "title": "Beta-Binomial Model",
    "section": "Binomial Data Model",
    "text": "Binomial Data Model\n\nIt is observed that Y=30 of the n=50 polled voters support Michelle.\nWe now want to find the likelihood function – remember that we treat Y=30 as the observed data and \\pi as unknown,\n\n\n\\begin{align*}\nf(y|\\pi) &= {50 \\choose y} \\pi^y (1-\\pi)^{50-y} \\\\\nL(\\pi|y=30) &= {50 \\choose 30} \\pi^{30} (1-\\pi)^{20}\n\\end{align*}\n\n\nThis is valid for \\pi \\in [0, 1]."
  },
  {
    "objectID": "files/lectures/05-1-beta-binomial.html#binomial-data-model-4",
    "href": "files/lectures/05-1-beta-binomial.html#binomial-data-model-4",
    "title": "Beta-Binomial Model",
    "section": "Binomial Data Model",
    "text": "Binomial Data Model\n\nWhat is the likelihood of 30/50 voters supporting Michelle?\n\n\ndbinom(30, 50, pi)\n\n\nYou try this for \\pi = \\{0.25, 0.50, 0.75\\}.\n\n\ndbinom(30, 50, 0.25)\ndbinom(30, 50, 0.5)\ndbinom(30, 50, 0.75)"
  },
  {
    "objectID": "files/lectures/05-1-beta-binomial.html#binomial-data-model-5",
    "href": "files/lectures/05-1-beta-binomial.html#binomial-data-model-5",
    "title": "Beta-Binomial Model",
    "section": "Binomial Data Model",
    "text": "Binomial Data Model\n\nWhat is the likelihood of 30/50 voters supporting Michelle?\n\n\ndbinom(30, 50, 0.25)\n\n[1] 1.29633e-07\n\ndbinom(30, 50, 0.5)\n\n[1] 0.04185915\n\ndbinom(30, 50, 0.75)\n\n[1] 0.007654701"
  },
  {
    "objectID": "files/lectures/05-1-beta-binomial.html#binomial-data-model-6",
    "href": "files/lectures/05-1-beta-binomial.html#binomial-data-model-6",
    "title": "Beta-Binomial Model",
    "section": "Binomial Data Model",
    "text": "Binomial Data Model\n\nChallenge!\nCreate a graph showing what happens to the likelihood for different values of \\pi.\n\ni.e., have \\pi on the x-axis and likelihood on the y-axis.\n\nTo get you started,\n\n\ngraph &lt;- tibble(pi = seq(0, 1, 0.001)) %&gt;%\n  mutate(likelihood = dbinom(30, 50, pi))"
  },
  {
    "objectID": "files/lectures/05-1-beta-binomial.html#binomial-data-model-7",
    "href": "files/lectures/05-1-beta-binomial.html#binomial-data-model-7",
    "title": "Beta-Binomial Model",
    "section": "Binomial Data Model",
    "text": "Binomial Data Model\n\nCreate a graph showing what happens to the likelihood for different values of \\pi.\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhere is the maximum?"
  },
  {
    "objectID": "files/lectures/05-1-beta-binomial.html#binomial-data-model-8",
    "href": "files/lectures/05-1-beta-binomial.html#binomial-data-model-8",
    "title": "Beta-Binomial Model",
    "section": "Binomial Data Model",
    "text": "Binomial Data Model\n\nWhere is the maximum?\n\n\n\n\nError in `geom_text()`:\n! Problem while setting up geom aesthetics.\nℹ Error occurred in the 3rd layer.\nCaused by error in `list_sizes()`:\n! `x$label` must be a vector, not a &lt;latexexpression/expression&gt; object."
  },
  {
    "objectID": "files/lectures/05-1-beta-binomial.html#the-beta-posterior-model",
    "href": "files/lectures/05-1-beta-binomial.html#the-beta-posterior-model",
    "title": "Beta-Binomial Model",
    "section": "The Beta Posterior Model",
    "text": "The Beta Posterior Model\n\nLooking at just the prior and the data distributions,\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe prior is a bit more pessimistic about Michelle’s election support than the data obtained from the latest poll."
  },
  {
    "objectID": "files/lectures/05-1-beta-binomial.html#the-beta-posterior-model-1",
    "href": "files/lectures/05-1-beta-binomial.html#the-beta-posterior-model-1",
    "title": "Beta-Binomial Model",
    "section": "The Beta Posterior Model",
    "text": "The Beta Posterior Model\n\nNow including the posterior,\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe can see that the posterior model of \\pi is continuous and \\in [0, 1].\nThe shape of the posterior appears to also have a Beta(\\alpha, \\beta) model.\n\nThe shape parameters (\\alpha and \\beta) have been updated."
  },
  {
    "objectID": "files/lectures/05-1-beta-binomial.html#the-beta-posterior-model-2",
    "href": "files/lectures/05-1-beta-binomial.html#the-beta-posterior-model-2",
    "title": "Beta-Binomial Model",
    "section": "The Beta Posterior Model",
    "text": "The Beta Posterior Model\n\nIf we were to collect more information about Michelle’s support, we would use the current posterior as the new prior, then update our posterior.\n\nHow do we know what the updated parameters are?\n\n\n\nsummarize_beta_binomial(alpha = 45, beta = 55, y = 30, n = 50)"
  },
  {
    "objectID": "files/lectures/05-1-beta-binomial.html#the-beta-posterior-model-3",
    "href": "files/lectures/05-1-beta-binomial.html#the-beta-posterior-model-3",
    "title": "Beta-Binomial Model",
    "section": "The Beta Posterior Model",
    "text": "The Beta Posterior Model\n\nWe used Michelle’s election support to understand the Beta-Binomial model.\nLet’s now generalize it for any appropriate situation.\n\n\n\\begin{align*}\nY|\\pi &\\sim \\text{Bin}(n, \\pi) \\\\\n\\pi &\\sim \\text{Beta}(\\alpha, \\beta) \\\\\n\\pi | (Y=y) &\\sim \\text{Beta}(\\alpha+y, \\beta+n-y)\n\\end{align*}\n\n\nWe can see that the posterior distribution reveals the influence of the prior (\\alpha and \\beta) and data (y and n)."
  },
  {
    "objectID": "files/lectures/05-1-beta-binomial.html#the-beta-posterior-model-4",
    "href": "files/lectures/05-1-beta-binomial.html#the-beta-posterior-model-4",
    "title": "Beta-Binomial Model",
    "section": "The Beta Posterior Model",
    "text": "The Beta Posterior Model\n\nUnder this updated distribution,\n\n\n\\pi | (Y=y) \\sim \\text{Beta}(\\alpha+y, \\beta+n-y)\n\n\nwe have updated moments:\n\n\n\\begin{align*}\nE[\\pi | Y = y] &= \\frac{\\alpha + y}{\\alpha + \\beta + n} \\\\\n\\text{Var}[\\pi|Y=y] &= \\frac{(\\alpha+y)(\\beta+n-y)}{(\\alpha+\\beta+n)^2(\\alpha+\\beta+1)}\n\\end{align*}"
  },
  {
    "objectID": "files/lectures/05-1-beta-binomial.html#the-beta-posterior-model-5",
    "href": "files/lectures/05-1-beta-binomial.html#the-beta-posterior-model-5",
    "title": "Beta-Binomial Model",
    "section": "The Beta Posterior Model",
    "text": "The Beta Posterior Model\n\nLet’s pause and think about this from a theoretical standpoint.\nThe Beta distribution is a conjugate prior for the likelihood.\n\nConjugate prior: the posterior is from the same model family as the prior.\n\nRecall the Beta prior, f(\\pi),\n\n L(\\pi|y) = {n \\choose y} \\pi^y (1-\\pi)^{n-y} \n\nand the likelihood function, L(\\pi|y).\n\n f(\\pi) = \\frac{\\Gamma(\\alpha+\\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)} \\pi^{\\alpha-1}(1-\\alpha)^{\\beta-1}"
  },
  {
    "objectID": "files/lectures/05-1-beta-binomial.html#the-beta-posterior-model-6",
    "href": "files/lectures/05-1-beta-binomial.html#the-beta-posterior-model-6",
    "title": "Beta-Binomial Model",
    "section": "The Beta Posterior Model",
    "text": "The Beta Posterior Model\n\nWe can put the prior and likelihood together to create the posterior,\n\n\n\\begin{align*}\nf(\\pi|y) &\\propto f(\\pi)L(\\pi|y) \\\\\n&= \\frac{\\Gamma(\\alpha+\\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)} \\pi^{\\alpha-1}(1-\\pi)^{\\beta-1} \\times {n \\choose y} \\pi^y (1-\\pi)^{n-1} \\\\\n&\\propto \\pi^{(\\alpha+y)-1} (1-\\pi)^{(\\beta+n-y)-1}\n\\end{align*}\n\n\nThis is the same structure as the normalized Beta(\\alpha+y, \\beta+n-y),\n\nf(\\pi|y) = \\frac{\\Gamma(\\alpha+\\beta+n)}{\\Gamma(\\alpha+y) \\Gamma(\\beta+n-y)} \\pi^{(\\alpha+y)-1} (1-\\pi)^{(\\beta+n-y)-1}"
  },
  {
    "objectID": "files/lectures/05-1-beta-binomial.html#beta-binomial-example",
    "href": "files/lectures/05-1-beta-binomial.html#beta-binomial-example",
    "title": "Beta-Binomial Model",
    "section": "Beta-Binomial: Example",
    "text": "Beta-Binomial: Example\n\nIn Mario Kart 8 Deluxe, item boxes give different items depending on race position. To reduce the “position bias,” only item boxes opened while the racer was in mid-pack (positions 4–10) were recorded.\nYou want to estimate the probability that an item box yields a Red Shell. When playing the Special Cup, only 31 red shells were seen in 114 boxes opened by mid-pack racers.\nFind the posterior distribution under two priors:\n\nFlat/uninformative prior, Beta(1,1).\nBeta(\\alpha, \\beta) of your choosing."
  },
  {
    "objectID": "files/lectures/05-1-beta-binomial.html#wrap-up-beta-binomial-model",
    "href": "files/lectures/05-1-beta-binomial.html#wrap-up-beta-binomial-model",
    "title": "Beta-Binomial Model",
    "section": "Wrap Up: Beta-Binomial Model",
    "text": "Wrap Up: Beta-Binomial Model\n\nWe have built the Beta-Binomial model for \\pi, an unknown proportion.\n\n\n\\begin{equation*}\n\\begin{aligned}\nY|\\pi &\\sim \\text{Bin}(n,\\pi) \\\\\n\\pi &\\sim \\text{Beta}(\\alpha,\\beta) &\n\\end{aligned}\n\\Rightarrow\n\\begin{aligned}\n&& \\pi | (Y=y) &\\sim \\text{Beta}(\\alpha+y, \\beta+n-y) \\\\\n\\end{aligned}\n\\end{equation*}\n\n\nThe prior model, f(\\pi), is given by Beta(\\alpha,\\beta).\nThe data model, f(Y|\\pi), is given by Bin(n,\\pi).\nThe likelihood function, L(\\pi|y), is obtained by plugging y into the Binomial pmf.\nThe posterior model is a Beta distribution with updated parameters \\alpha+y and \\beta+n-y."
  },
  {
    "objectID": "files/lectures/05-1-beta-binomial.html#homework-practice",
    "href": "files/lectures/05-1-beta-binomial.html#homework-practice",
    "title": "Beta-Binomial Model",
    "section": "Homework / Practice",
    "text": "Homework / Practice\n\nFrom the Bayes Rules! textbook:\n\n3.3\n3.9\n3.10\n3.18"
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#introduction",
    "href": "files/lectures/03-2-probability-distributions.html#introduction",
    "title": "Random Variables and their Distributions",
    "section": "Introduction",
    "text": "Introduction\n\nToday we will review the statistical distributions needed for this course.\nDiscrete distributions:\n\nBinomial\nPoisson\n\nContinuous distributions:\n\nUniform\nNormal\nGamma\nBeta"
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#basic-definitions",
    "href": "files/lectures/03-2-probability-distributions.html#basic-definitions",
    "title": "Random Variables and their Distributions",
    "section": "Basic Definitions",
    "text": "Basic Definitions\n\nDiscrete random variable: a variable that can assume only a finite or countably infinite number of distinct values.\nProbability distribution of a random variable: collection of probabilities for each value of the random variable.\nNotation:\n\nUppercase letter (e.g., Y) denotes a random variable.\nLowercase letter (e.g., y) denotes a particular value that the random variable may assume.\n\nThe specific observed value, y, is not random."
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#probability-distributions-for-discrete-rv",
    "href": "files/lectures/03-2-probability-distributions.html#probability-distributions-for-discrete-rv",
    "title": "Random Variables and their Distributions",
    "section": "Probability Distributions for Discrete RV",
    "text": "Probability Distributions for Discrete RV\n\nProbability function for \\boldsymbol Y: sum of the the probabilities of all sample points in S that are assigned the value y\n\nP[Y = y] = p(y): the probability that Y takes on the value y.\n\nProbability distribution for \\boldsymbol Y: a formula, table, or graph that provides p(y) \\ \\forall \\ y.\nTheorem:\n\nFor any discrete probability distribution, the following must be true:\n\n0 \\le p(y) \\le 1 \\  \\forall \\ y\n\\sum_y p(y) = 1 \\ \\forall \\ p(y) &gt; 0."
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#expected-values-for-discrete-rv",
    "href": "files/lectures/03-2-probability-distributions.html#expected-values-for-discrete-rv",
    "title": "Random Variables and their Distributions",
    "section": "Expected Values for Discrete RV",
    "text": "Expected Values for Discrete RV\n\nExpected value: Let Y be a discrete random variable with probability function p(y). Then the expected value of Y, E[Y], is defined to be\n\n\nE(Y) = \\sum_{y} y p(y)\n\n\nWhen p(y) is an accurate characterization of the population frequency distribution, then the expected value is the population mean.\n\n\nE[Y] = \\mu"
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#expected-values-for-discrete-rv-1",
    "href": "files/lectures/03-2-probability-distributions.html#expected-values-for-discrete-rv-1",
    "title": "Random Variables and their Distributions",
    "section": "Expected Values for Discrete RV",
    "text": "Expected Values for Discrete RV\n\nTheorem:\n\nLet Y be a discrete random variable with probability function p(y) and g(Y) be a real-valued function of Y (i.e., a transformed variable). Then the expected value of g(Y) is given by\n\n\n\nE[g(Y)] = \\sum_{y} g(y) p(y)"
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#expected-values-for-discrete-rv-2",
    "href": "files/lectures/03-2-probability-distributions.html#expected-values-for-discrete-rv-2",
    "title": "Random Variables and their Distributions",
    "section": "Expected Values for Discrete RV",
    "text": "Expected Values for Discrete RV\n\nVariance: if Y is a random variable with mean E[Y] = \\mu, the variance of a random variable Y is defined to be the expected value of (Y-\\mu)^2.\n\n\nV[Y] = E\\left[ (Y-\\mu)^2 \\right]\n\n\nIf p(y) is an accurate characterization of the population frequency distribution, then V(Y) is the population variance,\n\n\nV[Y] = \\sigma^2\n\n\nStandard deviation: the positive square root of V[Y]."
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#expected-values-for-discrete-rv-3",
    "href": "files/lectures/03-2-probability-distributions.html#expected-values-for-discrete-rv-3",
    "title": "Random Variables and their Distributions",
    "section": "Expected Values for Discrete RV",
    "text": "Expected Values for Discrete RV\n\nThere is an alternative (and easier) way to calculate the variance manually,\nTheorem:\n\nLet Y be a discrete random variable with probability function p(y) and mean E[Y] = \\mu. Then,\n\n\nV[Y] = \\sigma^2 = E\\left[(Y-\\mu)^2\\right] = E\\left[Y^2\\right] - \\mu^2"
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#expected-values-for-discrete-rv-4",
    "href": "files/lectures/03-2-probability-distributions.html#expected-values-for-discrete-rv-4",
    "title": "Random Variables and their Distributions",
    "section": "Expected Values for Discrete RV",
    "text": "Expected Values for Discrete RV\n\nThe probability distribution for a random variable Y is given below.\n\n\n\n\ny\np(y)\n\n\n\n\n0\n1/8\n\n\n1\n1/4\n\n\n2\n3/8\n\n\n3\n1/4\n\n\n\n\nFind the mean of Y."
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#expected-values-for-discrete-rv-5",
    "href": "files/lectures/03-2-probability-distributions.html#expected-values-for-discrete-rv-5",
    "title": "Random Variables and their Distributions",
    "section": "Expected Values for Discrete RV",
    "text": "Expected Values for Discrete RV\n\nThe probability distribution for a random variable Y is given below.\n\n\n\n\ny\np(y)\n\n\n\n\n0\n1/8\n\n\n1\n1/4\n\n\n2\n3/8\n\n\n3\n1/4\n\n\n\n\nFind the variance and standard deviation of Y."
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#binomial-probability-distribution",
    "href": "files/lectures/03-2-probability-distributions.html#binomial-probability-distribution",
    "title": "Random Variables and their Distributions",
    "section": "Binomial Probability Distribution",
    "text": "Binomial Probability Distribution\n\nBinomial experiment:\n\nThe experiment consists of a fixed number, n, of identical trials.\nEach trial results in one of two outcomes: success (S) or failure (F).\nThe probability of success on a single trial is equal to some value p and remains the same from trial to trial.\n\nThe probability of failure is equal to q = (1-p).\n\nThe trials are independent.\nThe random variable of interest is Y, the number of successes observed during the n trials."
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#binomial-probability-distribution-1",
    "href": "files/lectures/03-2-probability-distributions.html#binomial-probability-distribution-1",
    "title": "Random Variables and their Distributions",
    "section": "Binomial Probability Distribution",
    "text": "Binomial Probability Distribution\n\nA random variable Y is said to have a binomial distribution based on n trials with success probability p iff\n\n\np(y) = {n \\choose y}p^y q^{n-y}, \\text{ where } y = 0, 1, 2, ..., n, \\text{ and } 0 \\le p \\le1\n\n\nLet Y be a binomial random variable based on n trials and success probability p. Then\n\n\nE[Y] = \\mu = np \\ \\ \\ \\text{and} \\ \\ \\ V[Y] = \\sigma^2 = npq\n\n\nSee Wackerly pg. 107 for derivation."
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#binomial-probability-distribution-2",
    "href": "files/lectures/03-2-probability-distributions.html#binomial-probability-distribution-2",
    "title": "Random Variables and their Distributions",
    "section": "Binomial Probability Distribution",
    "text": "Binomial Probability Distribution\n\nWe can use R to find information related to the binomial distribution.\n\nP[X = x]: dbinom(x, size, prob)\nP[X \\le x]: pbinom(q, size, prob)\nP[X &gt; x]: pbinom(q, size, prob, lower.tail = FALSE)\n\nIn the functions,\n\nx or q is the value of X we are interested in\nsize is the sample size (n)\nprob is the probability of success, \\pi\nlower.tail has two options:\n\nTRUE (default) returns P[X \\le x]\nFALSE returns P[X &gt; x]"
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#binomial-probability-distribution-3",
    "href": "files/lectures/03-2-probability-distributions.html#binomial-probability-distribution-3",
    "title": "Random Variables and their Distributions",
    "section": "Binomial Probability Distribution",
    "text": "Binomial Probability Distribution\n\nThe manufacturer of a dairy drink wishes to compare a new formula (B) with that of the standard formula (A). Each of four judges perform a blinded taste test and report which glass he or she most enjoyed. Suppose that the two formulas are equally attractive.\n\nWhat is the probability distribution? \nWhat is the mean of the distribution? \nWhat is the variance of the distribution?"
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#binomial-probability-distribution-4",
    "href": "files/lectures/03-2-probability-distributions.html#binomial-probability-distribution-4",
    "title": "Random Variables and their Distributions",
    "section": "Binomial Probability Distribution",
    "text": "Binomial Probability Distribution\n\nThe manufacturer of a dairy drink wishes to compare a new formula (B) with that of the standard formula (A). Each of four judges perform a blinded taste test and report which glass he or she most enjoyed. Suppose that the two formulas are equally attractive.\nUse R to find:\n\nP[X = 2]\nP[X &gt; 2]\nP[X &lt; 4]"
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#binomial-probability-distribution-5",
    "href": "files/lectures/03-2-probability-distributions.html#binomial-probability-distribution-5",
    "title": "Random Variables and their Distributions",
    "section": "Binomial Probability Distribution",
    "text": "Binomial Probability Distribution\n\nThe manufacturer of a dairy drink wishes to compare a new formula (B) with that of the standard formula (A). Each of four judges perform a blinded taste test and report which glass he or she most enjoyed. Suppose that the two formulas are equally attractive.\nUse R to find:\n\nP[X = 2]\n\n\n\ndbinom(x = 2, size = 4, prob = 0.5)\n\n[1] 0.375"
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#binomial-probability-distribution-6",
    "href": "files/lectures/03-2-probability-distributions.html#binomial-probability-distribution-6",
    "title": "Random Variables and their Distributions",
    "section": "Binomial Probability Distribution",
    "text": "Binomial Probability Distribution\n\nThe manufacturer of a dairy drink wishes to compare a new formula (B) with that of the standard formula (A). Each of four judges perform a blinded taste test and report which glass he or she most enjoyed. Suppose that the two formulas are equally attractive.\nUse R to find:\n\nP[X &gt; 2]\n\n\n\npbinom(q = 2, size = 4, prob = 0.5, lower.tail = FALSE)\n\n[1] 0.3125"
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#binomial-probability-distribution-7",
    "href": "files/lectures/03-2-probability-distributions.html#binomial-probability-distribution-7",
    "title": "Random Variables and their Distributions",
    "section": "Binomial Probability Distribution",
    "text": "Binomial Probability Distribution\n\nThe manufacturer of a dairy drink wishes to compare a new formula (B) with that of the standard formula (A). Each of four judges perform a blinded taste test and report which glass he or she most enjoyed. Suppose that the two formulas are equally attractive.\nUse R to find:\n\nP[X &lt; 4] = P[X \\le 3]\n\n\n\npbinom(q = 3, size = 4, prob = 0.5)\n\n[1] 0.9375"
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#poisson-probability-distribution",
    "href": "files/lectures/03-2-probability-distributions.html#poisson-probability-distribution",
    "title": "Random Variables and their Distributions",
    "section": "Poisson Probability Distribution",
    "text": "Poisson Probability Distribution\n\nWe often use the Poisson distribution to model count data.\nA random variable Y is said to have a Poisson probability distribution iff\n\n\np(y) = \\frac{\\lambda^y}{y!}e^{-\\lambda}, \\text{ where } y=0,1,2,..., \\text{ and } \\lambda &gt; 0\n\n\nIf Y is a random variable with a Poisson distribution with parameter \\lambda, then\n\n\nE[Y] = \\mu = \\lambda \\text{ and } V[Y] = \\sigma^2 = \\lambda\n\n\nSee Wackerly pg. 134 for derivation."
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#poisson-probability-distribution-1",
    "href": "files/lectures/03-2-probability-distributions.html#poisson-probability-distribution-1",
    "title": "Random Variables and their Distributions",
    "section": "Poisson Probability Distribution",
    "text": "Poisson Probability Distribution\n\nWe can use R to find information related to the Poisson distribution.\n\nP[X = x]: dpois(x, lambda)\n\nP[X \\le x]: ppois(q, lambda)\n\nP[X &gt; x]: ppois(q, lambda, lower.tail = FALSE)\n\nIn the functions:\n\nx or q is the value of X we are interested in\n\nlambda is the rate of occurrence\nlower.tail has two options:\n\nTRUE (default) returns P[X \\le x]\n\nFALSE returns P[X &gt; x]"
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#poisson-probability-distribution-2",
    "href": "files/lectures/03-2-probability-distributions.html#poisson-probability-distribution-2",
    "title": "Random Variables and their Distributions",
    "section": "Poisson Probability Distribution",
    "text": "Poisson Probability Distribution\n\nCustomers arrive at a checkout counter in a department store according to a Poisson distribution at an average of seven per hour.\n\nWhat is the probability distribution? \nWhat is the mean of the distribution? \nWhat is the variance of the distribution?"
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#poisson-probability-distribution-3",
    "href": "files/lectures/03-2-probability-distributions.html#poisson-probability-distribution-3",
    "title": "Random Variables and their Distributions",
    "section": "Poisson Probability Distribution",
    "text": "Poisson Probability Distribution\n\nCustomers arrive at a checkout counter in a department store according to a Poisson distribution at an average of seven per hour. Use R to find the following probabilities.\n\nNo more than three customers arrive.\nAt least two customers arrive.\nExactly five customers arrive."
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#poisson-probability-distribution-4",
    "href": "files/lectures/03-2-probability-distributions.html#poisson-probability-distribution-4",
    "title": "Random Variables and their Distributions",
    "section": "Poisson Probability Distribution",
    "text": "Poisson Probability Distribution\n\nCustomers arrive at a checkout counter in a department store according to a Poisson distribution at an average of seven per hour. Use R to find the following probabilities.\n\nNo more than three customers arrive.\n\n\n\nppois(q = 3, lambda = 7)\n\n[1] 0.08176542"
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#poisson-probability-distribution-5",
    "href": "files/lectures/03-2-probability-distributions.html#poisson-probability-distribution-5",
    "title": "Random Variables and their Distributions",
    "section": "Poisson Probability Distribution",
    "text": "Poisson Probability Distribution\n\nCustomers arrive at a checkout counter in a department store according to a Poisson distribution at an average of seven per hour. Use R to find the following probabilities.\n\nAt least two customers arrive.\n\n\n\nppois(q = 1, lambda = 7, lower.tail = FALSE)\n\n[1] 0.9927049"
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#poisson-probability-distribution-6",
    "href": "files/lectures/03-2-probability-distributions.html#poisson-probability-distribution-6",
    "title": "Random Variables and their Distributions",
    "section": "Poisson Probability Distribution",
    "text": "Poisson Probability Distribution\n\nCustomers arrive at a checkout counter in a department store according to a Poisson distribution at an average of seven per hour. Use R to find the following probabilities.\n\nExactly five customers arrive.\n\n\n\ndpois(x = 5, lambda = 7)\n\n[1] 0.1277167"
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#probability-distributions-for-continuous-rv",
    "href": "files/lectures/03-2-probability-distributions.html#probability-distributions-for-continuous-rv",
    "title": "Random Variables and their Distributions",
    "section": "Probability Distributions for Continuous RV",
    "text": "Probability Distributions for Continuous RV\n\nThe distribution function of Y (any random varaible), denoted by F(y), is such that\n\nF(y) = P[Y \\le y] \\text{ for } -\\infty &lt; y &lt; \\infty\n\nTheorem: Properties of a Distribution Function\n\nIf F(y) is a distribution function, then\n\nF(-\\infty)   \\equiv \\underset{y \\to -\\infty}{\\lim} F(y) = 0\nF(\\infty)    \\equiv \\underset{y \\to \\infty}{\\lim} F(y) = 1\nF(y) is a nondecreasing function of y.\n\nIf y_1 and y_2 are any values such that y_1 &lt; y_2, then F(y_1) \\le F(y_2)."
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#probability-distributions-for-continuous-rv-1",
    "href": "files/lectures/03-2-probability-distributions.html#probability-distributions-for-continuous-rv-1",
    "title": "Random Variables and their Distributions",
    "section": "Probability Distributions for Continuous RV",
    "text": "Probability Distributions for Continuous RV\n\nContinuous random variable:\n\nA random variable Y with distribution function F(y) is said to be continuous if F(y) is continuous for -\\infty &lt; y &lt; \\infty.\n\nIf Y is a continuous random variable, then for any real number y, P[Y = y] = 0.\n\ni.e., we must remember to find the probability of an interval."
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#probability-distributions-for-continuous-rv-2",
    "href": "files/lectures/03-2-probability-distributions.html#probability-distributions-for-continuous-rv-2",
    "title": "Random Variables and their Distributions",
    "section": "Probability Distributions for Continuous RV",
    "text": "Probability Distributions for Continuous RV\n\nProbability density function:\n\nLet F(y) be the cumulative density function for a continuous random variable, Y. Then\n\n\np[Y = y] = f(y) = \\frac{dF(y)}{dy} = F'(y).\n\nTheorem: Properties of a Density Function\n\nIf f(y) is a density function for a continuous random variable, then\n\nf(y) \\ge 0 \\ \\forall y, \\ -\\infty &lt; y &lt; \\infty.\n\\int_{-\\infty}^{\\infty} f(y) dy = 1."
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#probability-distributions-for-continuous-rv-3",
    "href": "files/lectures/03-2-probability-distributions.html#probability-distributions-for-continuous-rv-3",
    "title": "Random Variables and their Distributions",
    "section": "Probability Distributions for Continuous RV",
    "text": "Probability Distributions for Continuous RV\n\nCumulative density function:\n\nLet f(y) be the probability density function for a continuous random variable, Y. Then\n\n\nP[Y \\le y] = F(y) = \\int_{-\\infty}^y f(t) dt, \\text{ for } y \\in {\\rm I\\!R}."
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#probability-distributions-for-continuous-rv-4",
    "href": "files/lectures/03-2-probability-distributions.html#probability-distributions-for-continuous-rv-4",
    "title": "Random Variables and their Distributions",
    "section": "Probability Distributions for Continuous RV",
    "text": "Probability Distributions for Continuous RV\n\nTheorem\n\nIf the random variable Y has density function f(y) and a &lt; b, then the probability that Y falls in the interval [a, b] is\n\n\n\nP[a \\le Y \\le b] = \\int_a^b f(y) dy."
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#expected-values-for-continuous-rv",
    "href": "files/lectures/03-2-probability-distributions.html#expected-values-for-continuous-rv",
    "title": "Random Variables and their Distributions",
    "section": "Expected Values for Continuous RV",
    "text": "Expected Values for Continuous RV\n\nExpected value:\n\nThe expected value of a continuous variable Y is\n\n\n\nE[Y] = \\int_{-\\infty}^{\\infty} y f(y) \\ dy\n\n\nThis is the continuous version of the expected value for a discrete random variable,\n\n\nE[Y] = \\sum_y y p(y)"
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#expected-values-for-continuous-rv-1",
    "href": "files/lectures/03-2-probability-distributions.html#expected-values-for-continuous-rv-1",
    "title": "Random Variables and their Distributions",
    "section": "Expected Values for Continuous RV",
    "text": "Expected Values for Continuous RV\n\nTheorem:\n\nLet g(Y) be a function of Y; then the expected value of g(Y) is given by\n\n\n\nE\\left[ g(Y) \\right] = \\int_{-\\infty}^{\\infty} g(y) f(y) \\ dy\n\n\nTheorem:\n\nLet c be a constant and let g(Y), g_1(Y), g_2(Y), …, g_k(Y) be functions of a continuous random variable, Y. Then the following results hold:\n\nE[c] = c\nE\\left[cg(Y)] = cE[g(Y)\\right]\nE\\left[g_1(Y)+...+g_k(Y)\\right] = E\\left[ g_1(Y) \\right] + ... + E\\left[ g_k(Y) \\right]"
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#uniform-probability-distribution",
    "href": "files/lectures/03-2-probability-distributions.html#uniform-probability-distribution",
    "title": "Random Variables and their Distributions",
    "section": "Uniform Probability Distribution",
    "text": "Uniform Probability Distribution\n\nUniform Distribution"
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#uniform-probability-distribution-1",
    "href": "files/lectures/03-2-probability-distributions.html#uniform-probability-distribution-1",
    "title": "Random Variables and their Distributions",
    "section": "Uniform Probability Distribution",
    "text": "Uniform Probability Distribution\n\nA random variable Y is said to have a uniform distribution iff\n\n\nf(y) = \\frac{1}{\\theta_2 - \\theta_1}, \\ \\theta_1 \\le y \\le \\theta_2\n\n\nIf \\theta_1 &lt; \\theta_2 and Y is a uniformly distributed r.v. on the interval (\\theta_1, \\theta_2), then\n\n\nE[Y] = \\mu = \\frac{\\theta_1+\\theta_2}{2} \\ \\ \\ \\text{and} \\ \\ \\ V[Y] = \\sigma^2 = \\frac{(\\theta_2-\\theta_1)^2}{12}\n\n\nSee Wackerly pg. 176 for derivation."
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#uniform-probability-distribution-2",
    "href": "files/lectures/03-2-probability-distributions.html#uniform-probability-distribution-2",
    "title": "Random Variables and their Distributions",
    "section": "Uniform Probability Distribution",
    "text": "Uniform Probability Distribution\n\nAn industrial psychologist has determined that it takes a worker between 9 and 15 minutes to complete a task on an automobile assembly line. If the time to complete the task is uniformly distributed over the interval 9 \\le y \\le 15, then determine:\n\nThe probability distribution. \nThe mean of the distribution. \nThe variance and standard deviation of the distribution."
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#uniform-probability-distribution-3",
    "href": "files/lectures/03-2-probability-distributions.html#uniform-probability-distribution-3",
    "title": "Random Variables and their Distributions",
    "section": "Uniform Probability Distribution",
    "text": "Uniform Probability Distribution\n\nWe can use R to find information related to the uniform distribution:\n\nP[X \\le x]: punif(q, min, max)\n\nP[X \\ge x]: punif(q, min, max, lower.tail = FALSE)\n\nIn the functions:\n\nq is the value of X we are interested in\n\nmin is the lower bound of the distribution\n\nmax is the upper bound of the distribution\n\nlower.tail has two options:\n\nTRUE (default) returns P[X \\le x]\n\nFALSE returns P[X \\ge x]"
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#uniform-probability-distribution-4",
    "href": "files/lectures/03-2-probability-distributions.html#uniform-probability-distribution-4",
    "title": "Random Variables and their Distributions",
    "section": "Uniform Probability Distribution",
    "text": "Uniform Probability Distribution\n\nAn industrial psychologist has determined that it takes a worker between 9 and 15 minutes to complete a task on an automobile assembly line. If the time to complete the task is uniformly distributed over the interval 9 \\le y \\le 15, then determine the following probabilities:\n\nA worker takes fewer than 13 minutes.\nA worker takes at least 11 minutes.\nA worker takes between 14 and 15 minutes."
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#uniform-probability-distribution-5",
    "href": "files/lectures/03-2-probability-distributions.html#uniform-probability-distribution-5",
    "title": "Random Variables and their Distributions",
    "section": "Uniform Probability Distribution",
    "text": "Uniform Probability Distribution\n\nAn industrial psychologist has determined that it takes a worker between 9 and 15 minutes to complete a task on an automobile assembly line. If the time to complete the task is uniformly distributed over the interval 9 \\le y \\le 15, then determine the following probabilities:\n\nA worker takes fewer than 13 minutes.\n\n\n\npunif(13, 9, 15)\n\n[1] 0.6666667"
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#uniform-probability-distribution-6",
    "href": "files/lectures/03-2-probability-distributions.html#uniform-probability-distribution-6",
    "title": "Random Variables and their Distributions",
    "section": "Uniform Probability Distribution",
    "text": "Uniform Probability Distribution\n\nAn industrial psychologist has determined that it takes a worker between 9 and 15 minutes to complete a task on an automobile assembly line. If the time to complete the task is uniformly distributed over the interval 9 \\le y \\le 15, then determine the following probabilities:\n\nA worker takes at least 11 minutes.\n\n\n\npunif(11, 9, 15, lower.tail = TRUE)\n\n[1] 0.3333333"
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#uniform-probability-distribution-7",
    "href": "files/lectures/03-2-probability-distributions.html#uniform-probability-distribution-7",
    "title": "Random Variables and their Distributions",
    "section": "Uniform Probability Distribution",
    "text": "Uniform Probability Distribution\n\nAn industrial psychologist has determined that it takes a worker between 9 and 15 minutes to complete a task on an automobile assembly line. If the time to complete the task is uniformly distributed over the interval 9 \\le y \\le 15, then determine the following probabilities:\n\nA worker takes between 14 and 15 minutes.\n\n\n\npunif(15, 9, 15) - punif(14, 9, 15)\n\n[1] 0.1666667"
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#normal-probability-distribution",
    "href": "files/lectures/03-2-probability-distributions.html#normal-probability-distribution",
    "title": "Random Variables and their Distributions",
    "section": "Normal Probability Distribution",
    "text": "Normal Probability Distribution\n\nNormal Distribution"
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#normal-probability-distribution-1",
    "href": "files/lectures/03-2-probability-distributions.html#normal-probability-distribution-1",
    "title": "Random Variables and their Distributions",
    "section": "Normal Probability Distribution",
    "text": "Normal Probability Distribution\n\nA random variable Y is said to have a normal distribution iff, for \\sigma &gt; 0 and -\\infty &lt; \\mu &lt; \\infty,\n\n\nf(y) = \\frac{1}{\\sigma \\sqrt{2\\pi}} e^{-(y-\\mu)^2/(2\\sigma^2)}\n\n\nIf Y is a random variable normally distributed with parameters \\mu and \\sigma, then\n\n\nE[Y] = \\mu =  \\ \\ \\ \\text{and} \\ \\ \\ V[Y] = \\sigma^2"
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#normal-probability-distribution-2",
    "href": "files/lectures/03-2-probability-distributions.html#normal-probability-distribution-2",
    "title": "Random Variables and their Distributions",
    "section": "Normal Probability Distribution",
    "text": "Normal Probability Distribution\n\nWe can use R to find information related to the normal distribution.\n\nP[X \\le x]: pnorm(q, mean, sd)\n\nP[X \\ge x]: pnorm(q, mean, sd, lower.tail = FALSE)\n\nIn the functions:\n\nq is the value of X we are interested in\n\nmean is the population mean \\mu\n\nsd is the standard deviation \\sigma\n\nlower.tail has two options:\n\nTRUE (default) returns P[X \\le x]\n\nFALSE returns P[X \\ge x]"
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#normal-probability-distribution-3",
    "href": "files/lectures/03-2-probability-distributions.html#normal-probability-distribution-3",
    "title": "Random Variables and their Distributions",
    "section": "Normal Probability Distribution",
    "text": "Normal Probability Distribution\n\nA random variable Y is said to have a standard normal distribution iff\n\n\nY \\sim N(\\mu=0,\\sigma=1)\n\n\nThe normal distribution is then simplified to\n\n\nf(y) = \\frac{1}{\\sqrt{2\\pi}} e^{-y^2/2}\n\n\nNote that in all cases of the normal distribution, we assume -\\infty &lt; y &lt; \\infty."
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#normal-probability-distribution-4",
    "href": "files/lectures/03-2-probability-distributions.html#normal-probability-distribution-4",
    "title": "Random Variables and their Distributions",
    "section": "Normal Probability Distribution",
    "text": "Normal Probability Distribution\n\nWhen using pnorm(), the default values for mean and sd are 1 and 0.\nThus, if we have the standard normal our R functions simplify to:\n\nP[Z \\le z]: pnorm(z)\n\nP[Z \\ge z]: pnorm(z, lower.tail = FALSE)\n\nIn the functions:\n\nq is the z-score value of interest\n\nlower.tail = TRUE returns P[Z \\le z]\n\nlower.tail = FALSE returns P[Z \\ge z]"
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#normal-probability-distribution-5",
    "href": "files/lectures/03-2-probability-distributions.html#normal-probability-distribution-5",
    "title": "Random Variables and their Distributions",
    "section": "Normal Probability Distribution",
    "text": "Normal Probability Distribution\n\nA geneticist working for a seed company develops a new carrot for growing in heavy clay soil. After measuring 5000 of these carrots, it can be said that the carrot length, Y, is normally distributed with \\mu = 11.5 cm and \\sigma = 1.15 cm. Determine\n\nThe probability distribution. \nThe mean of the distribution. \nThe variance and standard deviation of the distribution."
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#normal-probability-distribution-6",
    "href": "files/lectures/03-2-probability-distributions.html#normal-probability-distribution-6",
    "title": "Random Variables and their Distributions",
    "section": "Normal Probability Distribution",
    "text": "Normal Probability Distribution\n\nA geneticist working for a seed company develops a new carrot for growing in heavy clay soil. After measuring 5000 of these carrots, it can be said that the carrot length, Y, is normally distributed with \\mu = 11.5 cm and \\sigma = 1.15 cm.\n\nWhat is the probability that a carrot will be between 10 and 13 cm?\nWhat is the probability that a carrot will be less than 9 cm?\nWhat is the probability that a carrot will be 12 cm or larger?"
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#normal-probability-distribution-7",
    "href": "files/lectures/03-2-probability-distributions.html#normal-probability-distribution-7",
    "title": "Random Variables and their Distributions",
    "section": "Normal Probability Distribution",
    "text": "Normal Probability Distribution\n\nA geneticist working for a seed company develops a new carrot for growing in heavy clay soil. After measuring 5000 of these carrots, it can be said that the carrot length, Y, is normally distributed with \\mu = 11.5 cm and \\sigma = 1.15 cm.\n\nWhat is the probability that a carrot will be between 10 and 13 cm?\n\n\n\npnorm(q = 13, mean = 11.5, sd = 1.15) - pnorm(q = 10, mean = 11.5, sd = 1.15)\n\n[1] 0.807885"
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#normal-probability-distribution-8",
    "href": "files/lectures/03-2-probability-distributions.html#normal-probability-distribution-8",
    "title": "Random Variables and their Distributions",
    "section": "Normal Probability Distribution",
    "text": "Normal Probability Distribution\n\nA geneticist working for a seed company develops a new carrot for growing in heavy clay soil. After measuring 5000 of these carrots, it can be said that the carrot length, Y, is normally distributed with \\mu = 11.5 cm and \\sigma = 1.15 cm.\n\nWhat is the probability that a carrot will be less than 9 cm?\n\n\n\npnorm(q = 9, mean = 11.5, sd = 1.15)\n\n[1] 0.01485583"
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#normal-probability-distribution-9",
    "href": "files/lectures/03-2-probability-distributions.html#normal-probability-distribution-9",
    "title": "Random Variables and their Distributions",
    "section": "Normal Probability Distribution",
    "text": "Normal Probability Distribution\n\nA geneticist working for a seed company develops a new carrot for growing in heavy clay soil. After measuring 5000 of these carrots, it can be said that the carrot length, Y, is normally distributed with \\mu = 11.5 cm and \\sigma = 1.15 cm.\n\nWhat is the probability that a carrot will be 12 cm or larger?\n\n\n\npnorm(q = 12, mean = 11.5, sd = 1.15, lower.tail = FALSE)\n\n[1] 0.3318601"
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#gamma-probability-distribution",
    "href": "files/lectures/03-2-probability-distributions.html#gamma-probability-distribution",
    "title": "Random Variables and their Distributions",
    "section": "Gamma Probability Distribution",
    "text": "Gamma Probability Distribution\n\nGamma Distribution"
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#gamma-probability-distribution-1",
    "href": "files/lectures/03-2-probability-distributions.html#gamma-probability-distribution-1",
    "title": "Random Variables and their Distributions",
    "section": "Gamma Probability Distribution",
    "text": "Gamma Probability Distribution\n\nA random variable Y is said to have a gamma distribution with parameters \\alpha &gt; 0 and \\beta &gt; 0 iff,\n\n\nf(y) = \\frac{y^{\\alpha-1} e^{-y/\\beta}}{\\beta^{\\alpha} \\Gamma(\\alpha)}, \\ 0 \\le y &lt; \\infty\n\n\nNote that \\Gamma(\\alpha) = \\int_{0}^{\\infty} y^{\\alpha-1} e^{-y} \\ dy.\nIf Y has a gamma distribution with parameters \\alpha and \\beta, then\n\n\nE[Y] = \\mu = \\alpha\\beta \\ \\ \\ \\text{and} \\ \\ \\ V[Y] = \\sigma^2  = \\alpha\\beta^2\n\n\nSee Wackerly pg. 187 for derivation."
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#gamma-probability-distribution-2",
    "href": "files/lectures/03-2-probability-distributions.html#gamma-probability-distribution-2",
    "title": "Random Variables and their Distributions",
    "section": "Gamma Probability Distribution",
    "text": "Gamma Probability Distribution\n\nWe can use R to find information related to the Gamma distribution.\n\nP[X \\le x]: pgamma(q, shape, rate)\n\nP[X \\ge x]: pgamma(q, shape, rate, lower.tail = FALSE)\n\nIn the functions:\n\nq is the value of X we are interested in\n\nshape is the shape parameter, \\alpha\n\nscale is the scale parameter, \\beta\n\nAlternatively, can parameterize with rate = 1/\\beta, rate = 1 / scale\n\nlower.tail has two options:\n\nTRUE (default) returns P[X \\le x]\n\nFALSE returns P[X \\ge x]"
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#gamma-probability-distribution-3",
    "href": "files/lectures/03-2-probability-distributions.html#gamma-probability-distribution-3",
    "title": "Random Variables and their Distributions",
    "section": "Gamma Probability Distribution",
    "text": "Gamma Probability Distribution\n\nAnnual incomes for heads of household in an affluent section of a city have approximately a gamma distribution with \\alpha=32 and \\beta=2500. Determine:\n\nThe probability distribution. \nThe mean of the distribution. \nThe variance and standard deviation of the distribution."
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#gamma-probability-distribution-4",
    "href": "files/lectures/03-2-probability-distributions.html#gamma-probability-distribution-4",
    "title": "Random Variables and their Distributions",
    "section": "Gamma Probability Distribution",
    "text": "Gamma Probability Distribution\n\nAnnual incomes for heads of household in an affluent section of a city have approximately a gamma distribution with \\alpha=32 and \\beta=2500.\n\nWhat proportion have incomes in excess of $100,000?\nWhat proportion have incomes between $75,000 and $150,000?"
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#gamma-probability-distribution-5",
    "href": "files/lectures/03-2-probability-distributions.html#gamma-probability-distribution-5",
    "title": "Random Variables and their Distributions",
    "section": "Gamma Probability Distribution",
    "text": "Gamma Probability Distribution\n\nAnnual incomes for heads of household in a section of a city have approximately a gamma distribution with \\alpha=32 and \\beta=2500.\n\nWhat proportion have incomes in excess of $30,000?\n\n\n\npgamma(q = 100000, shape = 32, scale = 2500, lower.tail = FALSE)\n\n[1] 0.08552057"
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#gamma-probability-distribution-6",
    "href": "files/lectures/03-2-probability-distributions.html#gamma-probability-distribution-6",
    "title": "Random Variables and their Distributions",
    "section": "Gamma Probability Distribution",
    "text": "Gamma Probability Distribution\n\nAnnual incomes for heads of household in an affluent section of a city have approximately a gamma distribution with \\alpha=32 and \\beta=2500.\n\nWhat proportion have incomes between $75,000 and $150,000?\n\n\n\npgamma(q = 150000, shape = 32, scale = 2500) - pgamma(q = 75000, shape = 32, scale = 2500)\n\n[1] 0.6186147"
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#beta-probability-distribution",
    "href": "files/lectures/03-2-probability-distributions.html#beta-probability-distribution",
    "title": "Random Variables and their Distributions",
    "section": "Beta Probability Distribution",
    "text": "Beta Probability Distribution\n\nBeta Distribution"
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#beta-probability-distribution-1",
    "href": "files/lectures/03-2-probability-distributions.html#beta-probability-distribution-1",
    "title": "Random Variables and their Distributions",
    "section": "Beta Probability Distribution",
    "text": "Beta Probability Distribution\n\nA random variable Y is said to have a beta distribution with parameters \\alpha &gt; 0 and \\beta &gt; 0 iff,\n\n\nf(y) = \\frac{y^{\\alpha-1}(1-y)^{\\beta-1}}{B(\\alpha,\\beta)}, \\ 0 \\le y \\le 1\n\n\nNote: B(\\alpha,\\beta) = \\int_0^1 y^{\\alpha-1}(1-y)^{\\beta-1} \\ dy = \\frac{\\Gamma(\\alpha) \\Gamma(\\beta)}{\\Gamma(\\alpha+\\beta)}.\nIf Y has a beta distribution with parameters \\alpha &gt; 0 and \\beta &gt; 0, then\n\n\nE[Y] = \\mu = \\frac{\\alpha}{\\alpha+\\beta} \\ \\ \\ \\text{and} \\ \\ \\ V[Y] = \\sigma^2  = \\frac{\\alpha\\beta}{(\\alpha+\\beta)^2(\\alpha+\\beta+1)}"
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#beta-probability-distribution-2",
    "href": "files/lectures/03-2-probability-distributions.html#beta-probability-distribution-2",
    "title": "Random Variables and their Distributions",
    "section": "Beta Probability Distribution",
    "text": "Beta Probability Distribution\n\nWe can use R to find information related to the Beta distribution.\n\nP[X \\le x]: pbeta(q, shape1, shape2)\n\nP[X \\ge x]: pbeta(q, shape1, shape2, lower.tail = FALSE)\n\nIn the functions:\n\nq is the value of X we are interested in – must be in [0, 1]!\nshape1 is the first shape parameter, \\alpha\n\nshape2 is the second shape parameter, \\beta\n\nlower.tail has two options:\n\nTRUE (default) returns P[X \\le x]\n\nFALSE returns P[X \\ge x]"
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#beta-probability-distribution-3",
    "href": "files/lectures/03-2-probability-distributions.html#beta-probability-distribution-3",
    "title": "Random Variables and their Distributions",
    "section": "Beta Probability Distribution",
    "text": "Beta Probability Distribution\n\nIn a survey of cupcake preferences, 8 respondents liked the new cupcake flavor and 2 did not. We will model the proportion of all respondents who would like the cupcake flavor using a Beta distribution with \\alpha = 8 and \\beta = 2. Determine:\n\nThe probability distribution. \nThe mean of the distribution. \nThe variance and standard deviation of the distribution."
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#beta-probability-distribution-4",
    "href": "files/lectures/03-2-probability-distributions.html#beta-probability-distribution-4",
    "title": "Random Variables and their Distributions",
    "section": "Beta Probability Distribution",
    "text": "Beta Probability Distribution\n\nIn a survey of cupcake preferences, 8 respondents liked the new cupcake flavor and 2 did not. We will model the proportion of all respondents who would like the cupcake flavor using a Beta distribution with \\alpha = 8 and \\beta = 2. What is the probability that:\n\nfewer than 60% of respondents like the new flavor?\nmore than 90% of respondents like the new flavor?\nsomewhere between 70% and 90% of respondents like the new flavor?"
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#beta-probability-distribution-5",
    "href": "files/lectures/03-2-probability-distributions.html#beta-probability-distribution-5",
    "title": "Random Variables and their Distributions",
    "section": "Beta Probability Distribution",
    "text": "Beta Probability Distribution\n\nIn a survey of cupcake preferences, 8 respondents liked the new cupcake flavor and 2 did not. We will model the proportion of all respondents who would like the cupcake flavor using a Beta distribution with \\alpha = 8 and \\beta = 2. What is the probability that:\n\nfewer than 60% of respondents like the new flavor?\n\n\n\npbeta(q = 0.6, shape1 = 8, shape2 = 2)\n\n[1] 0.07054387"
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#beta-probability-distribution-6",
    "href": "files/lectures/03-2-probability-distributions.html#beta-probability-distribution-6",
    "title": "Random Variables and their Distributions",
    "section": "Beta Probability Distribution",
    "text": "Beta Probability Distribution\n\nIn a survey of cupcake preferences, 8 respondents liked the new cupcake flavor and 2 did not. We will model the proportion of all respondents who would like the cupcake flavor using a Beta distribution with \\alpha = 8 and \\beta = 2. What is the probability that:\n\nmore than 90% of respondents like the new flavor?\n\n\n\npbeta(q = 0.9, shape1 = 8, shape2 = 2, lower.tail = FALSE)\n\n[1] 0.225159"
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#beta-probability-distribution-7",
    "href": "files/lectures/03-2-probability-distributions.html#beta-probability-distribution-7",
    "title": "Random Variables and their Distributions",
    "section": "Beta Probability Distribution",
    "text": "Beta Probability Distribution\n\nIn a survey of cupcake preferences, 8 respondents liked the new cupcake flavor and 2 did not. We will model the proportion of all respondents who would like the cupcake flavor using a Beta distribution with \\alpha = 8 and \\beta = 2. What is the probability that:\n\nsomewhere between 70% and 90% of respondents like the new flavor?\n\n\n\npbeta(q = 0.9, shape1 = 8, shape2 = 2) - pbeta(q = 0.7, shape1 = 8, shape2 = 2)\n\n[1] 0.5788377"
  },
  {
    "objectID": "files/lectures/03-2-probability-distributions.html#wrap-up",
    "href": "files/lectures/03-2-probability-distributions.html#wrap-up",
    "title": "Random Variables and their Distributions",
    "section": "Wrap Up",
    "text": "Wrap Up\n\nYou now are ready to work on Assignment 1: Probability Distributions.\n\nDue Tuesday, September 16.\n.qmd file is available to download on Canvas.\n\nGoals:\n\nIdentify the applicable distribution.\nFind probabilities using said distributions.\n\nNext week:\n\nMonday: Thinking like a Bayesian\nWednesday: Beta-Binomial"
  },
  {
    "objectID": "files/lectures/07-2-bayesian-inference.html#introduction",
    "href": "files/lectures/07-2-bayesian-inference.html#introduction",
    "title": "Posterior Analysis",
    "section": "Introduction",
    "text": "Introduction\n\nNow that we know how to find posterior distributions, we will discuss how to use them to make inference.\nHere are the packages we need today:\n\n\nlibrary(bayesrules)\nlibrary(tidyverse)\nlibrary(rstan)\nlibrary(bayesplot)\nlibrary(broom.mixed)\nlibrary(janitor)\ndata(\"moma_sample\")"
  },
  {
    "objectID": "files/lectures/07-2-bayesian-inference.html#working-example",
    "href": "files/lectures/07-2-bayesian-inference.html#working-example",
    "title": "Posterior Analysis",
    "section": "Working Example",
    "text": "Working Example\n\nImagine you find yourself standing at the Museum of Modern Art (MoMA) in New York City, captivated by the artwork in front of you.\n\nWhat are the chances that this modern artist is Gen X or even younger, i.e., born in 1965 or later?\n\nLet \\pi denote the proportion of artists represented in major U.S. modern art museums that are Gen X or younger.\n\nLet’s let the Beta(4,6) prior model for \\pi reflect our prior assumption that major modern art museums disproportionately display artists born before 1965, i.e., \\pi most likely falls below 0.5.\nRationale: “modern art” dates back to the 1880s and it can take a while to gain recognition as an artist."
  },
  {
    "objectID": "files/lectures/07-2-bayesian-inference.html#working-example-1",
    "href": "files/lectures/07-2-bayesian-inference.html#working-example-1",
    "title": "Posterior Analysis",
    "section": "Working Example",
    "text": "Working Example\n\nThe Beta(4,6) prior model for \\pi:"
  },
  {
    "objectID": "files/lectures/07-2-bayesian-inference.html#working-example-2",
    "href": "files/lectures/07-2-bayesian-inference.html#working-example-2",
    "title": "Posterior Analysis",
    "section": "Working Example",
    "text": "Working Example\n\nLet’s consider the following dataset:\n\n\nhead(moma_sample)"
  },
  {
    "objectID": "files/lectures/07-2-bayesian-inference.html#working-example-3",
    "href": "files/lectures/07-2-bayesian-inference.html#working-example-3",
    "title": "Posterior Analysis",
    "section": "Working Example",
    "text": "Working Example\n\nCounting the number of Generation X and younger,\n\n\nmoma_sample %&gt;% \n  group_by(genx) %&gt;% \n  tally()"
  },
  {
    "objectID": "files/lectures/07-2-bayesian-inference.html#working-example-4",
    "href": "files/lectures/07-2-bayesian-inference.html#working-example-4",
    "title": "Posterior Analysis",
    "section": "Working Example",
    "text": "Working Example\n\nOur modeling is as follows,\n\n\n\\begin{align*}\nY|\\pi &\\sim \\text{Bin}(100,\\pi) \\\\\n\\pi &\\sim \\text{Beta}(4,6) \\\\\n\\pi | (Y = 14) &\\sim \\text{Beta(18,92)}\n\\end{align*}"
  },
  {
    "objectID": "files/lectures/07-2-bayesian-inference.html#working-example-5",
    "href": "files/lectures/07-2-bayesian-inference.html#working-example-5",
    "title": "Posterior Analysis",
    "section": "Working Example",
    "text": "Working Example"
  },
  {
    "objectID": "files/lectures/07-2-bayesian-inference.html#working-example-6",
    "href": "files/lectures/07-2-bayesian-inference.html#working-example-6",
    "title": "Posterior Analysis",
    "section": "Working Example",
    "text": "Working Example\n\nThere are three common tasks in posterior analysis:\n\nestimation,\nhypothesis testing, and\nprediction.\n\nFor example,\n\nWhat’s our estimate of \\pi?\nDoes our model support the claim that fewer than 20% of museum artists are Gen X or younger?\nIf we sample 20 more museum artists, how many do we predict will be Gen X or younger?"
  },
  {
    "objectID": "files/lectures/07-2-bayesian-inference.html#posterior-estimation",
    "href": "files/lectures/07-2-bayesian-inference.html#posterior-estimation",
    "title": "Posterior Analysis",
    "section": "Posterior Estimation",
    "text": "Posterior Estimation\n\nWe can construct posterior credible intervals.\n\nA posterior credible interval (CI) provides a range of posterior plausible values of \\theta, and thus a summary of both posterior central tendency and variability.\nA middle 95% CI is constructed by the 2.5th and 97.5th posterior percentiles, \\left( \\theta_{0.025}, \\theta_{0.975} \\right), and there is a 95% posterior probability that \\theta is in this range,\n\n\nP\\left[ \\theta \\in (\\theta_{0.025}, \\theta_{0.975})|Y=y \\right] = \\int_{\\theta_{0.025}}^{\\theta_{0.975}} f(\\theta|y) d\\theta = 0.95"
  },
  {
    "objectID": "files/lectures/07-2-bayesian-inference.html#posterior-estimation-1",
    "href": "files/lectures/07-2-bayesian-inference.html#posterior-estimation-1",
    "title": "Posterior Analysis",
    "section": "Posterior Estimation",
    "text": "Posterior Estimation\n\nRecall the Beta(18, 92) posterior model for \\pi, the proportion of modern art museum artists that are Gen X or younger.\n\n\nqbeta(c(0.025, 0.975), 18, 92) # 95% CI\n\n[1] 0.1009084 0.2379286\n\n\n\nThere is a 95% posterior probability that somewhere between 10% and 24% of museum artists are Gen X or younger."
  },
  {
    "objectID": "files/lectures/07-2-bayesian-inference.html#posterior-estimation-2",
    "href": "files/lectures/07-2-bayesian-inference.html#posterior-estimation-2",
    "title": "Posterior Analysis",
    "section": "Posterior Estimation",
    "text": "Posterior Estimation\n\nRecall the Beta(18, 92) posterior model for \\pi, the proportion of modern art museum artists that are Gen X or younger.\n\n\nqbeta(c(0.25, 0.75), 18, 92) # 50% CI\n\n[1] 0.1388414 0.1862197\n\n\n\nThere is a 50% posterior probability that somewhere between 14% and 19% of museum artists are Gen X or younger."
  },
  {
    "objectID": "files/lectures/07-2-bayesian-inference.html#posterior-estimation-3",
    "href": "files/lectures/07-2-bayesian-inference.html#posterior-estimation-3",
    "title": "Posterior Analysis",
    "section": "Posterior Estimation",
    "text": "Posterior Estimation\n\n95% is a common choice, however, note that it is somewhat arbitrary and used because of decades of tradition.\n\n\n\n\n\nThere is no one right credible interval.\n\nIt will just depend on the context of the analysis."
  },
  {
    "objectID": "files/lectures/07-2-bayesian-inference.html#posterior-hypothesis-testing",
    "href": "files/lectures/07-2-bayesian-inference.html#posterior-hypothesis-testing",
    "title": "Posterior Analysis",
    "section": "Posterior Hypothesis Testing",
    "text": "Posterior Hypothesis Testing\n\nSuppose we read an article claiming that fewer than 20% of museum artists are Gen X or younger.\n\n\n\n\n\nHow plausible is it that \\pi &lt; 0.2?\n\n\nP\\left[ \\pi &lt; 0.2 | Y = 14 \\right] = \\int_0^{0.2} f(\\pi|y = 14) d\\pi"
  },
  {
    "objectID": "files/lectures/07-2-bayesian-inference.html#posterior-hypothesis-testing-1",
    "href": "files/lectures/07-2-bayesian-inference.html#posterior-hypothesis-testing-1",
    "title": "Posterior Analysis",
    "section": "Posterior Hypothesis Testing",
    "text": "Posterior Hypothesis Testing\n\nPosterior Probability\n\n\nP\\left[ \\pi &lt; 0.2 | Y = 14 \\right] = \\int_0^{0.2} f(\\pi|y = 14) d\\pi\n\n\nWe can find this probability by using the pbeta() function:\n\n\npbeta(0.20, 18, 92)"
  },
  {
    "objectID": "files/lectures/07-2-bayesian-inference.html#posterior-hypothesis-testing-2",
    "href": "files/lectures/07-2-bayesian-inference.html#posterior-hypothesis-testing-2",
    "title": "Posterior Analysis",
    "section": "Posterior Hypothesis Testing",
    "text": "Posterior Hypothesis Testing\n\nWe can find this probability by using the pbeta() function:\n\n\npbeta(0.20, 18, 92)\n\n[1] 0.8489856\n\n\n\nThus,\n\n\nP\\left[ \\pi &lt; 0.2 | Y = 14 \\right] = 0.849\n\n\nThere is approximately an 84.9% posterior chance that Gen Xers account for fewer than 20% of modern art museum artists."
  },
  {
    "objectID": "files/lectures/07-2-bayesian-inference.html#posterior-hypothesis-testing-3",
    "href": "files/lectures/07-2-bayesian-inference.html#posterior-hypothesis-testing-3",
    "title": "Posterior Analysis",
    "section": "Posterior Hypothesis Testing",
    "text": "Posterior Hypothesis Testing\n\nPrior Probability\n\n\nP\\left[ \\pi &lt; 0.2 \\right] = \\int_0^{0.2} f(\\pi) d\\pi\n\n\nWe can find this probability by using the pbeta() function:\n\n\npbeta(0.20, 4, 6)"
  },
  {
    "objectID": "files/lectures/07-2-bayesian-inference.html#posterior-hypothesis-testing-4",
    "href": "files/lectures/07-2-bayesian-inference.html#posterior-hypothesis-testing-4",
    "title": "Posterior Analysis",
    "section": "Posterior Hypothesis Testing",
    "text": "Posterior Hypothesis Testing\n\nWe can find this probability by using the pbeta() function:\n\n\npbeta(0.20, 4, 6)\n\n[1] 0.08564173\n\n\n\nThus,\n\n\nP\\left[ \\pi &lt; 0.2 \\right] = 0.086\n\n\nThere is approximately an 8.6% prior chance that Gen Xers account for fewer than 20% of modern art museum artists."
  },
  {
    "objectID": "files/lectures/07-2-bayesian-inference.html#posterior-hypothesis-testing-5",
    "href": "files/lectures/07-2-bayesian-inference.html#posterior-hypothesis-testing-5",
    "title": "Posterior Analysis",
    "section": "Posterior Hypothesis Testing",
    "text": "Posterior Hypothesis Testing\n\nWe can create a table of information we will use to make inferences:\n\n\n\n\n\n\n\n\nHypotheses\n\n\n\n\n\n\nPrior Probability\n\n\n\n\n\n\nPosterior Probability\n\n\n\n\n\n\nH_0: \\pi \\ge 0.2\n\n\nP[H_0] = 0.914\n\n\nP[H_0 | Y= 14] = 0.151\n\n\n\n\nH_1: \\pi &lt; 0.2\n\n\nP[H_1] = 0.086\n\n\nP[H_1 | Y = 14] = 0.849"
  },
  {
    "objectID": "files/lectures/07-2-bayesian-inference.html#posterior-hypothesis-testing-6",
    "href": "files/lectures/07-2-bayesian-inference.html#posterior-hypothesis-testing-6",
    "title": "Posterior Analysis",
    "section": "Posterior Hypothesis Testing",
    "text": "Posterior Hypothesis Testing\n\nLet’s find the posterior odds in favor of H_1:\n\n\n\n\n\n\n\n\nHypotheses\n\n\n\n\n\n\nPrior Probability\n\n\n\n\n\n\nPosterior Probability\n\n\n\n\n\n\nH_0: \\pi \\ge 0.2\n\n\nP[H_0] = 0.914\n\n\nP[H_0 | Y= 14] = 0.151\n\n\n\n\nH_1: \\pi &lt; 0.2\n\n\nP[H_1] = 0.086\n\n\nP[H_1 | Y = 14] = 0.849\n\n\n\n\n\n\n\\begin{align*}\n\\text{posterior odds} &= \\frac{P\\left[ H_1 | Y = 14 \\right]}{P\\left[ H_0 | Y = 14 \\right]} \\\\\n&= \\frac{0.849}{0.151} \\\\\n&\\approx 5.62\n\\end{align*}\n\n\nOur posterior assessment, after considering data collected, suggests that \\pi is 5.62 times more likely to be below 0.2 rather than being above 0.2."
  },
  {
    "objectID": "files/lectures/07-2-bayesian-inference.html#posterior-hypothesis-testing-7",
    "href": "files/lectures/07-2-bayesian-inference.html#posterior-hypothesis-testing-7",
    "title": "Posterior Analysis",
    "section": "Posterior Hypothesis Testing",
    "text": "Posterior Hypothesis Testing\n\nLet’s find the prior odds in favor of H_1:\n\n\n\n\n\n\n\n\nHypotheses\n\n\n\n\n\n\nPrior Probability\n\n\n\n\n\n\nPosterior Probability\n\n\n\n\n\n\nH_0: \\pi \\ge 0.2\n\n\nP[H_0] = 0.914\n\n\nP[H_0 | Y= 14] = 0.151\n\n\n\n\nH_1: \\pi &lt; 0.2\n\n\nP[H_1] = 0.086\n\n\nP[H_1 | Y = 14] = 0.849\n\n\n\n\n\n\n\\begin{align*}\n\\text{prior odds} &= \\frac{P\\left[ H_1 \\right]}{P\\left[ H_0 \\right]} \\\\\n&= \\frac{0.086}{0.914} \\\\\n&\\approx 0.093\n\\end{align*}\n\n\nOur prior assessment, before considering the data collected, suggests that \\pi is 0.093 times more likely to be below 0.2 rather than being above 0.2."
  },
  {
    "objectID": "files/lectures/07-2-bayesian-inference.html#posterior-hypothesis-testing-8",
    "href": "files/lectures/07-2-bayesian-inference.html#posterior-hypothesis-testing-8",
    "title": "Posterior Analysis",
    "section": "Posterior Hypothesis Testing",
    "text": "Posterior Hypothesis Testing\n\nBayes Factor\n\nWhen we are comparing two competing hypotheses, H_0 vs. H_1, the Bayes Factor is an odds ratio for H_1.\ni.e., the Bayes Factor as a measure of evidence in favor of H_1.\n\n\n\n\\text{Bayes Factor} = \\frac{\\text{posterior odds}}{\\text{prior odds}} = \\frac{P\\left[H_1 | Y\\right] / P\\left[H_0 | Y\\right]}{P\\left[H_1\\right] / P\\left[H_0\\right]}\n\n\nThis will be our alternative to the p-value."
  },
  {
    "objectID": "files/lectures/07-2-bayesian-inference.html#posterior-hypothesis-testing-9",
    "href": "files/lectures/07-2-bayesian-inference.html#posterior-hypothesis-testing-9",
    "title": "Posterior Analysis",
    "section": "Posterior Hypothesis Testing",
    "text": "Posterior Hypothesis Testing\n\nBayes Factor\n\n\n\\text{Bayes Factor} = \\frac{\\text{posterior odds}}{\\text{prior odds}} = \\frac{P\\left[H_1 | Y\\right] / P\\left[H_0 | Y\\right]}{P\\left[H_1\\right] / P\\left[H_0\\right]}\n\n\nWe will compare the BF to 1.\n\nBF = 1: The plausibility of H_1 did not change in light of the observed data.\nBF &gt; 1: The plausibility of H_1 increased in light of the observed data.\n\nThe greater the Bayes Factor, the more convincing the evidence for H_1.\n\nBF &lt; 1: The plausibilty of H_1 decreased in light of the observed data."
  },
  {
    "objectID": "files/lectures/07-2-bayesian-inference.html#posterior-hypothesis-testing-10",
    "href": "files/lectures/07-2-bayesian-inference.html#posterior-hypothesis-testing-10",
    "title": "Posterior Analysis",
    "section": "Posterior Hypothesis Testing",
    "text": "Posterior Hypothesis Testing\n\nLet’s now find the Bayes Factor:\n\n\n\n\n\n\n\n\nHypotheses\n\n\n\n\n\n\nPrior Probability\n\n\n\n\n\n\nPosterior Probability\n\n\n\n\n\n\nH_0: \\pi \\ge 0.2\n\n\nP[H_0] = 0.914\n\n\nP[H_0 | Y= 14] = 0.151\n\n\n\n\nH_1: \\pi &lt; 0.2\n\n\nP[H_1] = 0.086\n\n\nP[H_1 | Y = 14] = 0.849\n\n\n\n\n\n\n\\begin{align*}\n\\text{Bayes Factor} &= \\frac{\\text{posterior odds}}{\\text{prior odds}} = \\frac{P\\left[H_1 | Y\\right] / P\\left[H_0 | Y\\right]}{P\\left[H_1\\right] / P\\left[H_0\\right]} \\\\\n&= \\frac{5.62}{0.09} \\\\\n&\\approx 62.44\n\\end{align*}"
  },
  {
    "objectID": "files/lectures/07-2-bayesian-inference.html#posterior-hypothesis-testing-11",
    "href": "files/lectures/07-2-bayesian-inference.html#posterior-hypothesis-testing-11",
    "title": "Posterior Analysis",
    "section": "Posterior Hypothesis Testing",
    "text": "Posterior Hypothesis Testing\n\nLet’s now find the Bayes Factor:\n\n\n\n\n\n\n\n\nHypotheses\n\n\n\n\n\n\nPrior Probability\n\n\n\n\n\n\nPosterior Probability\n\n\n\n\n\n\nH_0: \\pi \\ge 0.2\n\n\nP[H_0] = 0.914\n\n\nP[H_0 | Y= 14] = 0.151\n\n\n\n\nH_1: \\pi &lt; 0.2\n\n\nP[H_1] = 0.086\n\n\nP[H_1 | Y = 14] = 0.849\n\n\n\n\n\n\n\nprior_odds &lt;- pbeta(0.20, 4, 6)/(1-pbeta(0.20, 4, 6))\npost_odds &lt;- pbeta(0.20, 18, 92)/(1-pbeta(0.20, 18, 92))\n(BF &lt;- post_odds/prior_odds)\n\n[1] 60.02232"
  },
  {
    "objectID": "files/lectures/07-2-bayesian-inference.html#posterior-hypothesis-testing-12",
    "href": "files/lectures/07-2-bayesian-inference.html#posterior-hypothesis-testing-12",
    "title": "Posterior Analysis",
    "section": "Posterior Hypothesis Testing",
    "text": "Posterior Hypothesis Testing\n\nIt’s time to draw a conclusion!\n\nPosterior probability: 0.85\nBayes factor: 60\n\nWe have fairly convincing evidence in factor of the claim that fewer than 20% of artists at major modern art museums are Gen X or younger.\nThis gives us more information - we have a holistic measure of the level of uncertainty about the claim.\n\nThis should help us inform our next steps."
  },
  {
    "objectID": "files/lectures/07-2-bayesian-inference.html#posterior-hypothesis-testing-13",
    "href": "files/lectures/07-2-bayesian-inference.html#posterior-hypothesis-testing-13",
    "title": "Posterior Analysis",
    "section": "Posterior Hypothesis Testing",
    "text": "Posterior Hypothesis Testing\n\nWe now want to test whether or not 30% of major museum artists are Gen X or younger.\n\n\n\\begin{align*}\nH_0&: \\ \\pi = 0.3 \\\\\nH_1&: \\ \\pi \\ne 0.3\n\\end{align*}\n\n\nWhy is this an issue?"
  },
  {
    "objectID": "files/lectures/07-2-bayesian-inference.html#posterior-hypothesis-testing-14",
    "href": "files/lectures/07-2-bayesian-inference.html#posterior-hypothesis-testing-14",
    "title": "Posterior Analysis",
    "section": "Posterior Hypothesis Testing",
    "text": "Posterior Hypothesis Testing\n\nWe now want to test whether or not 30% of major museum artists are Gen X or younger.\n\n\n\\begin{align*}\nH_0&: \\ \\pi = 0.3 \\\\\nH_1&: \\ \\pi \\ne 0.3\n\\end{align*}\n\n\nWhy is this an issue? The posterior probability of a point hypothesis is always 0!\n\n\nP\\left[ \\pi =0.3 | Y = 14 \\right] = \\int_{0.3}^{0.3} f(\\pi|y = 14) d\\pi = 0"
  },
  {
    "objectID": "files/lectures/07-2-bayesian-inference.html#posterior-hypothesis-testing-15",
    "href": "files/lectures/07-2-bayesian-inference.html#posterior-hypothesis-testing-15",
    "title": "Posterior Analysis",
    "section": "Posterior Hypothesis Testing",
    "text": "Posterior Hypothesis Testing\n\nThe posterior probability of a point hypothesis is always 0!\n\n\nP\\left[ \\pi =0.3 | Y = 14 \\right] = \\int_{0.3}^{0.3} f(\\pi|y = 14) d\\pi = 0\n\n\n…. meaning:\n\n\n\\text{posterior odds}  = \\frac{P\\left[ H_1 | Y = 14 \\right]}{P\\left[ H_0 | Y = 14 \\right]} = \\frac{1}{0}"
  },
  {
    "objectID": "files/lectures/07-2-bayesian-inference.html#posterior-hypothesis-testing-16",
    "href": "files/lectures/07-2-bayesian-inference.html#posterior-hypothesis-testing-16",
    "title": "Posterior Analysis",
    "section": "Posterior Hypothesis Testing",
    "text": "Posterior Hypothesis Testing\n\nWelp.\nLet’s think about the 95% posterior credible interval for \\pi: (0.10, 0.24).\n\nDo we think that 0.3 is a plausible value?"
  },
  {
    "objectID": "files/lectures/07-2-bayesian-inference.html#posterior-hypothesis-testing-17",
    "href": "files/lectures/07-2-bayesian-inference.html#posterior-hypothesis-testing-17",
    "title": "Posterior Analysis",
    "section": "Posterior Hypothesis Testing",
    "text": "Posterior Hypothesis Testing\n\nLet’s reframe our hypotheses:\n\n\n\\begin{align*}\nH_0&: \\ \\pi \\in (0.25, 0.35) \\\\\nH_1&: \\ \\pi \\not\\in (0.25, 0.35)\n\\end{align*}\n\n\nNow, we can more rigorously claim belief in H_1.\n\nThe entire hypothesized range is above the 95% CI.\n\nThis also allows us a way to construct our hypothesis test with posterior probability and Bayes Factor."
  },
  {
    "objectID": "files/lectures/07-2-bayesian-inference.html#posterior-prediction",
    "href": "files/lectures/07-2-bayesian-inference.html#posterior-prediction",
    "title": "Posterior Analysis",
    "section": "Posterior Prediction",
    "text": "Posterior Prediction\n\nIn addition to estimating the posterior and using the posterior distribution for hypothesis testing, we may be interested in predicting the outcome in a new dataset.\nSuppose we get our hands on data for 20 more artworks displayed at the museum.\nBased on the posterior understanding of \\pi that we’ve developed throughout this chapter, what number would you predict are done by artists that are Gen X or younger?"
  },
  {
    "objectID": "files/lectures/07-2-bayesian-inference.html#posterior-prediction-1",
    "href": "files/lectures/07-2-bayesian-inference.html#posterior-prediction-1",
    "title": "Posterior Analysis",
    "section": "Posterior Prediction",
    "text": "Posterior Prediction\n\nSuppose we get our hands on data for 20 more artworks displayed at the museum.\nBased on the posterior understanding of \\pi that we’ve developed throughout this chapter, what number would you predict are done by artists that are Gen X or younger?\n\nLogical response:\n\nposterior guess was 16%\nn = 20\nn \\hat{\\pi} = 20(0.16) = 3.2"
  },
  {
    "objectID": "files/lectures/07-2-bayesian-inference.html#posterior-prediction-2",
    "href": "files/lectures/07-2-bayesian-inference.html#posterior-prediction-2",
    "title": "Posterior Analysis",
    "section": "Posterior Prediction",
    "text": "Posterior Prediction\n\nSuppose we get our hands on data for 20 more artworks displayed at the museum. What number would you predict are done by artists that are Gen X or younger?\n\nTwo sources of variability in prediction:\n\nSampling variability: we do not expect n \\hat{\\pi} to be an integer.\nPosterior variability in \\pi: we know that 0.16 is not the only plausible \\pi.\n\n95% credible interval for \\pi: (0.10, 0.24)\nWhat do we expect to see under each possible \\pi?"
  },
  {
    "objectID": "files/lectures/07-2-bayesian-inference.html#posterior-prediction-3",
    "href": "files/lectures/07-2-bayesian-inference.html#posterior-prediction-3",
    "title": "Posterior Analysis",
    "section": "Posterior Prediction",
    "text": "Posterior Prediction\n\nLet’s look at combining the sampling variability in our new Y and posterior variability in \\pi.\n\nLet Y' = y' be the (unknown) number of the 20 new artwork that are done by Gen X or younger artists.\n\ny' \\in \\{0, 1, ..., 20\\}.\n\nConditioned on \\pi, the sampling variability in Y' can be modeled\n\n\n\nY'|\\pi \\sim \\text{Bin}(20, \\pi)\n\n\nf(y'|\\pi) = P[Y' = y'|\\pi] = {20\\choose{y'}} \\pi^{y'} (1-\\pi)^{20-y'}"
  },
  {
    "objectID": "files/lectures/07-2-bayesian-inference.html#posterior-prediction-4",
    "href": "files/lectures/07-2-bayesian-inference.html#posterior-prediction-4",
    "title": "Posterior Analysis",
    "section": "Posterior Prediction",
    "text": "Posterior Prediction\n\nWe can weight f(y'|\\pi) by the posterior pdf, f(\\pi|y=14).\n\nThis captures the chance of observing Y' = y' Gen Xers for a given \\pi\nAt the same time, this accounts for the posterior plausibility of \\pi.\n\n\nf(y'|\\pi) f(\\pi|y=14)"
  },
  {
    "objectID": "files/lectures/07-2-bayesian-inference.html#posterior-prediction-5",
    "href": "files/lectures/07-2-bayesian-inference.html#posterior-prediction-5",
    "title": "Posterior Analysis",
    "section": "Posterior Prediction",
    "text": "Posterior Prediction\n\nThis leads us to the posterior predictive model of Y'.\n\n\nf(y'|y) = \\int f(y'|\\pi) f(\\pi|y) \\ d \\pi\n\n\nThe overall chance of observing Y' = y' weights the chance of observing this outcome under any possible \\pi by the posterior plausibility of \\pi.\n\nChance of observing this outcome under any \\pi: f(y'|\\pi).\nPosterior plausibility of \\pi: f(\\pi|y)."
  },
  {
    "objectID": "files/lectures/07-2-bayesian-inference.html#posterior-prediction-6",
    "href": "files/lectures/07-2-bayesian-inference.html#posterior-prediction-6",
    "title": "Posterior Analysis",
    "section": "Posterior Prediction",
    "text": "Posterior Prediction\n\n# STEP 1: DEFINE the model\nart_model &lt;- \"\n  data {\n    int&lt;lower = 0, upper = 100&gt; Y;\n  }\n  parameters {\n    real&lt;lower = 0, upper = 1&gt; pi;\n  }\n  model {\n    Y ~ binomial(100, pi);\n    pi ~ beta(4, 6);\n  }\n\""
  },
  {
    "objectID": "files/lectures/07-2-bayesian-inference.html#posterior-prediction-7",
    "href": "files/lectures/07-2-bayesian-inference.html#posterior-prediction-7",
    "title": "Posterior Analysis",
    "section": "Posterior Prediction",
    "text": "Posterior Prediction\n\n# STEP 2: SIMULATE the posterior\nart_sim &lt;- stan(model_code = art_model, \n                data = list(Y = 14),  \n                chains = 4, \n                iter = 5000*2, \n                seed = 84735)\n\n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 3e-06 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.03 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 1: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 1: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 1: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 1: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 1: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 1: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 1: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 1: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 1: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 1: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 1: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.013 seconds (Warm-up)\nChain 1:                0.013 seconds (Sampling)\nChain 1:                0.026 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 1e-06 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.01 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 2: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 2: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 2: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 2: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 2: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 2: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 2: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 2: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 2: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 2: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 2: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.013 seconds (Warm-up)\nChain 2:                0.014 seconds (Sampling)\nChain 2:                0.027 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 0 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 3: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 3: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 3: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 3: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 3: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 3: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 3: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 3: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 3: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 3: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 3: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 0.013 seconds (Warm-up)\nChain 3:                0.016 seconds (Sampling)\nChain 3:                0.029 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 1e-06 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.01 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 4: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 4: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 4: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 4: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 4: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 4: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 4: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 4: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 4: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 4: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 4: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 0.014 seconds (Warm-up)\nChain 4:                0.013 seconds (Sampling)\nChain 4:                0.027 seconds (Total)\nChain 4:"
  },
  {
    "objectID": "files/lectures/07-2-bayesian-inference.html#posterior-prediction-8",
    "href": "files/lectures/07-2-bayesian-inference.html#posterior-prediction-8",
    "title": "Posterior Analysis",
    "section": "Posterior Prediction",
    "text": "Posterior Prediction\n\n\n# Parallel trace plot\nmcmc_trace(art_sim, pars = \"pi\", size = 0.5) + xlab(\"iteration\")"
  },
  {
    "objectID": "files/lectures/07-2-bayesian-inference.html#posterior-prediction-9",
    "href": "files/lectures/07-2-bayesian-inference.html#posterior-prediction-9",
    "title": "Posterior Analysis",
    "section": "Posterior Prediction",
    "text": "Posterior Prediction\n\n\n# Density plot\nmcmc_dens_overlay(art_sim, pars = \"pi\")"
  },
  {
    "objectID": "files/lectures/07-2-bayesian-inference.html#posterior-prediction-10",
    "href": "files/lectures/07-2-bayesian-inference.html#posterior-prediction-10",
    "title": "Posterior Analysis",
    "section": "Posterior Prediction",
    "text": "Posterior Prediction\n\n\n# Autocorrelation plot\nmcmc_acf(art_sim, pars = \"pi\")"
  },
  {
    "objectID": "files/lectures/07-2-bayesian-inference.html#posterior-prediction-11",
    "href": "files/lectures/07-2-bayesian-inference.html#posterior-prediction-11",
    "title": "Posterior Analysis",
    "section": "Posterior Prediction",
    "text": "Posterior Prediction\n\nAs we saw previously, the posterior was Beta(18, 92).\nWe will use the tidy() function from the broom.mixed package.\n\n\nlibrary(broom.mixed)\ntidy(art_sim, conf.int = TRUE, conf.level = 0.95)\n\n\n  \n\n\n\n\nThe approximate middle 95% CI for \\pi is (0.100, 0.239).\nOur approximation of the actual posterior median is 0.162."
  },
  {
    "objectID": "files/lectures/07-2-bayesian-inference.html#posterior-prediction-12",
    "href": "files/lectures/07-2-bayesian-inference.html#posterior-prediction-12",
    "title": "Posterior Analysis",
    "section": "Posterior Prediction",
    "text": "Posterior Prediction\n\nWe can use the mcmc_areas() function from the bayesrules package to get a corresponding graph,\n\n\n\nmcmc_areas(art_sim, pars = \"pi\", prob = 0.95)"
  },
  {
    "objectID": "files/lectures/07-2-bayesian-inference.html#posterior-simulation",
    "href": "files/lectures/07-2-bayesian-inference.html#posterior-simulation",
    "title": "Posterior Analysis",
    "section": "Posterior Simulation",
    "text": "Posterior Simulation\n\nUnfortunately, tidy() does not give everything we may be interested in.\n\nWe can save the Markov chain values to a dataset and analyze.\n\n\n\n# Store the 4 chains in 1 data frame\nart_chains_df &lt;- as.data.frame(art_sim, pars = \"lp__\", include = FALSE)\ndim(art_chains_df)\n\n[1] 20000     1\n\nhead(art_chains_df, n=3)"
  },
  {
    "objectID": "files/lectures/07-2-bayesian-inference.html#posterior-simulation-1",
    "href": "files/lectures/07-2-bayesian-inference.html#posterior-simulation-1",
    "title": "Posterior Analysis",
    "section": "Posterior Simulation",
    "text": "Posterior Simulation\n\nWe can then summarize the Markov chain values,\n\n\nart_chains_df %&gt;% \n  summarize(post_mean = mean(pi), \n            post_median = median(pi),\n            post_mode = sample_mode(pi),\n            lower_95 = quantile(pi, 0.025),\n            upper_95 = quantile(pi, 0.975))\n\n\n  \n\n\n\n\nWe have reproduced/verified the results from tidy() (and then some!)"
  },
  {
    "objectID": "files/lectures/07-2-bayesian-inference.html#posterior-simulation-2",
    "href": "files/lectures/07-2-bayesian-inference.html#posterior-simulation-2",
    "title": "Posterior Analysis",
    "section": "Posterior Simulation",
    "text": "Posterior Simulation\n\nNow that we have saved the Markov chain values, we can use them to answer questions about the data.\nRecall, we were interested in testing the claim that fewer than 20% of major museum artists are Gen X.\n\n\nart_chains_df %&gt;% \n  mutate(exceeds = pi &lt; 0.20) %&gt;% \n  tabyl(exceeds)\n\n\n  \n\n\n\n\nBy the posterior, there is an 84.6% chance that Gen X artist representation is under 20%."
  },
  {
    "objectID": "files/lectures/07-2-bayesian-inference.html#posterior-simulation-3",
    "href": "files/lectures/07-2-bayesian-inference.html#posterior-simulation-3",
    "title": "Posterior Analysis",
    "section": "Posterior Simulation",
    "text": "Posterior Simulation\n\nLet us compare the results between using conjugate family knowledge and MCMC.\n\n\n\n\n\nFrom this, we can see that MCMC gave us an accurate approximation.\nWe should use this as “proof” that the approximations are “reliable” for non-conjugate families.\n\nAlways look at diagnostics!"
  },
  {
    "objectID": "files/lectures/07-2-bayesian-inference.html#homework",
    "href": "files/lectures/07-2-bayesian-inference.html#homework",
    "title": "Posterior Analysis",
    "section": "Homework",
    "text": "Homework\n\n8.4\n8.8\n8.9\n8.10\n8.14\n8.15\n8.16"
  },
  {
    "objectID": "files/lectures/05-2-example.html",
    "href": "files/lectures/05-2-example.html",
    "title": "Mario Kart Analysis",
    "section": "",
    "text": "In Mario Kart 8 Deluxe, item boxes give different items depending on race position. To reduce the “position bias,” only item boxes opened while the racer was in mid-pack (positions 4–10) were recorded.\nYou want to estimate the probability that an item box yields a Red Shell. When playing the Special Cup, only 31 red shells were seen in 114 boxes opened by mid-pack racers.\nFind the posterior distribution under two priors:\n\nFlat/uninformative prior, Beta(1,1).\nBeta(\\alpha, \\beta) of your choosing.\n\n\nFind the posterior under the flat prior, Beta(1,1).\n\nplot_beta_binomial(alpha = 1, beta = 1,\n                   y = 31, n = 114) +\n  theme_bw()\n\n\n\n\n\n\n\n\nFind the posterior under a prior of your choosing.\n\nplot_beta_binomial(alpha = 8, beta = 4,\n                   y = 31, n = 114) +\n  theme_bw()\n\n\n\n\n\n\n\nplot_beta_binomial(alpha = 23, beta = 57,\n                   y = 31, n = 114) +\n  theme_bw()\n\n\n\n\n\n\n\n\n\nSuppose that we know the individual data points for the Special Cup:\n\nCloudtop Cruise, of 28 boxes, 6 had a red shell.\nBone-Dry Dunes, of 27 boxes, 8 had a red shell.\nBowser’s Castle, of 29 boxes, 12 had a red shell.\nRainbow Road, of 30 boxes, 5 had a red shell.\n\nProve sequentiality to yourself.\n\nAnalyze the data race-by-race in this order: Cloudtop Cruise, Bone-Dry Dunes, Bowser’s Castle, and Rainbow Road.\nAnalyze the data race-by-race in this order: Rainbow Road, Bowser’s Castle, Cloudtop Cruise, and Bone-Dry Dunes.\n\n\nAnalyze the data race-by-race in this order: Cloudtop Cruise, Bone-Dry Dunes, Bowser’s Castle, and Rainbow Road.\n\nsummarize_beta_binomial(alpha = 1, beta = 1,\n                        y = 6+8+12+5, n = 28+27+29+30) # overall\n\n\n  \n\n\nsummarize_beta_binomial(alpha = 1, beta = 1,\n                        y = 6, n = 28) # cloudtop cruise\n\n\n  \n\n\nsummarize_beta_binomial(alpha = 7, beta = 23,\n                        y = 8, n = 27) # bone-dry dunes\n\n\n  \n\n\nsummarize_beta_binomial(alpha = 15, beta = 42,\n                        y = 12, n = 29) # bowser's castle\n\n\n  \n\n\nsummarize_beta_binomial(alpha = 27, beta = 59,\n                        y = 5, n = 30) # bowser's castle\n\n\n  \n\n\n\nFinal distribution is Beta(32, 84).\n\nsummarize_beta_binomial(alpha = 27, beta = 59,\n                        y = 5, n = 30) # rainbow road\n\n\n  \n\n\n\nFinal distribution is Beta(32, 84).\nAnalyze the data race-by-race in this order: Rainbow Road, Bowser’s Castle, Cloudtop Cruise, and Bone-Dry Dunes.\nIn order:\n\nsummarize_beta_binomial(alpha = 1, beta = 1,\n                        y = 5, n = 30) # rainbow road\n\n\n  \n\n\nsummarize_beta_binomial(alpha = 6, beta = 26,\n                        y = 12, n = 29) # bowser's castle\n\n\n  \n\n\nsummarize_beta_binomial(alpha = 18, beta = 43,\n                        y = 6, n = 28) # cloudtop cruise\n\n\n  \n\n\nsummarize_beta_binomial(alpha = 24, beta = 65,\n                        y = 8, n = 27) # bone-dry dunes\n\n\n  \n\n\n\nFinal distribution is Beta(32, 84).\nAll together:\n\nsummarize_beta_binomial(alpha = 1, beta = 1,\n                        y = 5+12+6+8, n = 30+29+28+27)\n\n\n  \n\n\n\nFinal distribution is Beta(32, 84)."
  },
  {
    "objectID": "files/lectures/05-2-example.html#example-mario-kart",
    "href": "files/lectures/05-2-example.html#example-mario-kart",
    "title": "Mario Kart Analysis",
    "section": "",
    "text": "In Mario Kart 8 Deluxe, item boxes give different items depending on race position. To reduce the “position bias,” only item boxes opened while the racer was in mid-pack (positions 4–10) were recorded.\nYou want to estimate the probability that an item box yields a Red Shell. When playing the Special Cup, only 31 red shells were seen in 114 boxes opened by mid-pack racers.\nFind the posterior distribution under two priors:\n\nFlat/uninformative prior, Beta(1,1).\nBeta(\\alpha, \\beta) of your choosing.\n\n\nFind the posterior under the flat prior, Beta(1,1).\n\nplot_beta_binomial(alpha = 1, beta = 1,\n                   y = 31, n = 114) +\n  theme_bw()\n\n\n\n\n\n\n\n\nFind the posterior under a prior of your choosing.\n\nplot_beta_binomial(alpha = 8, beta = 4,\n                   y = 31, n = 114) +\n  theme_bw()\n\n\n\n\n\n\n\nplot_beta_binomial(alpha = 23, beta = 57,\n                   y = 31, n = 114) +\n  theme_bw()\n\n\n\n\n\n\n\n\n\nSuppose that we know the individual data points for the Special Cup:\n\nCloudtop Cruise, of 28 boxes, 6 had a red shell.\nBone-Dry Dunes, of 27 boxes, 8 had a red shell.\nBowser’s Castle, of 29 boxes, 12 had a red shell.\nRainbow Road, of 30 boxes, 5 had a red shell.\n\nProve sequentiality to yourself.\n\nAnalyze the data race-by-race in this order: Cloudtop Cruise, Bone-Dry Dunes, Bowser’s Castle, and Rainbow Road.\nAnalyze the data race-by-race in this order: Rainbow Road, Bowser’s Castle, Cloudtop Cruise, and Bone-Dry Dunes.\n\n\nAnalyze the data race-by-race in this order: Cloudtop Cruise, Bone-Dry Dunes, Bowser’s Castle, and Rainbow Road.\n\nsummarize_beta_binomial(alpha = 1, beta = 1,\n                        y = 6+8+12+5, n = 28+27+29+30) # overall\n\n\n  \n\n\nsummarize_beta_binomial(alpha = 1, beta = 1,\n                        y = 6, n = 28) # cloudtop cruise\n\n\n  \n\n\nsummarize_beta_binomial(alpha = 7, beta = 23,\n                        y = 8, n = 27) # bone-dry dunes\n\n\n  \n\n\nsummarize_beta_binomial(alpha = 15, beta = 42,\n                        y = 12, n = 29) # bowser's castle\n\n\n  \n\n\nsummarize_beta_binomial(alpha = 27, beta = 59,\n                        y = 5, n = 30) # bowser's castle\n\n\n  \n\n\n\nFinal distribution is Beta(32, 84).\n\nsummarize_beta_binomial(alpha = 27, beta = 59,\n                        y = 5, n = 30) # rainbow road\n\n\n  \n\n\n\nFinal distribution is Beta(32, 84).\nAnalyze the data race-by-race in this order: Rainbow Road, Bowser’s Castle, Cloudtop Cruise, and Bone-Dry Dunes.\nIn order:\n\nsummarize_beta_binomial(alpha = 1, beta = 1,\n                        y = 5, n = 30) # rainbow road\n\n\n  \n\n\nsummarize_beta_binomial(alpha = 6, beta = 26,\n                        y = 12, n = 29) # bowser's castle\n\n\n  \n\n\nsummarize_beta_binomial(alpha = 18, beta = 43,\n                        y = 6, n = 28) # cloudtop cruise\n\n\n  \n\n\nsummarize_beta_binomial(alpha = 24, beta = 65,\n                        y = 8, n = 27) # bone-dry dunes\n\n\n  \n\n\n\nFinal distribution is Beta(32, 84).\nAll together:\n\nsummarize_beta_binomial(alpha = 1, beta = 1,\n                        y = 5+12+6+8, n = 30+29+28+27)\n\n\n  \n\n\n\nFinal distribution is Beta(32, 84)."
  },
  {
    "objectID": "files/lectures/06-2-normal-normal.html#introduction-normal-normal-model",
    "href": "files/lectures/06-2-normal-normal.html#introduction-normal-normal-model",
    "title": "Normal-Normal Model",
    "section": "Introduction: Normal-Normal Model",
    "text": "Introduction: Normal-Normal Model\n\nBefore today, we have learned two conjugate families:\n\nBeta-Binomial (binary outcomes)\n\ny \\sim \\text{Bin}(n, \\pi) (data distribution)\n\\pi \\sim \\text{Beta}(\\alpha, \\beta) (prior distribution)\n\\pi|y \\sim \\text{Beta}(\\alpha+y, \\beta+n-y) (posterior distribution)\n\nGamma-Poisson (count outcomes)\n\nY_i | \\lambda \\overset{ind}\\sim \\text{Pois}(\\lambda) (data distribution)\n\\lambda \\sim \\text{Gamma}(s, r) (prior distribution)\n\\lambda | \\overset{\\to}y \\sim \\text{Gamma}\\left( s + \\sum y_i, r + n \\right) (posterior distribution)\n\n\nNow, we will learn about another conjugate family, the Normal-Normal, for continuous outcomes."
  },
  {
    "objectID": "files/lectures/06-2-normal-normal.html#example-set-up",
    "href": "files/lectures/06-2-normal-normal.html#example-set-up",
    "title": "Normal-Normal Model",
    "section": "Example Set Up",
    "text": "Example Set Up\n\nAs scientists learn more about brain health, the dangers of concussions are gaining greater attention.\nWe are interested in \\mu, the average volume (cm3) of a specific part of the brain: the hippocampus.\nWikipedia tells us that among the general population of human adults, each half of the hippocampus has volume between 3.0 and 3.5 cm3.\n\nTotal hippocampal volume of both sides of the brain is between 6 and 7 cm3.\nLet’s assume that the mean hippocampal volume among people with a history of concussions is also somewhere between 6 and 7 cm3.\n\nWe will take a sample of n=25 participants and update our belief."
  },
  {
    "objectID": "files/lectures/06-2-normal-normal.html#the-normal-model",
    "href": "files/lectures/06-2-normal-normal.html#the-normal-model",
    "title": "Normal-Normal Model",
    "section": "The Normal Model",
    "text": "The Normal Model\n\nLet Y \\in \\mathbb{R} be a continuous random variable.\n\nThe variability in Y may be represented with a Normal model with mean parameter \\mu \\in \\mathbb{R} and standard deviation parameter \\sigma &gt; 0.\n\nThe Normal model’s pdf is as follows,\n\nf(y) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp \\left\\{ \\frac{-(y-\\mu)^2}{2\\sigma^2} \\right\\}"
  },
  {
    "objectID": "files/lectures/06-2-normal-normal.html#the-normal-model-1",
    "href": "files/lectures/06-2-normal-normal.html#the-normal-model-1",
    "title": "Normal-Normal Model",
    "section": "The Normal Model",
    "text": "The Normal Model\n\nIf we vary \\mu,"
  },
  {
    "objectID": "files/lectures/06-2-normal-normal.html#the-normal-model-2",
    "href": "files/lectures/06-2-normal-normal.html#the-normal-model-2",
    "title": "Normal-Normal Model",
    "section": "The Normal Model",
    "text": "The Normal Model\n\nIf we vary \\sigma,"
  },
  {
    "objectID": "files/lectures/06-2-normal-normal.html#the-normal-model-3",
    "href": "files/lectures/06-2-normal-normal.html#the-normal-model-3",
    "title": "Normal-Normal Model",
    "section": "The Normal Model",
    "text": "The Normal Model\n\nOur data model is as follows,\n\nY_i | \\mu \\sim N(\\mu, \\sigma^2)\n\nThe joint pdf is as follows,\n\n\nf(\\overset{\\to}y | \\mu) = \\prod_{i=1}^n f(y_i | \\mu) = \\prod_{i=1}^n \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp \\left\\{ \\frac{-(y_i-\\mu)^2}{2\\sigma^2} \\right\\}\n\n\nMeaning the likelihood is as follows,\n\n\nL(\\mu|\\overset{\\to}y) \\propto \\prod_{i=1}^n \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp \\left\\{ \\frac{-(y_i-\\mu)^2}{2\\sigma^2} \\right\\} = \\exp \\left\\{ \\frac{- \\sum_{i=1}^n(y_i-\\mu)^2}{2\\sigma^2} \\right\\}"
  },
  {
    "objectID": "files/lectures/06-2-normal-normal.html#the-normal-model-4",
    "href": "files/lectures/06-2-normal-normal.html#the-normal-model-4",
    "title": "Normal-Normal Model",
    "section": "The Normal Model",
    "text": "The Normal Model\n\nOur data model is as follows,\n\nY_i | \\mu \\sim N(\\mu, \\sigma^2)\n\nReturning to our brain analysis, we will assume that the hippocampal volumes of our n = 25 subjects have a normal distribution with mean \\mu and standard deviation \\sigma.\n\nRight now, we are only interested in \\mu, so we assume \\sigma = 0.5 cm3\nThis choice suggests that most people have hippocampal volumes within 2 \\sigma = 1 cm3."
  },
  {
    "objectID": "files/lectures/06-2-normal-normal.html#normal-prior",
    "href": "files/lectures/06-2-normal-normal.html#normal-prior",
    "title": "Normal-Normal Model",
    "section": "Normal Prior",
    "text": "Normal Prior\n\nWe know that with Y_i | \\mu \\sim N(\\mu, \\sigma^2), \\mu \\in \\mathbb{R}.\n\nWe think a normal prior for \\mu is reasonable.\n\nThus, we assume that \\mu has a normal distribution around some mean, \\theta, with standard deviation, \\tau.\n\n\\mu \\sim N(\\theta, \\tau^2),\n\nmeaning that \\mu has prior pdf\n\nf(\\mu) = \\frac{1}{\\sqrt{2 \\pi \\tau^2}} \\exp \\left\\{ \\frac{-(\\mu - \\theta)^2}{2 \\tau^2} \\right\\}"
  },
  {
    "objectID": "files/lectures/06-2-normal-normal.html#tuning-the-normal-prior",
    "href": "files/lectures/06-2-normal-normal.html#tuning-the-normal-prior",
    "title": "Normal-Normal Model",
    "section": "Tuning the Normal Prior",
    "text": "Tuning the Normal Prior\n\nWe can tune the hyperparameters \\theta and \\tau to reflect our understanding and uncertainty about the average hippocampal volume (\\mu) among people with a history of concussions.\nWikipedia showed us that hippocampal volumes tend to be between 6 and 7 cm3 \\to \\theta=6.5.\nWhen we set the standard deviation we can check the plausible range of values of \\mu:\n\nFollow up: why 2?\n\n\n\\theta \\pm 2 \\times \\tau\n\nIf we assume \\tau=0.4,\n\n(6.5 \\pm 2 \\times 0.4) = (5.7, 7.3)"
  },
  {
    "objectID": "files/lectures/06-2-normal-normal.html#tuning-the-normal-prior-1",
    "href": "files/lectures/06-2-normal-normal.html#tuning-the-normal-prior-1",
    "title": "Normal-Normal Model",
    "section": "Tuning the Normal Prior",
    "text": "Tuning the Normal Prior\n\nThus, our tuned prior is \\mu \\sim N(6.5, 0.4^2)\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis range incorporates our uncertainty - it is wider than the Wikipedia range."
  },
  {
    "objectID": "files/lectures/06-2-normal-normal.html#normal-normal-conjugacy",
    "href": "files/lectures/06-2-normal-normal.html#normal-normal-conjugacy",
    "title": "Normal-Normal Model",
    "section": "Normal-Normal Conjugacy",
    "text": "Normal-Normal Conjugacy\n\nLet \\mu \\in \\mathbb{R} be an unknown mean parameter and (Y_1, Y_2, ..., Y_n) be an independent N(\\mu, \\sigma^2) sample where \\sigma is assumed to be known.\nThe Normal-Normal Bayesian model is as follows:\n\n\n\\begin{align*}\nY_i | \\mu &\\overset{\\text{iid}} \\sim N(\\mu, \\sigma^2) \\\\\n\\mu &\\sim N(\\theta, \\tau^2) \\\\\n\\mu | \\overset{\\to}y &\\sim N\\left( \\theta \\frac{\\sigma^2}{n\\tau^2 + \\sigma^2} + \\bar{y} \\frac{n\\tau^2}{n\\tau^2 + \\sigma^2}, \\frac{\\tau^2 \\sigma^2}{n \\tau^2 + \\sigma^2} \\right)\n\\end{align*}"
  },
  {
    "objectID": "files/lectures/06-2-normal-normal.html#normal-normal-conjugacy-1",
    "href": "files/lectures/06-2-normal-normal.html#normal-normal-conjugacy-1",
    "title": "Normal-Normal Model",
    "section": "Normal-Normal Conjugacy",
    "text": "Normal-Normal Conjugacy\n\nLet’s think about our posterior and some implications,\n\n\\mu | \\overset{\\to}y \\sim N\\left( \\theta \\frac{\\sigma^2}{n\\tau^2 + \\sigma^2} + \\bar{y} \\frac{n\\tau^2}{n\\tau^2 + \\sigma^2}, \\frac{\\tau^2 \\sigma^2}{n \\tau^2 + \\sigma^2} \\right)\n\nWhat happens as n increases?"
  },
  {
    "objectID": "files/lectures/06-2-normal-normal.html#normal-normal-conjugacy-2",
    "href": "files/lectures/06-2-normal-normal.html#normal-normal-conjugacy-2",
    "title": "Normal-Normal Model",
    "section": "Normal-Normal Conjugacy",
    "text": "Normal-Normal Conjugacy\n\nLet’s think about our posterior and some implications,\n\n\\mu | \\overset{\\to}y \\sim N\\left( \\theta \\frac{\\sigma^2}{n\\tau^2 + \\sigma^2} + \\bar{y} \\frac{n\\tau^2}{n\\tau^2 + \\sigma^2}, \\frac{\\tau^2 \\sigma^2}{n \\tau^2 + \\sigma^2} \\right)\n\nWhat happens as n increases?\n\n\n\\begin{align*}\n\\frac{\\sigma^2}{n\\tau^2 + \\sigma^2} &\\to 0 \\\\\n\\frac{n\\tau^2}{n\\tau^2 + \\sigma^2} &\\to 1 \\\\\n\\frac{\\tau^2 \\sigma^2}{n \\tau^2 + \\sigma^2} &\\to 0\n\\end{align*}"
  },
  {
    "objectID": "files/lectures/06-2-normal-normal.html#normal-normal-conjugacy-3",
    "href": "files/lectures/06-2-normal-normal.html#normal-normal-conjugacy-3",
    "title": "Normal-Normal Model",
    "section": "Normal-Normal Conjugacy",
    "text": "Normal-Normal Conjugacy\n\nLet’s think about our posterior and some implications,\n\n\n\\begin{align*}\n\\frac{\\sigma^2}{n\\tau^2 + \\sigma^2} &\\to 0 \\\\\n\\frac{n\\tau^2}{n\\tau^2 + \\sigma^2} &\\to 1 \\\\\n\\frac{\\tau^2 \\sigma^2}{n \\tau^2 + \\sigma^2} &\\to 0\n\\end{align*}\n\n\nThe posterior mean places less weight on the prior mean and more weight on the sample mean \\bar{y}.\nThe posterior certainty about \\mu increases and becomes more in sync with the data."
  },
  {
    "objectID": "files/lectures/06-2-normal-normal.html#the-normal-posterior-model",
    "href": "files/lectures/06-2-normal-normal.html#the-normal-posterior-model",
    "title": "Normal-Normal Model",
    "section": "The Normal Posterior Model",
    "text": "The Normal Posterior Model\n\nLet us now apply this to our example.\nWe have our prior model, \\mu \\sim N(6.5, 0.4^2).\nLet’s look at the football dataset in the bayesrules package.\n\n\ndata(football)\nconcussion_subjects &lt;- football %&gt;% \n  filter(group == \"fb_concuss\")\n\n\nWhat is the average hippocampal volume?"
  },
  {
    "objectID": "files/lectures/06-2-normal-normal.html#the-normal-posterior-model-1",
    "href": "files/lectures/06-2-normal-normal.html#the-normal-posterior-model-1",
    "title": "Normal-Normal Model",
    "section": "The Normal Posterior Model",
    "text": "The Normal Posterior Model\n\nLet us now apply this to our example.\nWe have our prior model, \\mu \\sim N(6.5, 0.4^2).\nLet’s look at the football dataset in the bayesrules package.\n\n\ndata(football)\nconcussion_subjects &lt;- football %&gt;% \n  filter(group == \"fb_concuss\")\n\n\nWhat is the average hippocampal volume?\n\n\nmean(concussion_subjects$volume)\n\n[1] 5.7346"
  },
  {
    "objectID": "files/lectures/06-2-normal-normal.html#the-normal-posterior-model-2",
    "href": "files/lectures/06-2-normal-normal.html#the-normal-posterior-model-2",
    "title": "Normal-Normal Model",
    "section": "The Normal Posterior Model",
    "text": "The Normal Posterior Model\n\nWe can also plot the density!\n\n\n\nconcussion_subjects %&gt;% ggplot(aes(x = volume)) + geom_density() + theme_bw()"
  },
  {
    "objectID": "files/lectures/06-2-normal-normal.html#the-normal-posterior-model-3",
    "href": "files/lectures/06-2-normal-normal.html#the-normal-posterior-model-3",
    "title": "Normal-Normal Model",
    "section": "The Normal Posterior Model",
    "text": "The Normal Posterior Model\n\nNow, we can plug in the information we have (n = 25, \\bar{y} = 5.735, \\sigma = 0.5) into our likelihood,\n\n\nL(\\mu|\\overset{\\to}y) \\propto \\exp \\left\\{ \\frac{-(5.735 - \\mu)^2}{2(0.5^2/25)} \\right\\}"
  },
  {
    "objectID": "files/lectures/06-2-normal-normal.html#the-normal-posterior-model-4",
    "href": "files/lectures/06-2-normal-normal.html#the-normal-posterior-model-4",
    "title": "Normal-Normal Model",
    "section": "The Normal Posterior Model",
    "text": "The Normal Posterior Model\n\nWe are now ready to put together our posterior:\n\nData distribution, Y_i | \\mu \\overset{\\text{iid}} \\sim N(\\mu, \\sigma^2)\nPrior distribution, \\mu \\sim N(\\theta, \\tau^2)\n\nPosterior distribution, \\mu | \\overset{\\to}y \\sim N\\left( \\theta \\frac{\\sigma^2}{n\\tau^2 + \\sigma^2} + \\bar{y} \\frac{n\\tau^2}{n\\tau^2 + \\sigma^2}, \\frac{\\tau^2 \\sigma^2}{n \\tau^2 + \\sigma^2} \\right)\n\nGiven our information (\\theta=6.5, \\tau=0.4, n=25, \\bar{y}=5.735, \\sigma=0.5), our posterior is\n\n\n\\mu | \\overset{\\to}y \\sim N\\left( \\theta \\frac{\\sigma^2}{n\\tau^2 + \\sigma^2} + \\bar{y} \\frac{n\\tau^2}{n\\tau^2 + \\sigma^2}, \\frac{\\tau^2 \\sigma^2}{n \\tau^2 + \\sigma^2} \\right)"
  },
  {
    "objectID": "files/lectures/06-2-normal-normal.html#the-normal-posterior-model-5",
    "href": "files/lectures/06-2-normal-normal.html#the-normal-posterior-model-5",
    "title": "Normal-Normal Model",
    "section": "The Normal Posterior Model",
    "text": "The Normal Posterior Model\n\nGiven our information (\\theta=6.5, \\tau=0.4, n=25, \\bar{y}=5.735, \\sigma=0.5), our posterior is\n\n\n\\begin{align*}\n\\mu | \\overset{\\to}y &\\sim N\\left( \\theta \\frac{\\sigma^2}{n\\tau^2 + \\sigma^2} + \\bar{y} \\frac{n\\tau^2}{n\\tau^2 + \\sigma^2}, \\frac{\\tau^2 \\sigma^2}{n \\tau^2 + \\sigma^2} \\right) \\\\\n&\\sim N\\left( 6.5 \\frac{0.5^2}{25 \\cdot 0.4^2 + 0.5^2} + 5.735 \\frac{25 \\cdot 0.4^2}{25 \\cdot 0.4^2 + 0.5^2}, \\frac{0.4^2 \\cdot 0.5^2}{25 \\cdot 0.4^2 + 0.5^2} \\right) \\\\\n&\\sim N(6.5 \\cdot 0.0588 + 5.737 \\cdot 0.9412, 0.09^2) \\\\\n&\\sim N(5.78, 0.09^2)\n\\end{align*}\n\n\nLooking at the posterior, we can see the weights\n\n95% on the data mean, 6% on the prior mean."
  },
  {
    "objectID": "files/lectures/06-2-normal-normal.html#the-normal-posterior-model-6",
    "href": "files/lectures/06-2-normal-normal.html#the-normal-posterior-model-6",
    "title": "Normal-Normal Model",
    "section": "The Normal Posterior Model",
    "text": "The Normal Posterior Model\n\nLooking at just the prior and data distributions,"
  },
  {
    "objectID": "files/lectures/06-2-normal-normal.html#the-normal-posterior-model-7",
    "href": "files/lectures/06-2-normal-normal.html#the-normal-posterior-model-7",
    "title": "Normal-Normal Model",
    "section": "The Normal Posterior Model",
    "text": "The Normal Posterior Model\n\nNow including the posterior,"
  },
  {
    "objectID": "files/lectures/06-2-normal-normal.html#the-normal-posterior-model-8",
    "href": "files/lectures/06-2-normal-normal.html#the-normal-posterior-model-8",
    "title": "Normal-Normal Model",
    "section": "The Normal Posterior Model",
    "text": "The Normal Posterior Model\n\nWe can use the summarize_normal_normal() function to summarize the distribution,\n\n\nsummarize_normal_normal(mean = 6.5, sd = 0.4, sigma = 0.5, y_bar = 5.735, n = 25)"
  },
  {
    "objectID": "files/lectures/06-2-normal-normal.html#wrap-up-normal-normal-model",
    "href": "files/lectures/06-2-normal-normal.html#wrap-up-normal-normal-model",
    "title": "Normal-Normal Model",
    "section": "Wrap Up: Normal-Normal Model",
    "text": "Wrap Up: Normal-Normal Model\n\nWe have built the Normal-Normal model for \\mu, an unknown mean.\n\n\n\\begin{equation*}\n\\begin{aligned}\nY_i | \\mu &\\overset{\\text{iid}} \\sim N(\\mu, \\sigma^2) \\\\\n\\mu &\\sim N(\\theta, \\tau^2) &\n\\end{aligned}\n\\Rightarrow\n\\begin{aligned}\n&& \\mu | \\overset{\\to}y &\\sim N\\left( \\theta \\frac{\\sigma^2}{n\\tau^2 + \\sigma^2} + \\bar{y} \\frac{n\\tau^2}{n\\tau^2 + \\sigma^2}, \\frac{\\tau^2 \\sigma^2}{n \\tau^2 + \\sigma^2} \\right) \\\\\n\\end{aligned}\n\\end{equation*}\n\n\nThe prior model, f(\\mu), is given by N(\\theta,\\tau^2).\nThe data model, f(Y|\\mu), is given by N(\\mu, \\sigma^2).\nThe posterior model is a Normal distribution with updated parameters\n\nmean = \\theta \\frac{\\sigma^2}{n\\tau^2 + \\sigma^2} + \\bar{y} \\frac{n\\tau^2}{n\\tau^2 + \\sigma^2}\nvariance = \\frac{\\tau^2 \\sigma^2}{n \\tau^2 + \\sigma^2}"
  },
  {
    "objectID": "files/lectures/06-2-normal-normal.html#wrap-up",
    "href": "files/lectures/06-2-normal-normal.html#wrap-up",
    "title": "Normal-Normal Model",
    "section": "Wrap Up",
    "text": "Wrap Up\n\nThis week we have learned the other two conjugate families.\n\nGamma-Poisson: count outcomes\nNormal-Normal: continuous outcomes\n\nWhile we are not forced to analyze our data using conjugate families, our lives are much easier when we can use the known relationships.\nNow that we know how to specify the posterior distributions, we can focus on moving forward with drawing conclusions about the posterior distribution.\n\nProbabilities\nInference"
  },
  {
    "objectID": "files/lectures/06-2-normal-normal.html#homework-practice",
    "href": "files/lectures/06-2-normal-normal.html#homework-practice",
    "title": "Normal-Normal Model",
    "section": "Homework / Practice",
    "text": "Homework / Practice\n\nFrom the Bayes Rules! textbook:\n\n5.9\n5.10"
  },
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "Abbreviated Syllabus",
    "section": "",
    "text": "Note: this is an abbreviated syllabus. The full syllabus is available through Classmate and/or Canvas.\n\nInstructor Information\n\nDr. Samantha Seals\nOffice: 4/344\n\n\n\nMeeting Times and Location\n\nMW 4:00 pm–5:15 pm\nPhysical classroom: 4/406\nZoom classroom: see Canvas or full syllabus for link\n\n\n\nOffice Hours\n\nMonday: 9:00 am-11:00 am\nTuesday: 2:00 pm-4:00 pm\nWednesday: 9:00 am-11:00 am\nOther times by appointment\n\n\n\nGrading and Evaluation\nThe course grade will be determined as follows:\n\nAssignments (25%): Every module will finish with an assignment to demonstrate the knowledge gained. TThe resulting .html file should be submitted to the designated Assignment on Canvas by 11:59 pm on the specified date (see Canvas).\nProject (50%): Students in this course will be collaborating with students in GEO6118 - Research Design. The final research project will be submitted along with smaller pieces throughout the semester.\nFinal Exam (25%): The final exam will be a take home final.\n\n\n\nLate Policy\nAssigments have due dates, however, the dropboxes will not close until the end of the semester. All students are automatically granted “extensions” without question.\nNote that if there is not a submission when I go to grade (after the initial deadline), I will assign a zero (0) and request that you submit the assignment when you are able to. This is only for record keeping purposes. There is no penalty for submitting late and a full grade will be given upon review of your submission.\nExtensions are not available for project pieces or the final exam."
  }
]